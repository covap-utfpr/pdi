---
output:
  pdf_document: default
  html_document: default
  bookdown::gitbook:
    highlight: pygments
---

# Deep Learning em visão computacional 
Antes de iniciarmos o estudo sobre deep learning, e mais especificamente sobre redes neurais artificiais convolucionais, é importante termos uma visão ampla sobre a área e suas subdivisões, para conseguirmos nos localizar em meio a essa área que cresce cada vez mais. Por isso começaremos falando um pouco sobre inteligência artificial e suas subdivisões, além de sua conexão e uso com visão computacional, que é o nosso foco.

## Caracterização de AI, Machine Learning e Deep Learning
Esses três termos costumam causar certa confusão, principalmente em pessoas que estão começando a estudar essa área. De maneira geral, o termo Inteligência Artificial (IA) denomina uma área que possui muitas vertentes e tópicos de estudos, onde a maioria tem o foco em conseguir fazer os computadores realizarem tarefas complexas, que anteriormente eram realizadas exclusivamente por humanos.

No começo dos estudos sobre IA foram tentados e resolvidos muitos problemas que eram considerados difíceis para seres humanos, mas relativamente fáceis para os computadores[@goodfellow2016, p.1]. Esses eram problemas que podiam ser descritos formalmente, por meio de regras matemáticas, como exemplo temos o jogo de xadrez, onde, em 1997 o campeão Garry Kasparov perdeu para o IBM Deep Blue(Figura \@ref(fig:deepBlue)).

(ref:deepBlue) IBM Deep Blue [@img:deepblue]

```{r deepBlue, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:deepBlue)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/07-deepLearning/deep-blue.jpg'))
```

Com o tempo começamos a perceber que a dificuldade não residia nesses problemas, mas naqueles que são realizados facilmente, até instintivamente e intuitivamente pelos humanos, como reconhecer rostos familiares, entender linguagens, etc. A questão é que os seres humanos, no dia a dia, recebem e processam quantidades enormes de informações, e tentar fazer os computadores realizarem essas atividades somente com regras descritas por nós não era algo viável, por isso os pesquisadores começaram a desenvolver técnicas onde o próprio computador, através de algoritmos, aprendesse a retirar essas regras e informações sozinho de bases de dados brutos, a isso chamamos de Machine Learning(Aprendizado de Máquina).

Dentro da área de Machine Learning, temos um conjunto de técnicas e áreas de pesquisa, sendo que uma delas utiliza um modelo baseado na biologia de cérebros biológicos, contendo neurônios e conexões conhecidas como Redes Neurais. Na Figura \@ref(fig:cajalCortex) temos uma representação dos neurônios do córtex cerebral humano, onde podemos ver as conexões formadas por eles, que se assemelham com os modelos de redes neurais como o da Figura \@ref(fig:coloredNeuralNetwork).

(ref:cajalCortex) Representação da conexão de neurônios no córtex cerebral [@cajal, p.363]

```{r cajalCortex, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cajalCortex)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cajal-cortex.png'))
```

Atualmente, como ouvimos muito se falar sobre IAs, temos a tendência de pensar que essa é uma técnica moderna, mas ao contrário, a ideia de fazer os computadores imitarem o esquema de funcionamento do cérebro remonta a 1943, quando Warren McCulloch e Walter Pitts sugeriram a ideia em seu artigo “A logical calculus of the ideas immanent in nervous activity”[@mcculloch1943].

Como pode ser visto na Figura \@ref(fig:coloredNeuralNetwork), as redes neurais são formadas por camadas, sendo que os dados entram pela camada de Input, são processadas nas camadas Hidden e temos os dados de saída na camada Output. Cada uma dessas camadas é formada por um número de neurônios(representados pelos círculos) e tem as conexões representadas pelas setas. Por enquanto não nos aprofundaremos tanto no funcionamento das redes neurais, que serão abordadas na seção x.

(ref:coloredNeuralNetwork) Rede neural artificial [@img:coloredNeuralNetwork]

```{r coloredNeuralNetwork, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:coloredNeuralNetwork)', fig.align='center', out.width='50%'}
knitr::include_graphics(rep('imagens/07-deepLearning/colored-neural-network.png'))
```

Nos últimos anos, temos visto um leque de aplicações cada vez maior para as técnicas de IA. Nosso objetivo nesse material é introduzir, principalmente, o uso das Redes Neurais Artificiais na área da Visão Computacional, que é identificada como uma das subáreas da Inteligência Artificial pois busca reproduzir algumas das capacidades humanas a partir de sistemas autônomos. O principal interesse desta área é fazer com que computadores desempenhem funções semelhantes à visão humana, sendo capazes de receber dados visuais e com eles realizar reconhecimentos, classificações e análises. Análogo ao processo de aprendizado dos seres humanos, identifica-se que a melhora no desempenho da visão computacional está fortemente interligada com a evolução do aprendizado de máquina (machine learning), outro segmento da inteligência artificial.

Antes de entrarmos realmente no assunto de redes neurais, vamos apresentar, de maneira resumida, alguns tópicos principais da área de Machine Learning, pois como dito anteriormente, o deep learning e as redes neurais estão dentro dessa área, e o entendimento desses tópicos pode auxiliar no entendimento pleno dos tópicos futuros.

### Aprendizado supervisionado e não supervisionado
Dentro dos algoritmos de machine learning existe uma característica que os separa em diferentes tipos, baseado em sua forma de aprendizado, sendo os principais os algoritmos de aprendizado supervisionado e não supervisionado. 

Na aprendizagem supervisionada os algoritmos têm previamente os pares entrada-saída, ou seja, para cada entrada já temos conhecimento prévio de como deve ser a saída[@russell2016, p.695], e a partir disso nosso algoritmo deve aprender a generalizar bem as entradas. Podemos formalizar isso da seguinte forma[@russell2016, p.695]:

Dado um conjunto de treinamento de n pares $(x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)$ onde $x_i$ são as entradas e $y_i = f(x_i)$ as saídas, nosso algoritmo deve descobrir uma função $h$, conhecida como hipótese, que aproxime $f$. Para sabermos se nossa hipótese aproxima bem $f$ após ter treinado o algoritmo, utilizamos um conjunto de testes - que contém exemplos diferentes do conjunto de treinamento - e avaliamos o quão bem o algoritmo generaliza(dá respostas corretas) as novas entradas. 
Já na aprendizagem não supervisionada não há nenhum feedback para as saídas do algoritmo, ou seja, ele recebe somente os dados de entrada. Por isso, uma das principais tarefas designadas a esses tipos de algoritmos é a de clustering(agrupamento), onde o algoritmo aprende a encontrar padrões nos dados de entrada e separá-lo em grupos.

### Redes Neurais Artificiais
Parte da base teórica que fundamenta o aprendizado profundo surgiu inicialmente como  modelos para entender o aprendizado, ou seja, como o cérebro funciona. Desta forma, estas teorias ficaram conhecidas como Redes Neurais, uma das áreas do aprendizado profundo que mais cresceram nos últimos anos [@goodfellow2016, p.1]. Atualmente, os conceitos de Redes Neurais abordam princípios mais genéricos além da perspectiva da neurociência. Mesmo que as Redes Neurais não sejam capazes de explicar muito sobre o cérebro, não podendo ser encaradas como modelos realistas da função biológica, vários aspectos do aprendizado ainda continuam sendo inspirações.  

As redes foram pensadas para adquirir o conhecimento por um processo de aprendizagem. Semelhante ao que ocorre no cérebro, as interações entre os neurônios, ou pesos sinápticos, são responsáveis por armazenar o conhecimento. Em termos práticos, o conhecimento de uma rede seria a capacidade de uma máquina de realizar funções complexas de forma autônoma, como classificações e reconhecimentos de padrões.  As redes também são capazes de generalizar a informação aprendida,  extraindo características essenciais de exemplos e garantido respostas coerentes para novos casos[@haykin1999, p.28].

Mesmo que o termo Rede Neural só tenha começado a ganhar destaque nos últimos anos, os primeiros estudos teóricos começaram por volta de 1940[@goodfellow2016, p.12]. Um dos primeiros trabalhos publicados foi “A Logical Calculus of the Ideas Immamente in Nervous Activity” de 1943, em que os autores, Warren McCulloch e Walter Pitts, apresentaram um modelo artificial de um neurônio a partir da teoria de redes lógicas de nós[@goodfellow2016, p.14].

A Figura \@ref(fig:neuron) apresenta uma simplificação de um neurônio biológico, dividido em três partes principais: o corpo da célula, os dendritos e o axônio. Um neurônio recebe informações, ou impulsos nervosos, a partir dos dendritos. Estas informações são processadas no corpo celular e novos impulsos são transmitidos através do axônio para outros neurônios. A comunicação entre os neurônios, a sinapse, controla a transmissão dos impulsos, determinando o fluxo de informações com base na intensidade do sinal recebido[@haykin1999, p.36].

(ref:neuron) Representação de um neurônio biológico[@img:neuronCS]

```{r neuron, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:neuron)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/07-deepLearning/neuron.png'))
```

Por analogia, McCulloch e Pitts descreveram matematicamente um neurônio artificial como um modelo com $n$ terminais de entrada $x_m$, representando os dendritos, e apenas um ponto de saída $y_k$ como axônio (Figura \@ref(fig:artificialNeuron)). Para simular o comportamento das sinapses, cada entrada $x_m$ é associada com um peso $w_{km}$, sendo que o somatório representa a intensidade do sinal recebido ($v_k$).

(ref:artificialNeuron) Representação matemática de um neurônio artificial[@haykin1999, p.36]

```{r artificialNeuron, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:artificialNeuron)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/artificial-neuron.png'))
```

O sinal de resposta é estabelecido por uma função de ativação $\varphi$ aplicada ao valor da soma ponderada, e que apresenta comportamento limiar como na equação, em que a saída é zero ou um dependendo do valor limite (Figura \@ref(fig:limiarFunc)). O modelo também pode incluir um bias ($b_k$) no somatório para aumentar o grau de liberdade da função de ativação e garantir que um neurônio não apresente saída nula mesmo que todas as entradas sejam nulas. O valor do bias é ajustado junto com os pesos sinápticos[@haykin1999, p.37].

$$y_k=\varphi(\upsilon_k)=
\begin{cases}
 1 \text{ se } \upsilon_k > 0 \\ 
 0 \text{ se } \upsilon_k < 0 
\end{cases}$$

(ref:limiarFunc) Função de ativação de limiar[@haykin1999, p.36]

```{r limiarFunc, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:limiarFunc)', fig.align='center', out.width='50%'}
knitr::include_graphics(rep('imagens/07-deepLearning/limiar-function.png'))
```

O modelo proposto por McCulloch e Walter Pitts poderia realizar classificações em duas categorias, entretanto os pesos precisavam ser ajustados manualmente, pois não tinham a capacidade de aprender[@goodfellow2016, p.14]. Uma das primeiras discussões sobre regras de aprendizagem nas correções dos pesos sinápticos foi publicada em 1949 no livro de Donald Hebb “The Organization of Behavior”[@haykin1999, p.64]. No postulado de Hebb se apresenta que a conexão entre os neurônios é fortalecida cada vez que é utilizada, assim, os caminhos neurais no cérebro são continuamente modificados e formam agrupamentos.

A primeira Rede neural com capacidade de aprender os pesos das categorias foi o Perceptron apresentado por Frank Rosenblatt em 1958[@haykin1999, p.65]. O Perceptron tinha arquitetura semelhante a da Figura \@ref(fig:perceptron), uma rede de camada única além da entrada, e de aprendizado supervisionado. Inicialmente, foram lançadas grandes expectativas sobre as possíveis aplicações do Perceptron, entretanto, as limitações logo começaram a ser destacadas, muitas descritas no livro de Marvin Minsky e Seymour Papert publicado em 1969. Um perceptron de camada única realiza apenas a classificação de padrões linearmente separáveis em duas categorias, não podendo, por exemplo, representar o operador de lógica XOR, que não é linearmente separável[@goodfellow2016, p.14].

(ref:perceptron) Arquitetura Perceptron[@haykin1999, p.47]

```{r perceptron, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:perceptron)', fig.align='center', out.width='50%'}
knitr::include_graphics(rep('imagens/07-deepLearning/one-layer-perceptron.png'))
```

A imagem negativa sobre o perceptron e as limitações tecnológicas diminuiram a popularidade das redes neurais, reduzindo o número de pesquisas na área até os anos 80[@goodfellow2016, p.16]. O interesse pelas redes neurais começou a aumentar principalmente pelo uso da abordagem de processamento paralelo distribuído, como o aplicado no algoritmo de retropropagação (back-propagation) apresentado por Rumelhart, Hinton e Williams (1986). Ainda hoje este é o algoritmo mais utilizado para aprendizado profundo e foi crucial para o treinamento dos perceptrons de múltiplas camadas MLP[@haykin1999, p.184].

#### Rede MLP
Para que a rede de perceptrons de múltiplas camadas pudesse aprender seria necessário a retropropagação dos erros de trás para frente entre as camadas, tornando possível a minimização da função custo. A necessidade do cálculo da derivada do erro implicou no aparecimento de funções de ativação diferentes do utilizado no modelo original do perceptron, que não fossem de limitação abrupta Figura \@ref(fig:limiarFunc)- ativação limiar[@haykin1999, p.184]. Considerando que as funções de ativação são um dos elementos utilizados para a inclusão de não linearidade, ponto chave para que os modelos não se limitem à padrões linearmente separáveis, a abordagem foi a incorporação de funções não lineares, mas “bem comportadas“, ou seja,  que são “quase” lineares contínuas e deriváveis. 

Como as funções de ativação são responsáveis pelo intermédio das respostas entre as camadas, deveriam ser considerados formatos não lineares que não alterassem de forma radical a resposta da rede. Os perfis que mais se aproximavam destes comportamentos são as funções sigmóides, tangente hiperbólica e a função logística[@rateke1999].

A função sigmóide tem um formato em S, em que nas extremidades a função tem um comportamento constante, o que fica evidente no gráfico da função logística (Figura \@ref(fig:sigmoid)). O parâmetro a da equação logística na equação permite parametrizar o comportamento da função, alterando a inclinação. Quanto maior o valor de a, mais a função sigmóide se aproxima da função de limiar.

(ref:sigmoid) Função sigmóide[@haykin1999, p.39]

```{r sigmoid, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:sigmoid)', fig.align='center', out.width='50%'}
knitr::include_graphics(rep('imagens/07-deepLearning/sigmoid-function.png'))
```


$$\varphi(\upsilon)=\frac{1}{1+\exp(-a\upsilon)}$$

Diferente da função limiar que assume valores $0$ ou $1$, a função logística tem resultados em um intervalo contínuo entre $0$ e $1$[@haykin1999, p.40]. A função sigmóide também é diferenciável, enquanto que a função de limiar não. Uma forma anti-simétrica da sigmóide é a função tangente hiperbólica na equação. A função tangente hiperbólica é definida no intervalo $-1$ a $1$, o que permite a função sigmóide assumir também valores negativos[@haykin1999, p.40].

$$\varphi(\upsilon)=\tanh(a\upsilon)$$

Ao se propor um método eficiente no treinamento dos perceptrons de múltiplas camadas se tornou interessante a inclusão de uma ou mais camadas de neurônios ocultos entre a camada de entrada e de saída. A combinação de mais camadas permitiu que a rede fosse implementada para problemas mais complexos, não se restringindo às transformações lineares do modelo original do perceptron. Por meio das camadas ocultas é possível extrair de forma progressiva características importantes que definem os padrões de entrada[@haykin1999, p.184].

O neurônio matemático proposto inicialmente foi estendido para uma estrutura de conexões de elementos de processamento, os nós da rede. Os elementos foram organizados em camadas, e foram propostas diferentes configurações de conexões. Os formatos mais populares são definidos como uma arquitetura de rede neural, reconhecida pelo número de camadas da rede, número de nós em cada camada e tipo de conexão entre os nós. 

A arquitetura da rede MLP é composta por uma camada de entrada que recebe o sinal, uma camada de saída que retorna o resultado, e entre elas um número arbitrário de camadas ocultas (Figura \@ref(fig:mlpnet)).  Geralmente, a escolha do número de nós na camada de entrada e saída é direta. Por exemplo, em uma aplicação com imagens, o número de neurônios na camada de entrada pode corresponder ao número de pixels da imagem. Neste caso, a saída poderia ser projetada com um único neurônio indicando a probabilidade de um resultado positivo. Já o arranjo das camadas intermediárias não é tão simples, muitas vezes é definido empiricamente com base nas características dos dados de entrada e na complexidade do problema (CARVALHO; BRAGA; LUDERMIR, 1998).

(ref:mlpnet) Arquitetura da rede MLP[@haykin1999, p.186]

```{r mlpnet, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:mlpnet)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/07-deepLearning/mlp-network.png'))
```

Uma classificação comum das arquiteturas é com base no padrão de conexões, sendo identificadas duas classes principais: redes diretas (feed forward) e redes recorrentes[@haykin1999, p.46]. O modelo MLP tem arquitetura do tipo feedforward, em que a propagação da informação ocorre em uma única direção e os nós de uma mesma camada não são conectados entre si. A saída de uma camada é usada como entrada na próxima, sem loops, ou seja, não são enviadas de volta[@haykin1999, p.47]. 

Já nas tipologias recorrentes ocorre o feedback, um processo de realimentação, em que as saídas de nós são reinseridas como entradas em nós anteriores (Figura \@ref(fig:recnet)). O comportamento dos ciclos é dinâmico controlado por atrasos unitários[@haykin1999, p.49]. A ideia do modelo é estimular sinais em efeito cascata com dependência temporal.

(ref:recnet) Arquitetura rede recorrente[@haykin1999, p.49]

```{r recnet, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:recnet)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/07-deepLearning/recurrent-network.png'))
```

#### Backpropagation
Para explicar o algoritmo backpropagation no treinamento de redes neurais utilizaremos um exemplo de aplicação de rede MLP para o reconhecimento de números. O código da rede é uma implementação do livro online “Neural Networks and Deep Learning” escrito por Michael Nielsen. Os dados de treinamento foram retirados  do MNIST data set, que contém mais de 60000 imagens escaneadas de números escritos juntamente com os rótulos de classificação. As informações foram coletadas pelo Instituto Nacional de Padrões e Tecnologia dos Estados Unidos (NIST), sendo que as imagens são em escala de cinza e de tamanho 28 x 28 pixels como na Figura \@ref(fig:mnistZero).

(ref:mnistZero) Arquitetura rede recorrente[@nielsen2015]

```{r mnistZero, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:mnistZero)', fig.align='center', out.width='40%'}
knitr::include_graphics(rep('imagens/07-deepLearning/mnist-zero.png'))
```

O conjunto de dados originais do MNIST é dividido em duas partes, uma que contém 60000 imagens para treinamento e a outra com 10000 imagens para a fase de teste em que se avalia a acurácia da rede treinada para reconhecer os dígitos. No exemplo do autor Michael Nielsen, os dados de treinamento original também foram reorganizados em dois grupos, o primeiro com 50000 imagens que foram utilizados no treinamento e a outra parte (10000) foi reservada para a validação em que se definiu os hiperparâmetros da rede. 

Considerando imagens de 28x28 bits os dados de entrada foram definidos como um vetor $x$ de dimensão $784$, em que cada posição corresponde a um valor de pixel da imagem. Para o vetor $y$ de saída da rede se estabeleceu a dimensão $10$, em que cada posição faz referência a um dígito de $0$ a $9$. Assim, se uma entrada corresponde ao número $3$ então a saída esperada será o vetor transposto na forma $y(x)=(0,0,0,1,0,0,0,0,0,0)^T$. Com base no formato dos dados de entrada e saída da rede, o exemplo foi construído com uma rede MLP de três camadas como na Figura \@ref(fig:mlpTwo), com a primeira camada tendo $784$ nós e a última camada com $10$ nós. Na camada do meio, a camada oculta, utilizaremos $30$ nós, mas vale destacar que o autor Michael Nielsen definiu o número de nós após alguns testes otimizando a escolha dos parâmetros da rede.

(ref:mlpTwo) Rede MLP com uma camada oculta[@hertz2018]

```{r mlpTwo, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:mlpTwo)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/07-deepLearning/mlp-two-layers.png'))
```

Para carregar os dados e configurá-los no formato proposto utiliza-se o método “load_data_wrapper()”. Os dados são retirados do arquivo zip “mnist.pkl.gz'” e subdivididos dentro do método “load_data()” que retorna para o “load_data_wrapper()”, como dados de treinamento, validação e de teste. A função geral é chamada da seguinte forma:

```python
training_data, validation_data, test_data = load_data_wrapper()
```

No programa, a rede é construída a partir do comando  Network([784, 30, 10], cost=QuadraticCost), em que cada argumento corresponde ao número de nós na camada. Os atributos da classe Network incluem o número de camadas (num_layers), o número de nós em cada camada (sizes), os pesos e bias iniciais que são gerados de forma aleatória pelo método “default_weight_initializer()”, e a função custo (cost). A função custo aplicada neste exemplo, o erro quadrático (MSE), é definida na classe “QuadraticCost”.

```python
class Network(object):
  def __init__(self, sizes, cost=QuadraticCost):
    self.num_layers = len(sizes)
    self.sizes = sizes
    self.default_weight_initializer()
    self.cost=cost
```

A seguir apresentaremos um resumo da teoria matemática do método backpropagation e para facilitar este processo utilizaremos a nomenclatura dos elementos de uma rede neural com base no livro “Introduction To The Theory Of Neural Computation”[@hertz2018, p.116]. No treinamento de uma rede como a da Figura \@ref(fig:mlpTwo) é apresentado um conjunto de treinamento $\{\xi_k^\mu,\zeta_i^\mu\}$ , em que cada padrão $(\mu=1, 2,\dots, p)$ apresentado corresponde a um par de entrada ($\xi_k^\mu$) e saída esperada ($\zeta_i^\mu$). Neste exemplo, o número de padrões no treinamento é $p=50000$. O índice $k$ na camada de entrada faz referência ao valor em cada nó da camada, e o índice $i$ aos nós da camada da saída. A resposta final da rede é identificada como $O_i$ e o sinal de saída da camada oculta é $V_j$. A conexão entre a camada de entrada e a oculta é estabelecida pelos pesos $w_{jk}$, e os pesos $W_{ij}$ conectam a camada de saída com a intermediária. 

O backpropagation é um método supervisionado em que o treinamento ocorre em duas fases[@haykin1999, p.163]. Na etapa foward, uma entrada é apresentada para a rede e de acordo com as conexões estabelecidas entre as camadas é propagado sucessivamente os sinais de respostas até a camada de saída, gerando um resultado que se espera ser o mais próximo do padrão. Cada nó de uma camada  seguinte se conecta com todos os nós da camada anterior, sendo que o sinal recebido por este nó é uma ponderação dos pesos de todas as conexões. O sinal de entrada de cada nó recebe um bia e é passado para a próxima camada como uma resposta de uma função de ativação ($g$). A resposta de saída de um nó será denominada $V_j$ se o sinal for para uma camada intermediária, ou $O_i$ se for direcionado para a camada de saída.  

Imagine que um nó ($j$) da camada intermediária recebe como entrada:

$$h_j^\mu=\sum_{k}w_{jk}\xi_k^\mu$$

e produz como resposta:

$$V_j^\mu=g(h_j^\mu)=g(w_{jk}\xi_k^\mu)$$

Assim, um nó na camada de saída recebe como entrada o sinal propagado:

$$h_i^\mu=\sum_kW_{ij}V_j^\mu=\sum_kW_{ij}g(\sum_kw_{jk}\xi_k^\mu)$$

gerando como resposta da saída da rede:

$$O_i^\mu=g(h_i^\mu)=g(\sum_kW_{ij}V_j^\mu)=g(\sum_kW_{ij}g(\sum_kw_{jk}\xi_k^\mu))$$

No programa, a fase forward é representada pelo seguinte método:

```python
feedforward(self, a):
  for b, w in zip(self.biases, self.weights):
    a = sigmoid(np.dot(w, a)+b)
  return a
```

Neste exemplo a função de ativação é a função logística definida pelo método “sigmoid” e a sua derivada é calculada no método “sigmoid_prime”.

```python
sigmoid(z):
  return 1.0/(1.0+np.exp(-z))
 
sigmoid_prime(z):
  return sigmoid(z)*(1-sigmoid(z))
```

Na segunda fase, backward, os pesos e bias são corrigidos camada a camada, no sentido da saída da rede até a entrada, em um processo iterativo de forma que a saída i fique cada vez mais próxima do padrão esperado Oi, reduzindo o erro[@haykin1999, p.163]. Uma forma de avaliar como o erro é reduzido em relação às alterações dos parâmetros é determinando uma função Erro, ou custo, dependente dos pesos e bias. Adotamos como função custo o erro quadrático (MSE): 

$$E[w]=\frac{1}{2}\sum_{\mu i}[\zeta_i^\mu-O_i^\mu]^2 = \frac{1}{2}[\zeta_i^\mu W_{ij}g(\sum_kw_{jk}\xi_k^\mu)]$$

No programa, a função custo MSE é apresentada no método “fn(a, y)” na classe “QuadraticCost”:

```python
fn(a, y):
  return 0.5*np.linalg.norm(a-y)**2
```

A redução do erro envolve um processo de otimização, denominado descida em gradiente, em que se busca determinar os parâmetros (pesos e bias) que minimizam a função custo[@nielsen2015]. Neste método, a variação do erro pode ser escrita como derivadas parciais do erro em função dos pesos, compondo o vetor gradiente do erro. Como o vetor gradiente aponta no sentido de maior acréscimo do erro, a variação dos pesos é dada pelo negativo do gradiente, garantindo a redução mais rápida do erro. Assim, a regra do gradiente descendente aplicada nas conexões entre a camada oculta e de saída pode ser escrita como:

$$\Delta W_{ij}=-\eta\frac{\partial E}{\partial W_{ij}}=\eta\sum_\mu[\zeta_i^\mu-O_i^\mu]g'(h_i^\mu)V_j^\mu=\eta\sum_\mu\delta_i^\mu V$$
$$\delta_i^\mu=[\zeta_i^\mu-O_i^\mu]g'(h_i^\mu)$$

A fórmula de modificações dos pesos é conhecida como regra delta e recebe o termo $\eta$, a taxa de aprendizagem, para promover uma correção gradativa, sem alterações bruscas [@nielsen2015]. O termo $g’$ se refere a derivada da função de ativação e surge na fórmula devido a derivação da função erro. A regra delta aplicada nas conexões entre a camada oculta e de entrada utiliza a regra da cadeia pois as derivadas são em relação aos pesos $w_{jk}$, que se apresentam como dependência mais implícita ao erro. A correção dos pesos neste caso ocorre como:

$$\begin{split}
\Delta w_{ij}&=-\eta\frac{\partial E}{\partial w_{jk}}=-\eta\sum_\mu\frac{\partial E}{\partial V_j^\mu}\frac{\partial V_j^\mu}{\partial w_{jk}}=\eta\sum_{\mu i}[\zeta_i^\mu-O_i^\mu]g'(h_i^\mu)W_{ij}g'(h_j^\mu)\xi_k^\mu
\\ \\&=\eta\sum_{\mu i}\delta_i^\mu W_{ij}g'(h_j^\mu)\xi_k^\mu=\eta\sum_\mu\delta_j^\mu\xi_k^\mu
\end{split}$$

$$\delta_j^\mu=g'(h_j^\mu)\sum_i\delta_i^\mu W_{ij}$$

Esta regra também pode ser estendida para redes com mais de uma camada oculta[@rateke1999]. A regra delta generalizada para a m-ésima camada de uma rede pode ser escrita como:

$$\Delta w_{pq}^m=\eta\sum_\mu\delta_p^{m,\mu}V_q^{m-1,\mu}$$
$$\delta_p^{M,\mu}=[\zeta_p^\mu-O_p^\mu]g'(h_p^{M,\mu}) \text{, para camada de saida } m=M$$
$$\delta_p^{m,\mu}=g'(h_p^{m,\mu})\sum_r\delta_r^{m+1,\mu}w_{rp}^{m+1} \text{, para m<M}$$
A correção dos pesos ocorre considerando as conexões entre cada duas camadas, uma mais próxima da saída ($p$) e a outra mais interna ($q$). O vetor $V_q$ representa o sinal de ativação recebido pela camada dos nós “$p$”, e quando o cálculo envolve a camada de entrada e a primeira camada oculta este vetor é o padrão de entrada ($\xi_k^\mu$).  O fator delta ($\delta$) funciona como uma memória das respostas das camadas mais externas, ou seja, para modificar os pesos de trás para frente é necessário que as conexões das camadas mantenham memória das camadas que foram alteradas anteriormente.
O algoritmo do backpropagation é utilizado na etapa de treinamento pelo programa por meio do método “backprop”:

```python
backprop(self, x, y):
  nabla_b = [np.zeros(b.shape) for b in self.biases]
  nabla_w = [np.zeros(w.shape) for w in self.weights]
  # feedforward
  activation = x
  activations = [x] 
  zs = [] 
  for b, w in zip(self.biases, self.weights):
 	  z = np.dot(w, activation)+b
    zs.append(z)
    activation = sigmoid(z)
    activations.append(activation)
  # backward pass
  delta = (self.cost).delta(zs[-1], activations[-1], y)
  nabla_b[-1] = delta
  nabla_w[-1] = np.dot(delta, activations[-2].transpose())
  for l in range(2, self.num_layers):
    z = zs[-l]
    sp = sigmoid_prime(z)
    delta = np.dot(self.weights[-l+1].transpose(),delta) * sp
      nabla_b[-l] = delta
      nabla_w[-l] = np.dot(delta, 
      activations[-l-1].transpose())
  return (nabla_b, nabla_w)
```

Como destacado anteriormente, a primeira fase do backpropagation é o feedforward. Nesta etapa é recebido um padrão de entrada ($x$) e os pesos e bias inicializados aleatoriamente. Após o somatório das ponderações dos pesos e bias entre duas camadas, este valor é salvo no vetor “$zs$”, e o resultado da ativação deste valor é salvo em “activations”. A entrada da próxima camada é o sinal de ativação salvo em “actvivation”. Este processo ocorre da entrada até a camada de saída, salvando os sinais de ativação das camadas ocultas ($V_j$) em “activations”.
Na fase “backward pass”, calcula-se primeiro o delta ($\delta$) a partir da resposta da camada de saída salva como o último elemento do vetor “activations” e do padrão de saída esperado ($y$). O valor de delta neste caso, é calculado a partir do método “delta” da classe “QuadraticCost” como o produto entre a diferença da resposta de saída de rede ($a$) e do valor esperado ($y$) com a derivada do sinal de ativação da última camada:

```python
delta(z, a, y):
  return (a-y) * sigmoid_prime(z)
```

Após o cálculo do primeiro delta é determinado o incremento dos pesos ($\Delta W_{ij}$) entre a última camada e a camada oculta como o produto do delta ($\sigma_i$) pelo vetor de ativação ($V_j$) que a ultima camada recebeu como entrada. Os incrementos dos pesos são salvos no vetor “nabla_w”. Os deltas e incrementos dos pesos das camadas ocultas são obtidos de forma iterativa na estrutura de repetição. O cálculo do delta da camada m depende do somatório  dos produtos do delta calculado anteriormente, da camada mais externa, com o vetor peso da camada m. O valor do somatório é multiplicado pela derivada do sinal de ativação da camada m. Em seguida, o valor do incremento dos pesos é obtido pelo produto do delta atual com o valor de ativação recebido pela camada m. Após realizar este mesmo processo para todas as camadas, a função retorna um vetor com os incrementos dos pesos com base em um padrão ($\xi_k^\mu,\zeta_i^\mu$), o que ocorre para todos os padrões de treinamento.

Para acelerar o processo de aprendizagem, em vez de atualizar os pesos cada vez que se apresenta um padrão, o autor Michael Nielsen sugere no seu exemplo a utilização do método gradiente descendente estocástico.  A ideia é agrupar de forma aleatória os padrões de entrada formando o que ele chama de “mini-batch”. No método “update_mini_batch”, a função “backprop” retorna o incremento do peso calculado para cada padrão dentro do agrupamento, e estes são somados em “nabla-w” até todo o agrupamento ser apresentado, e então os pesos e os bias são ajustados. Em seguida são apresentados os outros “mini-batch” até que todo o conjunto de treinamento seja utilizado, encerrando uma época de treinamento. Ou seja, em cada época o conjunto de treinamento é subdividido em agrupamentos, e os pesos são atualizados apenas no final de apresentação de cada agrupamento como demonstrado a seguir: 

```python
update_mini_batch(self, mini_batch, eta, lmbda, n):
  nabla_b = [np.zeros(b.shape) for b in self.biases]
  nabla_w = [np.zeros(w.shape) for w in self.weights]
  for x, y in mini_batch:
    delta_nabla_b, delta_nabla_w = self.backprop(x, y)
    nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
  self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]
  self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]
```

O treinamento ocorre a partir do método  “SGD”, sigla para descida do gradiente estocástico. Em que são passados como parâmetros o conjunto de treinamento, o número de épocas, o tamanho do agrupamento (mini_batch_size) e a taxa de aprendizagem.

```python
net.SGD(training_data,30,10,0.5, evaluation_data=test_data,monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_accuracy=True, monitor_training_cost=True)
```

É no método “SGD” que ocorre a subdivisão dos padrões de treinamento em agrupamentos “mini_batch”. Em seguida é chamada a função “update_mini_batch” para cada agrupamento, até finalizar uma época, e este processo se repete para todas as épocas.

```python
SGD(self, training_data, epochs, mini_batch_size, eta,lmbda = 0.0 evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False):
  for j in range(epochs):
    random.shuffle(training_data)
    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]
    for mini_batch in mini_batches:
      self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))
    print ("Epoch %s training complete" % j)
```

Dentro do método “SGD” é possível configurar para avaliar o erro total e acurácia da rede após cada época de treinamento, tanto considerando os dados de treinamento quanto os dados de teste ou de validação. Para selecionar ou os dados de teste ou de validação, eles devem ser passados como parâmetros no “evaluation_data”. Ao selecionar as opções “monitor_evaluation_cost” ou “monitor_training_cost” é chamado o método “ total_cost” que retorna a soma dos erros avaliados para todo o conjunto de dados. 

```python
total_cost(self, data, lmbda, convert=False):
  cost = 0.0
  for x, y in data:
    a = self.feedforward(x)
    if convert: y = vectorized_result(y)
    cost += self.cost.fn(a, y)/len(data)
	return cost
```

Após cada época se estabelece um conjunto de pesos e bias, e ao utilizar o comando “feedforward” são estes parâmetros que definem a resposta de saída da rede ($a$) para cada padrão de entrada ($x$). Ao comparar a resposta ($a$) com o valor esperado ($y$) dentro da função custo MSE, método “fn” da classe “QuadraticCost”, quantifica-se o erro para cada padrão. 
O método “accuracy” é utilizado dentro do “SGD” quando se configura “monitor_evaluation_accuracy= True” ou “monitor_training_accuracy= True”. Esta função retorna a soma de resultados em que os valores de saída da rede corresponderam ao valor esperado ($y$). O sinal da rede é calculado pela função “feedforward”, que é utilizada para cada valor ($x$) do conjunto de dados, seja de treinamento ou de avaliação

```python
accuracy(self, data, convert=False):
  results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]
  return sum(int(x == y) for (x, y) in results)
```

Considerando que os valores do erro total e da acurácia são calculados para cada época, o método de treinamento “SGD” retorna quatro vetores dentro de uma tupla, cada um com o número de posições correspondentes ao número total de épocas. Assim, se o treinamento ocorrer em 30 épocas, então a primeira lista da tupla terá 30 elementos correspondentes ao custo total dos dados de avaliação no final de cada época.

```python
return evaluation_cost, evaluation_accuracy,training_cost, training_accuracy

```

Os resultados salvos podem ser plotados em gráficos para avaliar visualmente o desempenho da rede. Um gráfico muito comum para acompanhar o treinamento da rede é o de custo de treinamento, principalmente porque o aprendizado é guiado pela minimização desta curva. Na Figura \@ref(fig:cust30) se apresenta a curva de custo para uma configuração que utiliza o conjunto total de treinamento (50000 imagens) e com 30 épocas. Entretanto, não é indicado ter apenas este gráfico como base para estabelecer os hiperparâmetros da rede, como a taxa de aprendizagem e o número de épocas de treinamento. Por exemplo, a Figura \@ref(fig:cust100) também se refere a uma função de custo, mas para uma outra configuração de treinamento, que utiliza apenas 1000 imagens para treinamento e 100 épocas.

(ref:cust30) Curva de custo no treinamento com 30 épocas e 50000 imagens.

```{r cust30, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cust30)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/custo_30.jpg'))
```

(ref:cust100) Curva de custo no treinamento com 100 épocas e 1000 imagens.

```{r cust100, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cust100)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/custo_100.jpg'))
```

No fim do treinamento, as duas redes apresentaram erros na mesma ordem de grandeza, mas a capacidade de reconhecer números é bem diferente entre as duas. Esta diferença pode ser percebida ao comparar os gráficos de acurácia(Figuras \@ref(fig:acur30) e \@ref(fig:acur100)) considerando tanto os dados de treinamento quanto os de validação. O resultado do treinamento com todo o conjunto de dados mostra que a acurácia da rede para os dados de validação é bem próxima do resultado para os valores de treinamento, uma diferença de 1%. Já para a situação que utilizou apenas 1000 dados de treinamento, as curvas de acurácia para os dados de validação e de treinamento estão mais afastadas, apresentando uma diferença próxima de 14%.

(ref:acur30) Curvas de acurácia para rede treinada com 30 épocas e 1000 imagens.

```{r acur30, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:acur30)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/acuracia_30.jpg'))
```

(ref:acur100) Curvas de acurácia para rede treinada com 100 épocas e 1000 imagens.

```{r acur100, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:acur100)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/acuracia_100.jpg'))
```

Ao observar apenas a curva de erro se imagina que a rede esteja aprendendo até o final do treinamento, visto que o erro continua diminuindo. Entretanto, ao analisar as curvas de acurácia se identifica que a acurácia determinada pelos dados de validação aumenta rapidamente até uma determinada época, próximo de 40 no segundo caso, e em seguida fica estagnada. Assim, após a 40° época, a rede não está mais aprendendo a generalizar para os dados de validação, está ocorrendo "overfitting", ou seja, o treinamento não está melhorando a capacidade da rede. Mesmo que a acurácia do treinamento esteja aumentando depois desta época, pode ser que a rede esteja apenas decorando os dados de treinamento, pois não está mais se atendo apenas às informações gerais, necessárias para  reconhecer os números de forma geral[@nielsen2015]. 

Os casos mais comuns de se ocorrer "overfitting" é quando o número de dados do treinamento é muito baixo, como neste segundo caso com apenas 1000 imagens. Nesta situação, a rede tem poucos exemplos para extrair informações gerais, precisando muitas vezes aumentar o número de épocas de treinamento para que se alcance um desempenho mínimo. Quanto maior o número de épocas pode ser mais evidente o efeito de “overfiting”, por isso se recomenda observar quando a acurácia da validação começa a estagnar e a ficar muito distante da curva de treinamento[@nielsen2015].  

Observar o comportamento da acurácia da validação é um dos métodos para definir até quando a rede deve ser treinada, ou seja, o número de épocas. Os dados de validação ajudam no teste de diferentes configurações de hiperparâmetros da rede, como épocas de treinamento, taxa de aprendizado e número de nós. Só depois de definir estes parâmetros e treinar a rede que se recomenda a utilização dos dados de teste para verificar realmente a acurácia da rede, utilizando dados que ela ainda não teve contato[@nielsen2015]. Um teste com dados não conhecidos permite verificar se os parâmetros da rede podem ser aplicados em casos mais gerais ou se enquadram apenas em particularidades dos dados treinados. Por esta razão, na maioria dos casos os dados são divididos em três conjuntos - treinamento, validação e teste. 

## Redes neurais convolucionais(CNN)
A área de deep learning tem conseguido ótimo desempenho em aplicações, principalmente pelo desenvolvimento da área e pelo aumento do poder computacional e da quantidade de dados disponíveis[@geron2019, p.431]. Um dos tipos de redes neurais, conhecido como Redes Neurais Convolucionais(em inglês Convolutional Neural Networks(CNN)) tem também conseguido resultados ótimos, um dos motivos pelos quais foram adotadas em grande peso pela área de Visão Computacional, substituindo muitas das técnicas antigas, que por utilizarem algoritmos mais "estáticos" eram difíceis de serem aplicados em diferente áreas. 

### Blocos de construção de uma CNN
Nas redes neurais mais simples usamos basicamente neurônios e conexões entre eles para realizar a construção de um modelo. As redes neurais convolucionais contam com algumas estruturas a mais que são a chave de sua eficiência no trabalho com imagens, veremos elas a seguir.

#### Operador de convolução
A operação que dá nome a rede, a convolução é, como vista no tópico X, uma operação realizada entre duas funções. No nosso caso, como estamos trabalhando com imagens, usamos a convolução discreta.

Um ponto importante a se frisar é que matematicamente o que chamaremos de convolução é na verdade uma correlação, sendo que as duas são quase idênticas, a não ser pelo fato de que na convolução giramos o filtro(kernel) em 180º. A única vantagem que ganhamos em girar o filtro antes da operação é que ganhamos a propriedade comutativa, o que é útil matematicamente para derivação de provas mas não é importante na implementação de deep learning[@goodfellow2016, p.332].

Na literatura e nas bibliotecas de deep learning, incluindo CNNs, se tornou comum chamar as duas operações de convolução[@goodfellow2016, p.333], então aqui também usaremos essa convenção, utilizando a convolução sem girar o filtro(sendo então, uma correlação).

Relembrando do tópico X, a fórmula da correlação discreta é dada por:

$$g(x,y)=w(x,y)\bigstar f(x,y)=\sum_{s=-a}^a\sum_{t=-b}^bg(s,t)f(x+s,y+t)$$

Onde w é o nosso filtro(kernel) e f a nossa imagem. E relacionado a ela temos a fórmula da convolução:

$$g(x,y)=w(x,y)\ast f(x,y)=\sum_{s=-a}^a\sum_{t=-b}^bg(s,t)f(x-s,y-t)$$

Como podemos perceber observando as duas equações, essas operações são bem simples, sendo basicamente uma soma de produtos. Na figura \@ref(fig:cnnconv) temos uma representação de uma passo da convolução, onde podemos observar a seguinte operação:

$$
\text{w}\text{*f}\left(0,0\right)\text{=}\sum_{s}^{}\sum_{t}^{}\text{w}\left(s,t\right)\text{f}\left(0+s,0+t\right)\,\text{=}\,\\
\text{+w}\left(-1,-1\right)\text{f}\left(-1,-1\right)\text{+w}\left(-1,0\right)\text{f}\left(-1,0\right)\text{+w}\left(-1,1\right)\text{f}\left(-1,1\right)\\
\text{+w}\left(0,-1\right)\text{f}\left(0,-1\right)\text{+w}\left(0,0\right)\text{f}\left(0,0\right)\text{+w}\left(0,1\right)\text{f}\left(0,1\right)\\
\text{+w}\left(1,-1\right)\text{f}\left(1,-1\right)\text{+w}\left(0,1\right)\text{f}\left(0,-1\right)\text{+w}\left(1,1\right)\text{f}\left(-1,-1\right)\\
=\,1\cdot0+0\cdot0+\left(-1\right)\cdot0\\
+2\cdot0+0\cdot2+\left(-2\right)\cdot1\\
+1\cdot0+0\cdot9+\left(-1\right)\cdot3\\
=0\,-2-3\,=\,-5
$$
(ref:cnnconv) Convolução de uma imagem de tamanho 5x5 com um kernel de tamanho 3x3 e seu respectivo resultado.

```{r cnnconv, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnconv)', fig.align='center', out.width='85%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_conv.png'))
```

O exemplo anterior(figura \@ref(fig:cnnconv)) foi bem simples, mas sabemos que em várias aplicações não teremos a imagem de entrada representada por apenas uma matriz(configurando uma imagem em tons de cinza) mas em grande parte das vezes estaremos utilizando imagens que contém 3 dimensões, ou seja, teremos uma imagem no modelo RGB, onde estarão presentes três matrizes, cada uma representando um canal de cor. Na figura \@ref(fig:cnnconv3d) há um exemplo de convolução em imagens RGB, podemos ver que agora nosso kernel é também formado por três matrizes. Uma coisa importante a se notar é que o número de camadas do filtro têm que ser igual ao número de canais da imagem para que a operação de convolução possa ser feita.

(ref:cnnconv3d) Convolução de uma imagem de tamanho 5x5x5 com um kernel de tamanho 3x3x3 e seu respectivo resultado.

```{r cnnconv3d, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnconv3d)', fig.align='center', out.width='85%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_conv_3d.png'))
```

Na figura \@ref(fig:cnnconvblock) temos uma representação de uma camada de convolução com mais de um filtro, para cada um deles temos uma saída e como vemos, temos no resultado final um conjunto de dados onde o número de camadas de profundidade(também conhecidas como feature map) corresponderão ao número de filtros aplicados a entrada. Essa saída então pode ser enviada para frente na rede, passando mais vezes por convolução e tendo mais características extraídas.

(ref:cnnconvblock) Convolução com múltiplos kernels

```{r cnnconvblock, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnconvblock)', fig.align='center', out.width='85%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_conv_block.png'))
```

Os dois exemplos anteriores(figura \@ref(fig:cnnconv) e \@ref(fig:cnnconvblock)) também servem para nos mostrar uma das características da convolução que a fazem ser uma boa escolha para se trabalhar com imagens, chamamos essa característica de iterações esparsas(também conhecida como conectividade esparsa)[@goodfellow2016, p.335]. Esse atributo evidencia o fato de que cada unidade da saída, ou pixel, é conectada a somente uma fração das unidades de entrada, no nosso exemplo anterior cada saída é conectada a uma região de 9x9x3=243 pixels da entrada. Isso é muito útil, pois nossa imagem pode ter milhões de pixels, e usando kernels de tamanhos menores, conseguimos detectar pequenas características, como bordas, quinas, etc[@goodfellow2016, p.335]. Nas camadas de convolução, os valores que a rede deverá aprender são os valores presentes nos filtros, então dessa maneira teremos menos parâmetros para aprender e armazenar. Em uma rede neural simples, como vimos no tópico x, uma imagem na entrada significa que cada pixel seria conectado a cada neurônio na próxima camada, resultando assim em uma rede excessivamente grande.

Uma outra característica importante é o de compartilhamento de parâmetros, já que o mesmo filtro é aplicado a diferentes regiões da imagem utilizando os mesmos valores, diferentemente de uma rede neural sem camadas de convolução, onde temos uma matriz com pesos que são usados para somente uma conexão. O compartilhamento de parâmetros nos proporciona uma outra característica, que é a invariância à translação, isso quer dizer que se movemos a posição de um objeto na imagem de entrada, sua representação também será movida na imagem resultante[@goodfellow2016, p.339].

##### Padding
Nos exemplos de convolução(fig x e x) vemos que conforme aplicamos o kernel na imagem de entrada, o tamanho da imagem de saída é reduzido. De fato, se convolucionais uma imagem de tamanho m x n com um filtro de tamanho $k_m x k_n$ a imagem resultante terá uma altura de $m - k_m + 1$ e um comprimento de $n - k_n + 1$. Esse tipo de convolução, onde a imagem resultante é menor geralmente é chamada de "valid"(válida). 

Se queremos a imagem de saída com o mesmo tamanho da imagem de entrada, temos que adicionar mais linhas e colunas em nossa imagem, isso é conhecido como padding. Nesse caso utilizamos a fórmula $m + 2p - k_m + 1$ e $ n + 2p - k_n + 1$ onde p representa o padding. Por exemplo, nas figuras anteriores(fig x e x), se quiséssemos uma saída de igual tamanho a entrada, teríamos que utilizar um padding de $6 + 2p - 3 + 1 = 6 \Rightarrow p = 1$.

##### Stride
Os exemplos de convolução que vimos anteriormente utilizavam passos de deslocamento de um em um, mas podemos também utilizar passos maiores, pois assim reduzimos o custo computacional ao realizar esses passos intervalados. Isso, claro, tem um impacto no resultado final, diminuindo sua resolução, mas em casos onde não precisamos extrair características finas isso se torna uma boa opção[@goodfellow2016, p.348].

Quando utilizamos um valor de stride maior que um, isso também afetará o tamanho da saída, e será governado pela seguinte relação[@adrian2017, p.184]:

$$\frac{m+2p-k_m}{s}+1 \  \times \ \frac{n+2p-k_n}{s}+1$$

Onde m e n são as dimensões da imagem, $p$ é o padding, $k_m$ e $k_n$ as dimensões do kenel e $s$ o stride. Na figura \@ref(fig:cnnstride) temos os um exemplo com os passos(Figura \@ref(fig:cnnstride) a-d) de um convolução com $\text{stride} = 2$ utilizando um kernel de tamanho 3x3 sobre uma imagem de tamanho 5x5 e $\text{padding} = 0$.

(ref:cnnstride) Representação de uma convolução com stride = 2. Adaptado de [@dumoulin2016].

```{r cnnstride, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnstride)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_stride.png'))
```

#### Pooling
Essa é uma camada muito importante, que tem como objetivo realizar a subamostragem(subsampling) da imagem, para reduzir seu tamanho, e consequentemente diminuir o total de memória, processamento e parâmetros necessários, além de refrear o risco de overfitting[@geron2019, p.442, @adrian2017, p.187, @elgendy2020, p.114].

Como nas camadas de convolução, cada unidade da saída é conectada a uma região de entrada, então também devemos levar em consideração o tamanho, stride e padding. Mas, diferentemente da convolução, o “kernel” ou em outras palavras, a região que nos conectará com a entrada, não terá pesos mas apenas realiza uma operação, sendo as mais comuns o máximo ou a média[@geron2019, p.442].

Na figura \@ref(fig:cnnpooling) temos um exemplo de max pooling onde podemos ver seu funcionamento nos passos(Figura \@ref(fig:cnnpooling)a-d). Este exemplo utiliza uma região de 2x2, o que é muito comum[@adrian2017, p.187], com um stride de 1.

(ref:cnnpooling) Max pooling

```{r cnnpooling, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnpooling)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_pooling_max.png'))
```

Na Figura \@ref(fig:cnnpooling3d) temos outro exemplo de max pooling, mas desta vez realizado com uma entrada de maiores dimensões, podemos ver que a operação é realizada em cada uma das camadas do objeto de entrada, e que sua saída contém o mesmo número de camadas da entrada, sendo que é isso que tipicamente ocorre nesse tipo de operação[@geron2019, p.443].

(ref:cnnpooling3d) Max pooling em mais dimensões

```{r cnnpooling3d, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnpooling3d)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_pooling_max_3d.png'))
```

Apesar do pooling ser uma técnica muito difundida, podemos encontrar redes onde seus autores preferiram não utilizar pooling para realizar a subamostragem, mas utilizarem camadas de convolução com valores de stride e padding maiores para conseguir essa redução de dimensão[@elgendy2020, p.117, @adrian2017, p.188]. Essa maneira de trabalhar foi proposta por Springenberg et al. em seu artigo “Striving for Simplicity: The All Convolutional Net” de 2014, onde demonstram que mesmo redes sem camadas de pooling podem ter resultados bons em diferentes bases de dados, como o CIFAR-10 e ImageNet.

#### Camadas totalmente conectadas
As CNN’s geralmente tem várias camadas de convolução seguidas por camadas de ReLU que por sua vez são seguidas por camadas de pooling, e esse processo vai diminuindo as dimensões mxn e aumentando a profundidade, ou seja, a quantidade de camadas de características(conhecido como feature maps)[@elgendy2020, p.119, @geron2019, p.446]. Na Figura \@ref(fig:cnntypical) temos uma representação desse processo através da topologia da rede, e ao chegar ao final temos uma quantidade grande de camadas com as características extraídas da imagem de entrada e precisamos utilizar essas informações. Nessa mesma figura podemos ver que no final temos camadas totalmente conectadas(Fully connected) que é uma rede neural regular, uma MLP[@elgendy2020, p.119].

(ref:cnntypical) Típica arquitetura de uma rede neural convolucional[@geron2019, p.447]

```{r cnntypical, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnntypical)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_typical.png'))
```

Na Figura \@ref(fig:cnnfc) temos dessa última etapa, onde recebemos o resultado das camadas de convolução, neste caso um bloco de dados de 5x5x40 que é então planificado(flattened) em um vetor contínuo de uma dimensão e dado como entrada a uma rede MLP que ao final tem uma camada Softmax que faz a classificação da imagem de entrada.

(ref:cnnfc) Camada totalmente conectada[@elgendy2020, p.120]

```{r cnnfc, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:cnnfc)', fig.align='center', out.width='80%'}
knitr::include_graphics(rep('imagens/07-deepLearning/cnn_fc.png'))
```

### Por que usar convoluções
Até agora entendemos os blocos de construção das CNN’s e os motivos pelos quais são usados. Devemos saber que a convolução não é apenas usada por ser mais eficiente no tratamento de imagens mas também que a ideia de utilizá-la teve inspiração em nosso próprio sistema visual.

#### Córtex visual
Como as próprias redes neurais, as CNN's foram bio-inspiradas em estudos sobre o córtex visual do cérebro que começaram a ocorrer desde 1980[@geron2019, p.431], principalmente a partir dos trabalhos de David H. Hubel e Torsten Wiesel, onde foram realizados experimentos em animais, que permitiram aos dois pesquisadores deduzirem o funcionamento da estrutura do córtex visual.

De uma maneira simplificada, os sinais de luz recebidos pela retina são transmitidos ao cérebro através do nervo óptico, após isso chegam ao córtex visual primário que é formado principalmente por dois tipos de células[@goodfellow2016, p.365]:

- Células simples: essas células têm comportamentos que podem ser representados por funções lineares em uma imagem em uma pequena área conhecida como campo receptivo[@goodfellow2016, p.365, @geron2019, p.432]. Esse tipo de célula inspirou as unidades detectoras mais simples nas CNNs.

- Células complexas: também respondem a características da imagem, como as células simples, mas são invariantes a posição, ou seja, não fazem grande distinção de onde a característica aparece. Esse tipo de célula inspirou as unidades de pooling, que veremos mais adiante[@goodfellow2016, p.365].

Anatomicamente, quanto mais nos aprofundamos nas camadas do cérebro, mais camadas análogas à convolução e pooling são passadas, e encontramos células mais especializadas que respondem a padrões específicos sem serem afetadas por transformações na entrada. Sendo que até chegar nessas camadas mais profundas, é realizado uma sequência de detecções seguidas de camadas de pooling[@goodfellow2016, p.365].
