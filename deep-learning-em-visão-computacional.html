<!DOCTYPE html>
<html lang="pt-BR" xml:lang="pt-BR">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Deep Learning em visão computacional | Material introdutório de Processamento Digital de Imagens e Visão Computacional</title>
  <meta name="description" content="Capítulo 7 Deep Learning em visão computacional | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Deep Learning em visão computacional | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Deep Learning em visão computacional | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  
  
  

<meta name="author" content="" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="segmentação.html"/>
<link rel="next" href="separação-plano-de-fundo.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="logo"><a href="./"><img src="imagens/logo.jpeg"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Início</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#relação-de-processamento-digital-de-imagem-visão-computacional-e-computação-gráfica"><i class="fa fa-check"></i><b>1.1</b> Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#aplicações-processamento-digital-de-imagens"><i class="fa fa-check"></i><b>1.2</b> Aplicações Processamento Digital de Imagens</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#etapas-do-processamento-e-análise-de-imagens"><i class="fa fa-check"></i><b>1.3</b> Etapas do Processamento e Análise de Imagens</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html"><i class="fa fa-check"></i><b>2</b> Formação da imagem</a><ul>
<li class="chapter" data-level="2.1" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#câmera-pinhole-e-geometria"><i class="fa fa-check"></i><b>2.1</b> Câmera <em>pinhole</em> e geometria</a></li>
<li class="chapter" data-level="2.2" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#lentes-finas"><i class="fa fa-check"></i><b>2.2</b> Lentes Finas</a></li>
<li class="chapter" data-level="2.3" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#sensor"><i class="fa fa-check"></i><b>2.3</b> Sensor</a></li>
<li class="chapter" data-level="2.4" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#amostragem-e-quantização"><i class="fa fa-check"></i><b>2.4</b> Amostragem e Quantização</a><ul>
<li class="chapter" data-level="2.4.1" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#amostragem"><i class="fa fa-check"></i><b>2.4.1</b> Amostragem</a></li>
<li class="chapter" data-level="2.4.2" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#quantização"><i class="fa fa-check"></i><b>2.4.2</b> Quantização</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#formacaoImg"><i class="fa fa-check"></i><b>2.5</b> Definição de imagem digital</a></li>
<li class="chapter" data-level="2.6" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#resolução-espacial-e-de-intensidade"><i class="fa fa-check"></i><b>2.6</b> Resolução espacial e de intensidade</a></li>
<li class="chapter" data-level="2.7" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#pixels"><i class="fa fa-check"></i><b>2.7</b> Pixels</a><ul>
<li class="chapter" data-level="2.7.1" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#vizin"><i class="fa fa-check"></i><b>2.7.1</b> Vizinhança</a></li>
<li class="chapter" data-level="2.7.2" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#contiv"><i class="fa fa-check"></i><b>2.7.2</b> Conectividade</a></li>
<li class="chapter" data-level="2.7.3" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#adja"><i class="fa fa-check"></i><b>2.7.3</b> Adjacência</a></li>
<li class="chapter" data-level="2.7.4" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#camin"><i class="fa fa-check"></i><b>2.7.4</b> Caminho</a></li>
<li class="chapter" data-level="2.7.5" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#componente-conexa"><i class="fa fa-check"></i><b>2.7.5</b> Componente Conexa</a></li>
<li class="chapter" data-level="2.7.6" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#borda-e-interior"><i class="fa fa-check"></i><b>2.7.6</b> Borda e Interior</a></li>
<li class="chapter" data-level="2.7.7" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#medidas-de-distância"><i class="fa fa-check"></i><b>2.7.7</b> Medidas de Distância</a></li>
<li class="chapter" data-level="2.7.8" data-path="formação-da-imagem.html"><a href="formação-da-imagem.html#operações-lógico-aritméticas"><i class="fa fa-check"></i><b>2.7.8</b> Operações Lógico-aritméticas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html"><i class="fa fa-check"></i><b>3</b> Transformacões geométricas</a><ul>
<li class="chapter" data-level="3.1" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#definição"><i class="fa fa-check"></i><b>3.1</b> Definição</a></li>
<li class="chapter" data-level="3.2" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#sistema-de-coordenadas-objetos-2d-e-3d"><i class="fa fa-check"></i><b>3.2</b> Sistema de coordenadas objetos (2D e 3D)</a></li>
<li class="chapter" data-level="3.3" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#representação-vetorial-e-matricial-de-imagens-digitalizadas"><i class="fa fa-check"></i><b>3.3</b> Representação Vetorial e Matricial de Imagens digitalizadas</a></li>
<li class="chapter" data-level="3.4" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#matrizes-em-computação-gráfica"><i class="fa fa-check"></i><b>3.4</b> Matrizes em Computação gráfica</a></li>
<li class="chapter" data-level="3.5" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#transformações-em-pontos-e-objetos"><i class="fa fa-check"></i><b>3.5</b> Transformações em Pontos e Objetos</a></li>
<li class="chapter" data-level="3.6" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#transformação-de-translação"><i class="fa fa-check"></i><b>3.6</b> Transformação de Translação</a></li>
<li class="chapter" data-level="3.7" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#transformação-de-escala"><i class="fa fa-check"></i><b>3.7</b> Transformação de Escala</a></li>
<li class="chapter" data-level="3.8" data-path="transformacões-geométricas.html"><a href="transformacões-geométricas.html#transformação-de-rotação"><i class="fa fa-check"></i><b>3.8</b> Transformação de Rotação</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html"><i class="fa fa-check"></i><b>4</b> Transformações radiométricas</a><ul>
<li class="chapter" data-level="4.1" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#transformação-linear"><i class="fa fa-check"></i><b>4.1</b> Transformação Linear</a></li>
<li class="chapter" data-level="4.2" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#transformação-logarítmica"><i class="fa fa-check"></i><b>4.2</b> Transformação Logarítmica</a></li>
<li class="chapter" data-level="4.3" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#transformação-de-potência"><i class="fa fa-check"></i><b>4.3</b> Transformação de Potência</a></li>
<li class="chapter" data-level="4.4" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#processamento-de-histograma"><i class="fa fa-check"></i><b>4.4</b> Processamento de histograma</a></li>
<li class="chapter" data-level="4.5" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#equalização-do-histograma"><i class="fa fa-check"></i><b>4.5</b> Equalização do histograma</a></li>
<li class="chapter" data-level="4.6" data-path="transformações-radiométricas.html"><a href="transformações-radiométricas.html#especificação-de-histograma"><i class="fa fa-check"></i><b>4.6</b> Especificação de histograma</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="filtros-digitais.html"><a href="filtros-digitais.html"><i class="fa fa-check"></i><b>5</b> Filtros Digitais</a><ul>
<li class="chapter" data-level="5.1" data-path="filtros-digitais.html"><a href="filtros-digitais.html#convolução"><i class="fa fa-check"></i><b>5.1</b> Convolução</a><ul>
<li class="chapter" data-level="5.1.1" data-path="filtros-digitais.html"><a href="filtros-digitais.html#definção-matemática-da-convolução"><i class="fa fa-check"></i><b>5.1.1</b> Definção matemática da convolução</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="filtros-digitais.html"><a href="filtros-digitais.html#filtro-da-média"><i class="fa fa-check"></i><b>5.2</b> Filtro da Média</a></li>
<li class="chapter" data-level="5.3" data-path="filtros-digitais.html"><a href="filtros-digitais.html#filtro-da-mediana"><i class="fa fa-check"></i><b>5.3</b> Filtro da Mediana</a></li>
<li class="chapter" data-level="5.4" data-path="filtros-digitais.html"><a href="filtros-digitais.html#filtro-gaussiano"><i class="fa fa-check"></i><b>5.4</b> Filtro Gaussiano</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="segmentação.html"><a href="segmentação.html"><i class="fa fa-check"></i><b>6</b> Segmentação</a><ul>
<li class="chapter" data-level="6.1" data-path="segmentação.html"><a href="segmentação.html#detecção-por-descontinuidade"><i class="fa fa-check"></i><b>6.1</b> Detecção por descontinuidade</a><ul>
<li class="chapter" data-level="6.1.1" data-path="segmentação.html"><a href="segmentação.html#detecção-de-pontos-isolados"><i class="fa fa-check"></i><b>6.1.1</b> Detecção de pontos isolados</a></li>
<li class="chapter" data-level="6.1.2" data-path="segmentação.html"><a href="segmentação.html#detecção-de-linhas"><i class="fa fa-check"></i><b>6.1.2</b> Detecção de linhas</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="segmentação.html"><a href="segmentação.html#detecção-de-bordas"><i class="fa fa-check"></i><b>6.2</b> Detecção de Bordas</a><ul>
<li class="chapter" data-level="6.2.1" data-path="segmentação.html"><a href="segmentação.html#modelos-de-bordas"><i class="fa fa-check"></i><b>6.2.1</b> Modelos de Bordas</a></li>
<li class="chapter" data-level="6.2.2" data-path="segmentação.html"><a href="segmentação.html#método-do-gradiente-roberts-prewitt-sobel"><i class="fa fa-check"></i><b>6.2.2</b> Método do gradiente ( Roberts, Prewitt, Sobel)</a></li>
<li class="chapter" data-level="6.2.3" data-path="segmentação.html"><a href="segmentação.html#método-de-marr-hildreth"><i class="fa fa-check"></i><b>6.2.3</b> Método de Marr-Hildreth</a></li>
<li class="chapter" data-level="6.2.4" data-path="segmentação.html"><a href="segmentação.html#método-de-canny"><i class="fa fa-check"></i><b>6.2.4</b> Método de Canny</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="segmentação.html"><a href="segmentação.html#transformada-de-hough"><i class="fa fa-check"></i><b>6.3</b> Transformada de Hough</a><ul>
<li class="chapter" data-level="6.3.1" data-path="segmentação.html"><a href="segmentação.html#transformada-de-hough-para-detecção-de-linhas"><i class="fa fa-check"></i><b>6.3.1</b> Transformada de Hough para detecção de linhas</a></li>
<li class="chapter" data-level="6.3.2" data-path="segmentação.html"><a href="segmentação.html#transformada-de-hough-para-detecção-de-círculos"><i class="fa fa-check"></i><b>6.3.2</b> Transformada de Hough para detecção de círculos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="segmentação.html"><a href="segmentação.html#detecção-de-quinas"><i class="fa fa-check"></i><b>6.4</b> Detecção de Quinas</a><ul>
<li class="chapter" data-level="6.4.1" data-path="segmentação.html"><a href="segmentação.html#detector-de-quinas-de-moravec"><i class="fa fa-check"></i><b>6.4.1</b> Detector de Quinas de Moravec</a></li>
<li class="chapter" data-level="6.4.2" data-path="segmentação.html"><a href="segmentação.html#detector-de-quinas-de-harris"><i class="fa fa-check"></i><b>6.4.2</b> Detector de Quinas de Harris</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="segmentação.html"><a href="segmentação.html#detecção-de-blobs"><i class="fa fa-check"></i><b>6.5</b> Detecção de Blobs</a><ul>
<li class="chapter" data-level="6.5.1" data-path="segmentação.html"><a href="segmentação.html#log"><i class="fa fa-check"></i><b>6.5.1</b> LoG</a></li>
<li class="chapter" data-level="6.5.2" data-path="segmentação.html"><a href="segmentação.html#dog"><i class="fa fa-check"></i><b>6.5.2</b> DoG</a></li>
<li class="chapter" data-level="6.5.3" data-path="segmentação.html"><a href="segmentação.html#doh"><i class="fa fa-check"></i><b>6.5.3</b> DoH</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="segmentação.html"><a href="segmentação.html#limiarização"><i class="fa fa-check"></i><b>6.6</b> Limiarização</a><ul>
<li class="chapter" data-level="6.6.1" data-path="segmentação.html"><a href="segmentação.html#limiarização-global-simples"><i class="fa fa-check"></i><b>6.6.1</b> Limiarização global simples</a></li>
<li class="chapter" data-level="6.6.2" data-path="segmentação.html"><a href="segmentação.html#limiarização-pelo-método-de-otsu"><i class="fa fa-check"></i><b>6.6.2</b> Limiarização pelo Método de Otsu</a></li>
<li class="chapter" data-level="6.6.3" data-path="segmentação.html"><a href="segmentação.html#uso-de-suavização-para-limiarização"><i class="fa fa-check"></i><b>6.6.3</b> Uso de suavização para limiarização</a></li>
<li class="chapter" data-level="6.6.4" data-path="segmentação.html"><a href="segmentação.html#uso-de-bordas-para-limiarização"><i class="fa fa-check"></i><b>6.6.4</b> Uso de bordas para limiarização</a></li>
<li class="chapter" data-level="6.6.5" data-path="segmentação.html"><a href="segmentação.html#limiares-múltiplos"><i class="fa fa-check"></i><b>6.6.5</b> Limiares Múltiplos</a></li>
<li class="chapter" data-level="6.6.6" data-path="segmentação.html"><a href="segmentação.html#limiarização-variável"><i class="fa fa-check"></i><b>6.6.6</b> Limiarização variável</a></li>
<li class="chapter" data-level="6.6.7" data-path="segmentação.html"><a href="segmentação.html#particionamento-da-imagem"><i class="fa fa-check"></i><b>6.6.7</b> Particionamento da imagem</a></li>
<li class="chapter" data-level="6.6.8" data-path="segmentação.html"><a href="segmentação.html#limiarização-variável-baseada-nas-propriedades-locais-da-imagem"><i class="fa fa-check"></i><b>6.6.8</b> Limiarização variável baseada nas propriedades locais da imagem</a></li>
<li class="chapter" data-level="6.6.9" data-path="segmentação.html"><a href="segmentação.html#usando-média-de-movimento"><i class="fa fa-check"></i><b>6.6.9</b> Usando média de movimento</a></li>
<li class="chapter" data-level="6.6.10" data-path="segmentação.html"><a href="segmentação.html#limiarização-baseada-em-diversas-variáveis"><i class="fa fa-check"></i><b>6.6.10</b> Limiarização baseada em diversas variáveis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html"><i class="fa fa-check"></i><b>7</b> Deep Learning em visão computacional</a><ul>
<li class="chapter" data-level="7.1" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#caracterização-de-ai-machine-learning-e-deep-learning"><i class="fa fa-check"></i><b>7.1</b> Caracterização de AI, Machine Learning e Deep Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#aprendizado-supervisionado-e-não-supervisionado"><i class="fa fa-check"></i><b>7.1.1</b> Aprendizado supervisionado e não supervisionado</a></li>
<li class="chapter" data-level="7.1.2" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#redes-neurais-artificiais"><i class="fa fa-check"></i><b>7.1.2</b> Redes Neurais Artificiais</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#redes-neurais-convolucionaiscnn"><i class="fa fa-check"></i><b>7.2</b> Redes neurais convolucionais(CNN)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#blocos-de-construção-de-uma-cnn"><i class="fa fa-check"></i><b>7.2.1</b> Blocos de construção de uma CNN</a></li>
<li class="chapter" data-level="7.2.2" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#por-que-usar-convoluções"><i class="fa fa-check"></i><b>7.2.2</b> Por que usar convoluções</a></li>
<li class="chapter" data-level="7.2.3" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#redes-cnns-clássicas"><i class="fa fa-check"></i><b>7.2.3</b> Redes CNN’s clássicas</a></li>
<li class="chapter" data-level="7.2.4" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#aprendizado-por-transferência"><i class="fa fa-check"></i><b>7.2.4</b> Aprendizado por transferência</a></li>
<li class="chapter" data-level="7.2.5" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#redes-neurais-na-prática"><i class="fa fa-check"></i><b>7.2.5</b> Redes neurais na prática</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#redes-neurais-siamesas"><i class="fa fa-check"></i><b>7.3</b> Redes Neurais Siamesas</a><ul>
<li class="chapter" data-level="7.3.1" data-path="deep-learning-em-visão-computacional.html"><a href="deep-learning-em-visão-computacional.html#arquitetura"><i class="fa fa-check"></i><b>7.3.1</b> Arquitetura</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="separação-plano-de-fundo.html"><a href="separação-plano-de-fundo.html"><i class="fa fa-check"></i><b>8</b> Separação Plano de Fundo</a><ul>
<li class="chapter" data-level="8.1" data-path="separação-plano-de-fundo.html"><a href="separação-plano-de-fundo.html#fixo"><i class="fa fa-check"></i><b>8.1</b> Fixo</a></li>
<li class="chapter" data-level="8.2" data-path="separação-plano-de-fundo.html"><a href="separação-plano-de-fundo.html#média-temporal"><i class="fa fa-check"></i><b>8.2</b> Média Temporal</a></li>
<li class="chapter" data-level="8.3" data-path="separação-plano-de-fundo.html"><a href="separação-plano-de-fundo.html#mediana-temporal"><i class="fa fa-check"></i><b>8.3</b> Mediana Temporal</a></li>
<li class="chapter" data-level="8.4" data-path="separação-plano-de-fundo.html"><a href="separação-plano-de-fundo.html#exemplos-comparativos-da-média-temporal-média-espaço-temporal-e-mediana-temporal"><i class="fa fa-check"></i><b>8.4</b> Exemplos comparativos da Média Temporal, Média Espaço-Temporal e Mediana Temporal</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="refêrencias.html"><a href="refêrencias.html"><i class="fa fa-check"></i>Refêrencias</a></li>
<li class="divider"></li>
<li><center>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
</a></li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Material introdutório de Processamento Digital de Imagens e Visão Computacional</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning-em-visão-computacional" class="section level1">
<h1><span class="header-section-number">Capítulo 7</span> Deep Learning em visão computacional</h1>
<p>Antes de iniciarmos o estudo sobre deep learning, e mais especificamente sobre redes neurais artificiais convolucionais, é importante termos uma visão ampla sobre a área e suas subdivisões, para conseguirmos nos localizar em meio a essa área que cresce cada vez mais. Por isso começaremos falando um pouco sobre inteligência artificial e suas subdivisões, além de sua conexão e uso com visão computacional, que é o nosso foco.</p>
<div id="caracterização-de-ai-machine-learning-e-deep-learning" class="section level2">
<h2><span class="header-section-number">7.1</span> Caracterização de AI, Machine Learning e Deep Learning</h2>
<p>Esses três termos costumam causar certa confusão, principalmente em pessoas que estão começando a estudar essa área. De maneira geral, o termo Inteligência Artificial (IA) denomina uma área que possui muitas vertentes e tópicos de estudos, onde a maioria tem o foco em conseguir fazer os computadores realizarem tarefas complexas, que anteriormente eram realizadas exclusivamente por humanos.</p>
<p>No começo dos estudos sobre IA foram tentados e resolvidos muitos problemas que eram considerados difíceis para seres humanos, mas relativamente fáceis para os computadores<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 1]</span>. Esses eram problemas que podiam ser descritos formalmente, por meio de regras matemáticas, como exemplo temos o jogo de xadrez, onde, em 1997 o campeão Garry Kasparov perdeu para o IBM Deep Blue(Figura <a href="deep-learning-em-visão-computacional.html#fig:deepBlue">7.1</a>).</p>

<div class="figure" style="text-align: center"><span id="fig:deepBlue"></span>
<img src="imagens/07-deepLearning/deep-blue.jpg" alt="IBM Deep Blue [23]" width="55%" />
<p class="caption">
Figura 7.1: IBM Deep Blue <span class="citation">[<a href="#ref-img:deepblue" role="doc-biblioref">23</a>]</span>
</p>
</div>
<p>Com o tempo começamos a perceber que a dificuldade não residia nesses problemas, mas naqueles que são realizados facilmente, até instintivamente e intuitivamente pelos humanos, como reconhecer rostos familiares, entender linguagens, etc. A questão é que os seres humanos, no dia a dia, recebem e processam quantidades enormes de informações, e tentar fazer os computadores realizarem essas atividades somente com regras descritas por nós não era algo viável, por isso os pesquisadores começaram a desenvolver técnicas onde o próprio computador, através de algoritmos, aprendesse a retirar essas regras e informações sozinho de bases de dados brutos, a isso chamamos de Machine Learning(Aprendizado de Máquina).</p>
<p>Dentro da área de Machine Learning, temos um conjunto de técnicas e áreas de pesquisa, sendo que uma delas utiliza um modelo baseado na biologia de cérebros biológicos, contendo neurônios e conexões conhecidas como Redes Neurais. Na Figura <a href="deep-learning-em-visão-computacional.html#fig:cajalCortex">7.2</a> temos uma representação dos neurônios do córtex cerebral humano, onde podemos ver as conexões formadas por eles, que se assemelham com os modelos de redes neurais como o da Figura <a href="deep-learning-em-visão-computacional.html#fig:coloredNeuralNetwork">7.3</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:cajalCortex"></span>
<img src="imagens/07-deepLearning/cajal-cortex.png" alt="Representação da conexão de neurônios no córtex cerebral [24, p. 363]" width="90%" />
<p class="caption">
Figura 7.2: Representação da conexão de neurônios no córtex cerebral <span class="citation">[<a href="#ref-cajal" role="doc-biblioref">24</a>, p. 363]</span>
</p>
</div>
<p>Atualmente, como ouvimos muito se falar sobre IAs, temos a tendência de pensar que essa é uma técnica moderna, mas ao contrário, a ideia de fazer os computadores imitarem o esquema de funcionamento do cérebro remonta a 1943, quando Warren McCulloch e Walter Pitts sugeriram a ideia em seu artigo “A logical calculus of the ideas immanent in nervous activity”<span class="citation">[<a href="#ref-mcculloch1943" role="doc-biblioref">25</a>]</span>.</p>
<p>Como pode ser visto na Figura <a href="deep-learning-em-visão-computacional.html#fig:coloredNeuralNetwork">7.3</a>, as redes neurais são formadas por camadas, sendo que os dados entram pela camada de Input, são processadas nas camadas Hidden e temos os dados de saída na camada Output. Cada uma dessas camadas é formada por um número de neurônios(representados pelos círculos) e tem as conexões representadas pelas setas. Por enquanto não nos aprofundaremos tanto no funcionamento das redes neurais, que serão abordadas na seção x.</p>

<div class="figure" style="text-align: center"><span id="fig:coloredNeuralNetwork"></span>
<img src="imagens/07-deepLearning/colored-neural-network.png" alt="Rede neural artificial [26]" width="50%" />
<p class="caption">
Figura 7.3: Rede neural artificial <span class="citation">[<a href="#ref-img:coloredNeuralNetwork" role="doc-biblioref">26</a>]</span>
</p>
</div>
<p>Nos últimos anos, temos visto um leque de aplicações cada vez maior para as técnicas de IA. Nosso objetivo nesse material é introduzir, principalmente, o uso das Redes Neurais Artificiais na área da Visão Computacional, que é identificada como uma das subáreas da Inteligência Artificial pois busca reproduzir algumas das capacidades humanas a partir de sistemas autônomos. O principal interesse desta área é fazer com que computadores desempenhem funções semelhantes à visão humana, sendo capazes de receber dados visuais e com eles realizar reconhecimentos, classificações e análises. Análogo ao processo de aprendizado dos seres humanos, identifica-se que a melhora no desempenho da visão computacional está fortemente interligada com a evolução do aprendizado de máquina (machine learning), outro segmento da inteligência artificial.</p>
<p>Antes de entrarmos realmente no assunto de redes neurais, vamos apresentar, de maneira resumida, alguns tópicos principais da área de Machine Learning, pois como dito anteriormente, o deep learning e as redes neurais estão dentro dessa área, e o entendimento desses tópicos pode auxiliar no entendimento pleno dos tópicos futuros.</p>
<div id="aprendizado-supervisionado-e-não-supervisionado" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Aprendizado supervisionado e não supervisionado</h3>
<p>Dentro dos algoritmos de machine learning existe uma característica que os separa em diferentes tipos, baseado em sua forma de aprendizado, sendo os principais os algoritmos de aprendizado supervisionado e não supervisionado.</p>
<p>Na aprendizagem supervisionada os algoritmos têm previamente os pares entrada-saída, ou seja, para cada entrada já temos conhecimento prévio de como deve ser a saída<span class="citation">[<a href="#ref-russell2016" role="doc-biblioref">27</a>, p. 695]</span>, e a partir disso nosso algoritmo deve aprender a generalizar bem as entradas. Podemos formalizar isso da seguinte forma<span class="citation">[<a href="#ref-russell2016" role="doc-biblioref">27</a>, p. 695]</span>:</p>
<p>Dado um conjunto de treinamento de n pares <span class="math inline">\((x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)\)</span> onde <span class="math inline">\(x_i\)</span> são as entradas e <span class="math inline">\(y_i = f(x_i)\)</span> as saídas, nosso algoritmo deve descobrir uma função <span class="math inline">\(h\)</span>, conhecida como hipótese, que aproxime <span class="math inline">\(f\)</span>. Para sabermos se nossa hipótese aproxima bem <span class="math inline">\(f\)</span> após ter treinado o algoritmo, utilizamos um conjunto de testes - que contém exemplos diferentes do conjunto de treinamento - e avaliamos o quão bem o algoritmo generaliza(dá respostas corretas) as novas entradas.
Já na aprendizagem não supervisionada não há nenhum feedback para as saídas do algoritmo, ou seja, ele recebe somente os dados de entrada. Por isso, uma das principais tarefas designadas a esses tipos de algoritmos é a de clustering(agrupamento), onde o algoritmo aprende a encontrar padrões nos dados de entrada e separá-lo em grupos.</p>
</div>
<div id="redes-neurais-artificiais" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Redes Neurais Artificiais</h3>
<p>Parte da base teórica que fundamenta o aprendizado profundo surgiu inicialmente como modelos para entender o aprendizado, ou seja, como o cérebro funciona. Desta forma, estas teorias ficaram conhecidas como Redes Neurais, uma das áreas do aprendizado profundo que mais cresceram nos últimos anos <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 1]</span>. Atualmente, os conceitos de Redes Neurais abordam princípios mais genéricos além da perspectiva da neurociência. Mesmo que as Redes Neurais não sejam capazes de explicar muito sobre o cérebro, não podendo ser encaradas como modelos realistas da função biológica, vários aspectos do aprendizado ainda continuam sendo inspirações.</p>
<p>As redes foram pensadas para adquirir o conhecimento por um processo de aprendizagem. Semelhante ao que ocorre no cérebro, as interações entre os neurônios, ou pesos sinápticos, são responsáveis por armazenar o conhecimento. Em termos práticos, o conhecimento de uma rede seria a capacidade de uma máquina de realizar funções complexas de forma autônoma, como classificações e reconhecimentos de padrões. As redes também são capazes de generalizar a informação aprendida, extraindo características essenciais de exemplos e garantido respostas coerentes para novos casos<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 28]</span>.</p>
<p>Mesmo que o termo Rede Neural só tenha começado a ganhar destaque nos últimos anos, os primeiros estudos teóricos começaram por volta de 1940<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 12]</span>. Um dos primeiros trabalhos publicados foi “A Logical Calculus of the Ideas Immamente in Nervous Activity” de 1943, em que os autores, Warren McCulloch e Walter Pitts, apresentaram um modelo artificial de um neurônio a partir da teoria de redes lógicas de nós<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 14]</span>.</p>
<p>A Figura <a href="deep-learning-em-visão-computacional.html#fig:neuron">7.4</a> apresenta uma simplificação de um neurônio biológico, dividido em três partes principais: o corpo da célula, os dendritos e o axônio. Um neurônio recebe informações, ou impulsos nervosos, a partir dos dendritos. Estas informações são processadas no corpo celular e novos impulsos são transmitidos através do axônio para outros neurônios. A comunicação entre os neurônios, a sinapse, controla a transmissão dos impulsos, determinando o fluxo de informações com base na intensidade do sinal recebido<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 36]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:neuron"></span>
<img src="imagens/07-deepLearning/neuron.png" alt="Representação de um neurônio biológico[29]" width="60%" />
<p class="caption">
Figura 7.4: Representação de um neurônio biológico<span class="citation">[<a href="#ref-img:neuronCS" role="doc-biblioref">29</a>]</span>
</p>
</div>
<p>Por analogia, McCulloch e Pitts descreveram matematicamente um neurônio artificial como um modelo com <span class="math inline">\(n\)</span> terminais de entrada <span class="math inline">\(x_m\)</span>, representando os dendritos, e apenas um ponto de saída <span class="math inline">\(y_k\)</span> como axônio (Figura <a href="deep-learning-em-visão-computacional.html#fig:artificialNeuron">7.5</a>). Para simular o comportamento das sinapses, cada entrada <span class="math inline">\(x_m\)</span> é associada com um peso <span class="math inline">\(w_{km}\)</span>, sendo que o somatório representa a intensidade do sinal recebido (<span class="math inline">\(v_k\)</span>).</p>

<div class="figure" style="text-align: center"><span id="fig:artificialNeuron"></span>
<img src="imagens/07-deepLearning/artificial-neuron.png" alt="Representação matemática de um neurônio artificial[28, p. 36]" width="70%" />
<p class="caption">
Figura 7.5: Representação matemática de um neurônio artificial<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 36]</span>
</p>
</div>
<p>O sinal de resposta é estabelecido por uma função de ativação <span class="math inline">\(\varphi\)</span> aplicada ao valor da soma ponderada, e que apresenta comportamento limiar como na equação, em que a saída é zero ou um dependendo do valor limite (Figura <a href="deep-learning-em-visão-computacional.html#fig:limiarFunc">7.6</a>). O modelo também pode incluir um bias (<span class="math inline">\(b_k\)</span>) no somatório para aumentar o grau de liberdade da função de ativação e garantir que um neurônio não apresente saída nula mesmo que todas as entradas sejam nulas. O valor do bias é ajustado junto com os pesos sinápticos<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 37]</span>.</p>
<p><span class="math display">\[y_k=\varphi(\upsilon_k)=
\begin{cases}
 1 \text{ se } \upsilon_k &gt; 0 \\ 
 0 \text{ se } \upsilon_k &lt; 0 
\end{cases}\]</span></p>

<div class="figure" style="text-align: center"><span id="fig:limiarFunc"></span>
<img src="imagens/07-deepLearning/limiar-function.png" alt="Função de ativação de limiar[28, p. 36]" width="50%" />
<p class="caption">
Figura 7.6: Função de ativação de limiar<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 36]</span>
</p>
</div>
<p>O modelo proposto por McCulloch e Walter Pitts poderia realizar classificações em duas categorias, entretanto os pesos precisavam ser ajustados manualmente, pois não tinham a capacidade de aprender<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 14]</span>. Uma das primeiras discussões sobre regras de aprendizagem nas correções dos pesos sinápticos foi publicada em 1949 no livro de Donald Hebb “The Organization of Behavior”<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 64]</span>. No postulado de Hebb se apresenta que a conexão entre os neurônios é fortalecida cada vez que é utilizada, assim, os caminhos neurais no cérebro são continuamente modificados e formam agrupamentos.</p>
<p>A primeira Rede neural com capacidade de aprender os pesos das categorias foi o Perceptron apresentado por Frank Rosenblatt em 1958<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 65]</span>. O Perceptron tinha arquitetura semelhante a da Figura <a href="deep-learning-em-visão-computacional.html#fig:perceptron">7.7</a>, uma rede de camada única além da entrada, e de aprendizado supervisionado. Inicialmente, foram lançadas grandes expectativas sobre as possíveis aplicações do Perceptron, entretanto, as limitações logo começaram a ser destacadas, muitas descritas no livro de Marvin Minsky e Seymour Papert publicado em 1969. Um perceptron de camada única realiza apenas a classificação de padrões linearmente separáveis em duas categorias, não podendo, por exemplo, representar o operador de lógica XOR, que não é linearmente separável<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 14]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:perceptron"></span>
<img src="imagens/07-deepLearning/one-layer-perceptron.png" alt="Arquitetura Perceptron[28, p. 47]" width="50%" />
<p class="caption">
Figura 7.7: Arquitetura Perceptron<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 47]</span>
</p>
</div>
<p>A imagem negativa sobre o perceptron e as limitações tecnológicas diminuiram a popularidade das redes neurais, reduzindo o número de pesquisas na área até os anos 80<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 16]</span>. O interesse pelas redes neurais começou a aumentar principalmente pelo uso da abordagem de processamento paralelo distribuído, como o aplicado no algoritmo de retropropagação (back-propagation) apresentado por Rumelhart, Hinton e Williams (1986). Ainda hoje este é o algoritmo mais utilizado para aprendizado profundo e foi crucial para o treinamento dos perceptrons de múltiplas camadas MLP<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 184]</span>.</p>
<div id="rede-mlp" class="section level4">
<h4><span class="header-section-number">7.1.2.1</span> Rede MLP</h4>
<p>Para que a rede de perceptrons de múltiplas camadas pudesse aprender seria necessário a retropropagação dos erros de trás para frente entre as camadas, tornando possível a minimização da função custo. A necessidade do cálculo da derivada do erro implicou no aparecimento de funções de ativação diferentes do utilizado no modelo original do perceptron, que não fossem de limitação abrupta Figura <a href="deep-learning-em-visão-computacional.html#fig:limiarFunc">7.6</a>- ativação limiar<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 184]</span>. Considerando que as funções de ativação são um dos elementos utilizados para a inclusão de não linearidade, ponto chave para que os modelos não se limitem à padrões linearmente separáveis, a abordagem foi a incorporação de funções não lineares, mas “bem comportadas“, ou seja, que são “quase” lineares contínuas e deriváveis.</p>
<p>Como as funções de ativação são responsáveis pelo intermédio das respostas entre as camadas, deveriam ser considerados formatos não lineares que não alterassem de forma radical a resposta da rede. Os perfis que mais se aproximavam destes comportamentos são as funções sigmóides, tangente hiperbólica e a função logística<span class="citation">[<a href="#ref-rateke1999" role="doc-biblioref">30</a>]</span>.</p>
<p>A função sigmóide tem um formato em S, em que nas extremidades a função tem um comportamento constante, o que fica evidente no gráfico da função logística (Figura <a href="deep-learning-em-visão-computacional.html#fig:sigmoid">7.8</a>). O parâmetro a da equação logística na equação permite parametrizar o comportamento da função, alterando a inclinação. Quanto maior o valor de a, mais a função sigmóide se aproxima da função de limiar.</p>

<div class="figure" style="text-align: center"><span id="fig:sigmoid"></span>
<img src="imagens/07-deepLearning/sigmoid-function.png" alt="Função sigmóide[28, p. 39]" width="50%" />
<p class="caption">
Figura 7.8: Função sigmóide<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 39]</span>
</p>
</div>
<p><span class="math display">\[\varphi(\upsilon)=\frac{1}{1+\exp(-a\upsilon)}\]</span></p>
<p>Diferente da função limiar que assume valores <span class="math inline">\(0\)</span> ou <span class="math inline">\(1\)</span>, a função logística tem resultados em um intervalo contínuo entre <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span><span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 40]</span>. A função sigmóide também é diferenciável, enquanto que a função de limiar não. Uma forma anti-simétrica da sigmóide é a função tangente hiperbólica na equação. A função tangente hiperbólica é definida no intervalo <span class="math inline">\(-1\)</span> a <span class="math inline">\(1\)</span>, o que permite a função sigmóide assumir também valores negativos<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 40]</span>.</p>
<p><span class="math display">\[\varphi(\upsilon)=\tanh(a\upsilon)\]</span></p>
<p>Ao se propor um método eficiente no treinamento dos perceptrons de múltiplas camadas se tornou interessante a inclusão de uma ou mais camadas de neurônios ocultos entre a camada de entrada e de saída. A combinação de mais camadas permitiu que a rede fosse implementada para problemas mais complexos, não se restringindo às transformações lineares do modelo original do perceptron. Por meio das camadas ocultas é possível extrair de forma progressiva características importantes que definem os padrões de entrada<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 184]</span>.</p>
<p>O neurônio matemático proposto inicialmente foi estendido para uma estrutura de conexões de elementos de processamento, os nós da rede. Os elementos foram organizados em camadas, e foram propostas diferentes configurações de conexões. Os formatos mais populares são definidos como uma arquitetura de rede neural, reconhecida pelo número de camadas da rede, número de nós em cada camada e tipo de conexão entre os nós.</p>
<p>A arquitetura da rede MLP é composta por uma camada de entrada que recebe o sinal, uma camada de saída que retorna o resultado, e entre elas um número arbitrário de camadas ocultas (Figura <a href="deep-learning-em-visão-computacional.html#fig:mlpnet">7.9</a>). Geralmente, a escolha do número de nós na camada de entrada e saída é direta. Por exemplo, em uma aplicação com imagens, o número de neurônios na camada de entrada pode corresponder ao número de pixels da imagem. Neste caso, a saída poderia ser projetada com um único neurônio indicando a probabilidade de um resultado positivo. Já o arranjo das camadas intermediárias não é tão simples, muitas vezes é definido empiricamente com base nas características dos dados de entrada e na complexidade do problema (CARVALHO; BRAGA; LUDERMIR, 1998).</p>

<div class="figure" style="text-align: center"><span id="fig:mlpnet"></span>
<img src="imagens/07-deepLearning/mlp-network.png" alt="Arquitetura da rede MLP[28, p. 186]" width="90%" />
<p class="caption">
Figura 7.9: Arquitetura da rede MLP<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 186]</span>
</p>
</div>
<p>Uma classificação comum das arquiteturas é com base no padrão de conexões, sendo identificadas duas classes principais: redes diretas (feed forward) e redes recorrentes<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 46]</span>. O modelo MLP tem arquitetura do tipo feedforward, em que a propagação da informação ocorre em uma única direção e os nós de uma mesma camada não são conectados entre si. A saída de uma camada é usada como entrada na próxima, sem loops, ou seja, não são enviadas de volta<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 47]</span>.</p>
<p>Já nas tipologias recorrentes ocorre o feedback, um processo de realimentação, em que as saídas de nós são reinseridas como entradas em nós anteriores (Figura <a href="deep-learning-em-visão-computacional.html#fig:recnet">7.10</a>). O comportamento dos ciclos é dinâmico controlado por atrasos unitários<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 49]</span>. A ideia do modelo é estimular sinais em efeito cascata com dependência temporal.</p>

<div class="figure" style="text-align: center"><span id="fig:recnet"></span>
<img src="imagens/07-deepLearning/recurrent-network.png" alt="Arquitetura rede recorrente[28, p. 49]" width="90%" />
<p class="caption">
Figura 7.10: Arquitetura rede recorrente<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 49]</span>
</p>
</div>
</div>
<div id="backpropagation" class="section level4">
<h4><span class="header-section-number">7.1.2.2</span> Backpropagation</h4>
<p>Para explicar o algoritmo backpropagation no treinamento de redes neurais utilizaremos um exemplo de aplicação de rede MLP para o reconhecimento de números. O código da rede é uma implementação do livro online “Neural Networks and Deep Learning” escrito por Michael Nielsen. Os dados de treinamento foram retirados do MNIST data set, que contém mais de 60000 imagens escaneadas de números escritos juntamente com os rótulos de classificação. As informações foram coletadas pelo Instituto Nacional de Padrões e Tecnologia dos Estados Unidos (NIST), sendo que as imagens são em escala de cinza e de tamanho 28 x 28 pixels como na Figura <a href="deep-learning-em-visão-computacional.html#fig:mnistZero">7.11</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:mnistZero"></span>
<img src="imagens/07-deepLearning/mnist-zero.png" alt="Arquitetura rede recorrente[31]" width="40%" />
<p class="caption">
Figura 7.11: Arquitetura rede recorrente<span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>
</p>
</div>
<p>O conjunto de dados originais do MNIST é dividido em duas partes, uma que contém 60000 imagens para treinamento e a outra com 10000 imagens para a fase de teste em que se avalia a acurácia da rede treinada para reconhecer os dígitos. No exemplo do autor Michael Nielsen, os dados de treinamento original também foram reorganizados em dois grupos, o primeiro com 50000 imagens que foram utilizados no treinamento e a outra parte (10000) foi reservada para a validação em que se definiu os hiperparâmetros da rede.</p>
<p>Considerando imagens de 28x28 bits os dados de entrada foram definidos como um vetor <span class="math inline">\(x\)</span> de dimensão <span class="math inline">\(784\)</span>, em que cada posição corresponde a um valor de pixel da imagem. Para o vetor <span class="math inline">\(y\)</span> de saída da rede se estabeleceu a dimensão <span class="math inline">\(10\)</span>, em que cada posição faz referência a um dígito de <span class="math inline">\(0\)</span> a <span class="math inline">\(9\)</span>. Assim, se uma entrada corresponde ao número <span class="math inline">\(3\)</span> então a saída esperada será o vetor transposto na forma <span class="math inline">\(y(x)=(0,0,0,1,0,0,0,0,0,0)^T\)</span>. Com base no formato dos dados de entrada e saída da rede, o exemplo foi construído com uma rede MLP de três camadas como na Figura <a href="deep-learning-em-visão-computacional.html#fig:mlpTwo">7.12</a>, com a primeira camada tendo <span class="math inline">\(784\)</span> nós e a última camada com <span class="math inline">\(10\)</span> nós. Na camada do meio, a camada oculta, utilizaremos <span class="math inline">\(30\)</span> nós, mas vale destacar que o autor Michael Nielsen definiu o número de nós após alguns testes otimizando a escolha dos parâmetros da rede.</p>

<div class="figure" style="text-align: center"><span id="fig:mlpTwo"></span>
<img src="imagens/07-deepLearning/mlp-two-layers.png" alt="Rede MLP com uma camada oculta[32]" width="60%" />
<p class="caption">
Figura 7.12: Rede MLP com uma camada oculta<span class="citation">[<a href="#ref-hertz2018" role="doc-biblioref">32</a>]</span>
</p>
</div>
<p>Para carregar os dados e configurá-los no formato proposto utiliza-se o método “load_data_wrapper()”. Os dados são retirados do arquivo zip “mnist.pkl.gz’” e subdivididos dentro do método “load_data()” que retorna para o “load_data_wrapper()”, como dados de treinamento, validação e de teste. A função geral é chamada da seguinte forma:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="deep-learning-em-visão-computacional.html#cb1-1"></a>training_data, validation_data, test_data <span class="op">=</span> load_data_wrapper()</span></code></pre></div>
<p>No programa, a rede é construída a partir do comando Network([784, 30, 10], cost=QuadraticCost), em que cada argumento corresponde ao número de nós na camada. Os atributos da classe Network incluem o número de camadas (num_layers), o número de nós em cada camada (sizes), os pesos e bias iniciais que são gerados de forma aleatória pelo método “default_weight_initializer()”, e a função custo (cost). A função custo aplicada neste exemplo, o erro quadrático (MSE), é definida na classe “QuadraticCost”.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="deep-learning-em-visão-computacional.html#cb2-1"></a><span class="kw">class</span> Network(<span class="bu">object</span>):</span>
<span id="cb2-2"><a href="deep-learning-em-visão-computacional.html#cb2-2"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sizes, cost<span class="op">=</span>QuadraticCost):</span>
<span id="cb2-3"><a href="deep-learning-em-visão-computacional.html#cb2-3"></a>    <span class="va">self</span>.num_layers <span class="op">=</span> <span class="bu">len</span>(sizes)</span>
<span id="cb2-4"><a href="deep-learning-em-visão-computacional.html#cb2-4"></a>    <span class="va">self</span>.sizes <span class="op">=</span> sizes</span>
<span id="cb2-5"><a href="deep-learning-em-visão-computacional.html#cb2-5"></a>    <span class="va">self</span>.default_weight_initializer()</span>
<span id="cb2-6"><a href="deep-learning-em-visão-computacional.html#cb2-6"></a>    <span class="va">self</span>.cost<span class="op">=</span>cost</span></code></pre></div>
<p>A seguir apresentaremos um resumo da teoria matemática do método backpropagation e para facilitar este processo utilizaremos a nomenclatura dos elementos de uma rede neural com base no livro “Introduction To The Theory Of Neural Computation”<span class="citation">[<a href="#ref-hertz2018" role="doc-biblioref">32</a>, p. 116]</span>. No treinamento de uma rede como a da Figura <a href="deep-learning-em-visão-computacional.html#fig:mlpTwo">7.12</a> é apresentado um conjunto de treinamento <span class="math inline">\(\{\xi_k^\mu,\zeta_i^\mu\}\)</span> , em que cada padrão <span class="math inline">\((\mu=1, 2,\dots, p)\)</span> apresentado corresponde a um par de entrada (<span class="math inline">\(\xi_k^\mu\)</span>) e saída esperada (<span class="math inline">\(\zeta_i^\mu\)</span>). Neste exemplo, o número de padrões no treinamento é <span class="math inline">\(p=50000\)</span>. O índice <span class="math inline">\(k\)</span> na camada de entrada faz referência ao valor em cada nó da camada, e o índice <span class="math inline">\(i\)</span> aos nós da camada da saída. A resposta final da rede é identificada como <span class="math inline">\(O_i\)</span> e o sinal de saída da camada oculta é <span class="math inline">\(V_j\)</span>. A conexão entre a camada de entrada e a oculta é estabelecida pelos pesos <span class="math inline">\(w_{jk}\)</span>, e os pesos <span class="math inline">\(W_{ij}\)</span> conectam a camada de saída com a intermediária.</p>
<p>O backpropagation é um método supervisionado em que o treinamento ocorre em duas fases<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 163]</span>. Na etapa foward, uma entrada é apresentada para a rede e de acordo com as conexões estabelecidas entre as camadas é propagado sucessivamente os sinais de respostas até a camada de saída, gerando um resultado que se espera ser o mais próximo do padrão. Cada nó de uma camada seguinte se conecta com todos os nós da camada anterior, sendo que o sinal recebido por este nó é uma ponderação dos pesos de todas as conexões. O sinal de entrada de cada nó recebe um bia e é passado para a próxima camada como uma resposta de uma função de ativação (<span class="math inline">\(g\)</span>). A resposta de saída de um nó será denominada <span class="math inline">\(V_j\)</span> se o sinal for para uma camada intermediária, ou <span class="math inline">\(O_i\)</span> se for direcionado para a camada de saída.</p>
<p>Imagine que um nó (<span class="math inline">\(j\)</span>) da camada intermediária recebe como entrada:</p>
<p><span class="math display">\[h_j^\mu=\sum_{k}w_{jk}\xi_k^\mu\]</span></p>
<p>e produz como resposta:</p>
<p><span class="math display">\[V_j^\mu=g(h_j^\mu)=g(w_{jk}\xi_k^\mu)\]</span></p>
<p>Assim, um nó na camada de saída recebe como entrada o sinal propagado:</p>
<p><span class="math display">\[h_i^\mu=\sum_kW_{ij}V_j^\mu=\sum_kW_{ij}g(\sum_kw_{jk}\xi_k^\mu)\]</span></p>
<p>gerando como resposta da saída da rede:</p>
<p><span class="math display">\[O_i^\mu=g(h_i^\mu)=g(\sum_kW_{ij}V_j^\mu)=g(\sum_kW_{ij}g(\sum_kw_{jk}\xi_k^\mu))\]</span></p>
<p>No programa, a fase forward é representada pelo seguinte método:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="deep-learning-em-visão-computacional.html#cb3-1"></a>feedforward(<span class="va">self</span>, a):</span>
<span id="cb3-2"><a href="deep-learning-em-visão-computacional.html#cb3-2"></a>  <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</span>
<span id="cb3-3"><a href="deep-learning-em-visão-computacional.html#cb3-3"></a>    a <span class="op">=</span> sigmoid(np.dot(w, a)<span class="op">+</span>b)</span>
<span id="cb3-4"><a href="deep-learning-em-visão-computacional.html#cb3-4"></a>  <span class="cf">return</span> a</span></code></pre></div>
<p>Neste exemplo a função de ativação é a função logística definida pelo método “sigmoid” e a sua derivada é calculada no método “sigmoid_prime”.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="deep-learning-em-visão-computacional.html#cb4-1"></a>sigmoid(z):</span>
<span id="cb4-2"><a href="deep-learning-em-visão-computacional.html#cb4-2"></a>  <span class="cf">return</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span><span class="op">+</span>np.exp(<span class="op">-</span>z))</span>
<span id="cb4-3"><a href="deep-learning-em-visão-computacional.html#cb4-3"></a> </span>
<span id="cb4-4"><a href="deep-learning-em-visão-computacional.html#cb4-4"></a>sigmoid_prime(z):</span>
<span id="cb4-5"><a href="deep-learning-em-visão-computacional.html#cb4-5"></a>  <span class="cf">return</span> sigmoid(z)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>sigmoid(z))</span></code></pre></div>
<p>Na segunda fase, backward, os pesos e bias são corrigidos camada a camada, no sentido da saída da rede até a entrada, em um processo iterativo de forma que a saída i fique cada vez mais próxima do padrão esperado Oi, reduzindo o erro<span class="citation">[<a href="#ref-haykin1999" role="doc-biblioref">28</a>, p. 163]</span>. Uma forma de avaliar como o erro é reduzido em relação às alterações dos parâmetros é determinando uma função Erro, ou custo, dependente dos pesos e bias. Adotamos como função custo o erro quadrático (MSE):</p>
<p><span class="math display">\[E[w]=\frac{1}{2}\sum_{\mu i}[\zeta_i^\mu-O_i^\mu]^2 = \frac{1}{2}[\zeta_i^\mu W_{ij}g(\sum_kw_{jk}\xi_k^\mu)]\]</span></p>
<p>No programa, a função custo MSE é apresentada no método “fn(a, y)” na classe “QuadraticCost”:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="deep-learning-em-visão-computacional.html#cb5-1"></a>fn(a, y):</span>
<span id="cb5-2"><a href="deep-learning-em-visão-computacional.html#cb5-2"></a>  <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>np.linalg.norm(a<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span></span></code></pre></div>
<p>A redução do erro envolve um processo de otimização, denominado descida em gradiente, em que se busca determinar os parâmetros (pesos e bias) que minimizam a função custo<span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>. Neste método, a variação do erro pode ser escrita como derivadas parciais do erro em função dos pesos, compondo o vetor gradiente do erro. Como o vetor gradiente aponta no sentido de maior acréscimo do erro, a variação dos pesos é dada pelo negativo do gradiente, garantindo a redução mais rápida do erro. Assim, a regra do gradiente descendente aplicada nas conexões entre a camada oculta e de saída pode ser escrita como:</p>
<p><span class="math display">\[\Delta W_{ij}=-\eta\frac{\partial E}{\partial W_{ij}}=\eta\sum_\mu[\zeta_i^\mu-O_i^\mu]g&#39;(h_i^\mu)V_j^\mu=\eta\sum_\mu\delta_i^\mu V\]</span>
<span class="math display">\[\delta_i^\mu=[\zeta_i^\mu-O_i^\mu]g&#39;(h_i^\mu)\]</span></p>
<p>A fórmula de modificações dos pesos é conhecida como regra delta e recebe o termo <span class="math inline">\(\eta\)</span>, a taxa de aprendizagem, para promover uma correção gradativa, sem alterações bruscas <span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>. O termo <span class="math inline">\(g’\)</span> se refere a derivada da função de ativação e surge na fórmula devido a derivação da função erro. A regra delta aplicada nas conexões entre a camada oculta e de entrada utiliza a regra da cadeia pois as derivadas são em relação aos pesos <span class="math inline">\(w_{jk}\)</span>, que se apresentam como dependência mais implícita ao erro. A correção dos pesos neste caso ocorre como:</p>
<p><span class="math display">\[\begin{split}
\Delta w_{ij}&amp;=-\eta\frac{\partial E}{\partial w_{jk}}=-\eta\sum_\mu\frac{\partial E}{\partial V_j^\mu}\frac{\partial V_j^\mu}{\partial w_{jk}}=\eta\sum_{\mu i}[\zeta_i^\mu-O_i^\mu]g&#39;(h_i^\mu)W_{ij}g&#39;(h_j^\mu)\xi_k^\mu
\\ \\&amp;=\eta\sum_{\mu i}\delta_i^\mu W_{ij}g&#39;(h_j^\mu)\xi_k^\mu=\eta\sum_\mu\delta_j^\mu\xi_k^\mu
\end{split}\]</span></p>
<p><span class="math display">\[\delta_j^\mu=g&#39;(h_j^\mu)\sum_i\delta_i^\mu W_{ij}\]</span></p>
<p>Esta regra também pode ser estendida para redes com mais de uma camada oculta<span class="citation">[<a href="#ref-rateke1999" role="doc-biblioref">30</a>]</span>. A regra delta generalizada para a m-ésima camada de uma rede pode ser escrita como:</p>
<p><span class="math display">\[\Delta w_{pq}^m=\eta\sum_\mu\delta_p^{m,\mu}V_q^{m-1,\mu}\]</span>
<span class="math display">\[\delta_p^{M,\mu}=[\zeta_p^\mu-O_p^\mu]g&#39;(h_p^{M,\mu}) \text{, para camada de saida } m=M\]</span>
<span class="math display">\[\delta_p^{m,\mu}=g&#39;(h_p^{m,\mu})\sum_r\delta_r^{m+1,\mu}w_{rp}^{m+1} \text{, para m&lt;M}\]</span>
A correção dos pesos ocorre considerando as conexões entre cada duas camadas, uma mais próxima da saída (<span class="math inline">\(p\)</span>) e a outra mais interna (<span class="math inline">\(q\)</span>). O vetor <span class="math inline">\(V_q\)</span> representa o sinal de ativação recebido pela camada dos nós “<span class="math inline">\(p\)</span>”, e quando o cálculo envolve a camada de entrada e a primeira camada oculta este vetor é o padrão de entrada (<span class="math inline">\(\xi_k^\mu\)</span>). O fator delta (<span class="math inline">\(\delta\)</span>) funciona como uma memória das respostas das camadas mais externas, ou seja, para modificar os pesos de trás para frente é necessário que as conexões das camadas mantenham memória das camadas que foram alteradas anteriormente.
O algoritmo do backpropagation é utilizado na etapa de treinamento pelo programa por meio do método “backprop”:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="deep-learning-em-visão-computacional.html#cb6-1"></a>backprop(<span class="va">self</span>, x, y):</span>
<span id="cb6-2"><a href="deep-learning-em-visão-computacional.html#cb6-2"></a>  nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</span>
<span id="cb6-3"><a href="deep-learning-em-visão-computacional.html#cb6-3"></a>  nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</span>
<span id="cb6-4"><a href="deep-learning-em-visão-computacional.html#cb6-4"></a>  <span class="co"># feedforward</span></span>
<span id="cb6-5"><a href="deep-learning-em-visão-computacional.html#cb6-5"></a>  activation <span class="op">=</span> x</span>
<span id="cb6-6"><a href="deep-learning-em-visão-computacional.html#cb6-6"></a>  activations <span class="op">=</span> [x] </span>
<span id="cb6-7"><a href="deep-learning-em-visão-computacional.html#cb6-7"></a>  zs <span class="op">=</span> [] </span>
<span id="cb6-8"><a href="deep-learning-em-visão-computacional.html#cb6-8"></a>  <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</span>
<span id="cb6-9"><a href="deep-learning-em-visão-computacional.html#cb6-9"></a>      z <span class="op">=</span> np.dot(w, activation)<span class="op">+</span>b</span>
<span id="cb6-10"><a href="deep-learning-em-visão-computacional.html#cb6-10"></a>    zs.append(z)</span>
<span id="cb6-11"><a href="deep-learning-em-visão-computacional.html#cb6-11"></a>    activation <span class="op">=</span> sigmoid(z)</span>
<span id="cb6-12"><a href="deep-learning-em-visão-computacional.html#cb6-12"></a>    activations.append(activation)</span>
<span id="cb6-13"><a href="deep-learning-em-visão-computacional.html#cb6-13"></a>  <span class="co"># backward pass</span></span>
<span id="cb6-14"><a href="deep-learning-em-visão-computacional.html#cb6-14"></a>  delta <span class="op">=</span> (<span class="va">self</span>.cost).delta(zs[<span class="op">-</span><span class="dv">1</span>], activations[<span class="op">-</span><span class="dv">1</span>], y)</span>
<span id="cb6-15"><a href="deep-learning-em-visão-computacional.html#cb6-15"></a>  nabla_b[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> delta</span>
<span id="cb6-16"><a href="deep-learning-em-visão-computacional.html#cb6-16"></a>  nabla_w[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> np.dot(delta, activations[<span class="op">-</span><span class="dv">2</span>].transpose())</span>
<span id="cb6-17"><a href="deep-learning-em-visão-computacional.html#cb6-17"></a>  <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="va">self</span>.num_layers):</span>
<span id="cb6-18"><a href="deep-learning-em-visão-computacional.html#cb6-18"></a>    z <span class="op">=</span> zs[<span class="op">-</span>l]</span>
<span id="cb6-19"><a href="deep-learning-em-visão-computacional.html#cb6-19"></a>    sp <span class="op">=</span> sigmoid_prime(z)</span>
<span id="cb6-20"><a href="deep-learning-em-visão-computacional.html#cb6-20"></a>    delta <span class="op">=</span> np.dot(<span class="va">self</span>.weights[<span class="op">-</span>l<span class="op">+</span><span class="dv">1</span>].transpose(),delta) <span class="op">*</span> sp</span>
<span id="cb6-21"><a href="deep-learning-em-visão-computacional.html#cb6-21"></a>      nabla_b[<span class="op">-</span>l] <span class="op">=</span> delta</span>
<span id="cb6-22"><a href="deep-learning-em-visão-computacional.html#cb6-22"></a>      nabla_w[<span class="op">-</span>l] <span class="op">=</span> np.dot(delta, </span>
<span id="cb6-23"><a href="deep-learning-em-visão-computacional.html#cb6-23"></a>      activations[<span class="op">-</span>l<span class="dv">-1</span>].transpose())</span>
<span id="cb6-24"><a href="deep-learning-em-visão-computacional.html#cb6-24"></a>  <span class="cf">return</span> (nabla_b, nabla_w)</span></code></pre></div>
<p>Como destacado anteriormente, a primeira fase do backpropagation é o feedforward. Nesta etapa é recebido um padrão de entrada (<span class="math inline">\(x\)</span>) e os pesos e bias inicializados aleatoriamente. Após o somatório das ponderações dos pesos e bias entre duas camadas, este valor é salvo no vetor “<span class="math inline">\(zs\)</span>”, e o resultado da ativação deste valor é salvo em “activations”. A entrada da próxima camada é o sinal de ativação salvo em “actvivation”. Este processo ocorre da entrada até a camada de saída, salvando os sinais de ativação das camadas ocultas (<span class="math inline">\(V_j\)</span>) em “activations”.
Na fase “backward pass”, calcula-se primeiro o delta (<span class="math inline">\(\delta\)</span>) a partir da resposta da camada de saída salva como o último elemento do vetor “activations” e do padrão de saída esperado (<span class="math inline">\(y\)</span>). O valor de delta neste caso, é calculado a partir do método “delta” da classe “QuadraticCost” como o produto entre a diferença da resposta de saída de rede (<span class="math inline">\(a\)</span>) e do valor esperado (<span class="math inline">\(y\)</span>) com a derivada do sinal de ativação da última camada:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="deep-learning-em-visão-computacional.html#cb7-1"></a>delta(z, a, y):</span>
<span id="cb7-2"><a href="deep-learning-em-visão-computacional.html#cb7-2"></a>  <span class="cf">return</span> (a<span class="op">-</span>y) <span class="op">*</span> sigmoid_prime(z)</span></code></pre></div>
<p>Após o cálculo do primeiro delta é determinado o incremento dos pesos (<span class="math inline">\(\Delta W_{ij}\)</span>) entre a última camada e a camada oculta como o produto do delta (<span class="math inline">\(\sigma_i\)</span>) pelo vetor de ativação (<span class="math inline">\(V_j\)</span>) que a ultima camada recebeu como entrada. Os incrementos dos pesos são salvos no vetor “nabla_w”. Os deltas e incrementos dos pesos das camadas ocultas são obtidos de forma iterativa na estrutura de repetição. O cálculo do delta da camada m depende do somatório dos produtos do delta calculado anteriormente, da camada mais externa, com o vetor peso da camada m. O valor do somatório é multiplicado pela derivada do sinal de ativação da camada m. Em seguida, o valor do incremento dos pesos é obtido pelo produto do delta atual com o valor de ativação recebido pela camada m. Após realizar este mesmo processo para todas as camadas, a função retorna um vetor com os incrementos dos pesos com base em um padrão (<span class="math inline">\(\xi_k^\mu,\zeta_i^\mu\)</span>), o que ocorre para todos os padrões de treinamento.</p>
<p>Para acelerar o processo de aprendizagem, em vez de atualizar os pesos cada vez que se apresenta um padrão, o autor Michael Nielsen sugere no seu exemplo a utilização do método gradiente descendente estocástico. A ideia é agrupar de forma aleatória os padrões de entrada formando o que ele chama de “mini-batch”. No método “update_mini_batch”, a função “backprop” retorna o incremento do peso calculado para cada padrão dentro do agrupamento, e estes são somados em “nabla-w” até todo o agrupamento ser apresentado, e então os pesos e os bias são ajustados. Em seguida são apresentados os outros “mini-batch” até que todo o conjunto de treinamento seja utilizado, encerrando uma época de treinamento. Ou seja, em cada época o conjunto de treinamento é subdividido em agrupamentos, e os pesos são atualizados apenas no final de apresentação de cada agrupamento como demonstrado a seguir:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="deep-learning-em-visão-computacional.html#cb8-1"></a>update_mini_batch(<span class="va">self</span>, mini_batch, eta, lmbda, n):</span>
<span id="cb8-2"><a href="deep-learning-em-visão-computacional.html#cb8-2"></a>  nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</span>
<span id="cb8-3"><a href="deep-learning-em-visão-computacional.html#cb8-3"></a>  nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</span>
<span id="cb8-4"><a href="deep-learning-em-visão-computacional.html#cb8-4"></a>  <span class="cf">for</span> x, y <span class="kw">in</span> mini_batch:</span>
<span id="cb8-5"><a href="deep-learning-em-visão-computacional.html#cb8-5"></a>    delta_nabla_b, delta_nabla_w <span class="op">=</span> <span class="va">self</span>.backprop(x, y)</span>
<span id="cb8-6"><a href="deep-learning-em-visão-computacional.html#cb8-6"></a>    nabla_b <span class="op">=</span> [nb<span class="op">+</span>dnb <span class="cf">for</span> nb, dnb <span class="kw">in</span> <span class="bu">zip</span>(nabla_b, delta_nabla_b)]</span>
<span id="cb8-7"><a href="deep-learning-em-visão-computacional.html#cb8-7"></a>    nabla_w <span class="op">=</span> [nw<span class="op">+</span>dnw <span class="cf">for</span> nw, dnw <span class="kw">in</span> <span class="bu">zip</span>(nabla_w, delta_nabla_w)]</span>
<span id="cb8-8"><a href="deep-learning-em-visão-computacional.html#cb8-8"></a>  <span class="va">self</span>.weights <span class="op">=</span> [(<span class="dv">1</span><span class="op">-</span>eta<span class="op">*</span>(lmbda<span class="op">/</span>n))<span class="op">*</span>w<span class="op">-</span>(eta<span class="op">/</span><span class="bu">len</span>(mini_batch))<span class="op">*</span>nw <span class="cf">for</span> w, nw <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.weights, nabla_w)]</span>
<span id="cb8-9"><a href="deep-learning-em-visão-computacional.html#cb8-9"></a>  <span class="va">self</span>.biases <span class="op">=</span> [b<span class="op">-</span>(eta<span class="op">/</span><span class="bu">len</span>(mini_batch))<span class="op">*</span>nb <span class="cf">for</span> b, nb <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, nabla_b)]</span></code></pre></div>
<p>O treinamento ocorre a partir do método “SGD”, sigla para descida do gradiente estocástico. Em que são passados como parâmetros o conjunto de treinamento, o número de épocas, o tamanho do agrupamento (mini_batch_size) e a taxa de aprendizagem.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="deep-learning-em-visão-computacional.html#cb9-1"></a>net.SGD(training_data,<span class="dv">30</span>,<span class="dv">10</span>,<span class="fl">0.5</span>, evaluation_data<span class="op">=</span>test_data,monitor_evaluation_cost<span class="op">=</span><span class="va">True</span>, monitor_evaluation_accuracy<span class="op">=</span><span class="va">True</span>, monitor_training_accuracy<span class="op">=</span><span class="va">True</span>, monitor_training_cost<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>É no método “SGD” que ocorre a subdivisão dos padrões de treinamento em agrupamentos “mini_batch”. Em seguida é chamada a função “update_mini_batch” para cada agrupamento, até finalizar uma época, e este processo se repete para todas as épocas.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="deep-learning-em-visão-computacional.html#cb10-1"></a>SGD(<span class="va">self</span>, training_data, epochs, mini_batch_size, eta,lmbda <span class="op">=</span> <span class="fl">0.0</span> evaluation_data<span class="op">=</span><span class="va">None</span>, monitor_evaluation_cost<span class="op">=</span><span class="va">False</span>, monitor_evaluation_accuracy<span class="op">=</span><span class="va">False</span>, monitor_training_cost<span class="op">=</span><span class="va">False</span>, monitor_training_accuracy<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb10-2"><a href="deep-learning-em-visão-computacional.html#cb10-2"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb10-3"><a href="deep-learning-em-visão-computacional.html#cb10-3"></a>    random.shuffle(training_data)</span>
<span id="cb10-4"><a href="deep-learning-em-visão-computacional.html#cb10-4"></a>    mini_batches <span class="op">=</span> [training_data[k:k<span class="op">+</span>mini_batch_size] <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, mini_batch_size)]</span>
<span id="cb10-5"><a href="deep-learning-em-visão-computacional.html#cb10-5"></a>    <span class="cf">for</span> mini_batch <span class="kw">in</span> mini_batches:</span>
<span id="cb10-6"><a href="deep-learning-em-visão-computacional.html#cb10-6"></a>      <span class="va">self</span>.update_mini_batch(mini_batch, eta, lmbda, <span class="bu">len</span>(training_data))</span>
<span id="cb10-7"><a href="deep-learning-em-visão-computacional.html#cb10-7"></a>    <span class="bu">print</span> (<span class="st">&quot;Epoch </span><span class="sc">%s</span><span class="st"> training complete&quot;</span> <span class="op">%</span> j)</span></code></pre></div>
<p>Dentro do método “SGD” é possível configurar para avaliar o erro total e acurácia da rede após cada época de treinamento, tanto considerando os dados de treinamento quanto os dados de teste ou de validação. Para selecionar ou os dados de teste ou de validação, eles devem ser passados como parâmetros no “evaluation_data”. Ao selecionar as opções “monitor_evaluation_cost” ou “monitor_training_cost” é chamado o método “ total_cost” que retorna a soma dos erros avaliados para todo o conjunto de dados.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="deep-learning-em-visão-computacional.html#cb11-1"></a>total_cost(<span class="va">self</span>, data, lmbda, convert<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb11-2"><a href="deep-learning-em-visão-computacional.html#cb11-2"></a>  cost <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb11-3"><a href="deep-learning-em-visão-computacional.html#cb11-3"></a>  <span class="cf">for</span> x, y <span class="kw">in</span> data:</span>
<span id="cb11-4"><a href="deep-learning-em-visão-computacional.html#cb11-4"></a>    a <span class="op">=</span> <span class="va">self</span>.feedforward(x)</span>
<span id="cb11-5"><a href="deep-learning-em-visão-computacional.html#cb11-5"></a>    <span class="cf">if</span> convert: y <span class="op">=</span> vectorized_result(y)</span>
<span id="cb11-6"><a href="deep-learning-em-visão-computacional.html#cb11-6"></a>    cost <span class="op">+=</span> <span class="va">self</span>.cost.fn(a, y)<span class="op">/</span><span class="bu">len</span>(data)</span>
<span id="cb11-7"><a href="deep-learning-em-visão-computacional.html#cb11-7"></a>    <span class="cf">return</span> cost</span></code></pre></div>
<p>Após cada época se estabelece um conjunto de pesos e bias, e ao utilizar o comando “feedforward” são estes parâmetros que definem a resposta de saída da rede (<span class="math inline">\(a\)</span>) para cada padrão de entrada (<span class="math inline">\(x\)</span>). Ao comparar a resposta (<span class="math inline">\(a\)</span>) com o valor esperado (<span class="math inline">\(y\)</span>) dentro da função custo MSE, método “fn” da classe “QuadraticCost”, quantifica-se o erro para cada padrão.
O método “accuracy” é utilizado dentro do “SGD” quando se configura “monitor_evaluation_accuracy= True” ou “monitor_training_accuracy= True”. Esta função retorna a soma de resultados em que os valores de saída da rede corresponderam ao valor esperado (<span class="math inline">\(y\)</span>). O sinal da rede é calculado pela função “feedforward”, que é utilizada para cada valor (<span class="math inline">\(x\)</span>) do conjunto de dados, seja de treinamento ou de avaliação</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="deep-learning-em-visão-computacional.html#cb12-1"></a>accuracy(<span class="va">self</span>, data, convert<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-2"><a href="deep-learning-em-visão-computacional.html#cb12-2"></a>  results <span class="op">=</span> [(np.argmax(<span class="va">self</span>.feedforward(x)), y) <span class="cf">for</span> (x, y) <span class="kw">in</span> data]</span>
<span id="cb12-3"><a href="deep-learning-em-visão-computacional.html#cb12-3"></a>  <span class="cf">return</span> <span class="bu">sum</span>(<span class="bu">int</span>(x <span class="op">==</span> y) <span class="cf">for</span> (x, y) <span class="kw">in</span> results)</span></code></pre></div>
<p>Considerando que os valores do erro total e da acurácia são calculados para cada época, o método de treinamento “SGD” retorna quatro vetores dentro de uma tupla, cada um com o número de posições correspondentes ao número total de épocas. Assim, se o treinamento ocorrer em 30 épocas, então a primeira lista da tupla terá 30 elementos correspondentes ao custo total dos dados de avaliação no final de cada época.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="deep-learning-em-visão-computacional.html#cb13-1"></a><span class="cf">return</span> evaluation_cost, evaluation_accuracy,training_cost, training_accuracy</span></code></pre></div>
<p>Os resultados salvos podem ser plotados em gráficos para avaliar visualmente o desempenho da rede. Um gráfico muito comum para acompanhar o treinamento da rede é o de custo de treinamento, principalmente porque o aprendizado é guiado pela minimização desta curva. Na Figura <a href="deep-learning-em-visão-computacional.html#fig:cust30">7.13</a> se apresenta a curva de custo para uma configuração que utiliza o conjunto total de treinamento (50000 imagens) e com 30 épocas. Entretanto, não é indicado ter apenas este gráfico como base para estabelecer os hiperparâmetros da rede, como a taxa de aprendizagem e o número de épocas de treinamento. Por exemplo, a Figura <a href="deep-learning-em-visão-computacional.html#fig:cust100">7.14</a> também se refere a uma função de custo, mas para uma outra configuração de treinamento, que utiliza apenas 1000 imagens para treinamento e 100 épocas.</p>

<div class="figure" style="text-align: center"><span id="fig:cust30"></span>
<img src="imagens/07-deepLearning/custo_30.jpg" alt="Curva de custo no treinamento com 30 épocas e 50000 imagens." width="70%" />
<p class="caption">
Figura 7.13: Curva de custo no treinamento com 30 épocas e 50000 imagens.
</p>
</div>

<div class="figure" style="text-align: center"><span id="fig:cust100"></span>
<img src="imagens/07-deepLearning/custo_100.jpg" alt="Curva de custo no treinamento com 100 épocas e 1000 imagens." width="70%" />
<p class="caption">
Figura 7.14: Curva de custo no treinamento com 100 épocas e 1000 imagens.
</p>
</div>
<p>No fim do treinamento, as duas redes apresentaram erros na mesma ordem de grandeza, mas a capacidade de reconhecer números é bem diferente entre as duas. Esta diferença pode ser percebida ao comparar os gráficos de acurácia(Figuras <a href="deep-learning-em-visão-computacional.html#fig:acur30">7.15</a> e <a href="deep-learning-em-visão-computacional.html#fig:acur100">7.16</a>) considerando tanto os dados de treinamento quanto os de validação. O resultado do treinamento com todo o conjunto de dados mostra que a acurácia da rede para os dados de validação é bem próxima do resultado para os valores de treinamento, uma diferença de 1%. Já para a situação que utilizou apenas 1000 dados de treinamento, as curvas de acurácia para os dados de validação e de treinamento estão mais afastadas, apresentando uma diferença próxima de 14%.</p>

<div class="figure" style="text-align: center"><span id="fig:acur30"></span>
<img src="imagens/07-deepLearning/acuracia_30.jpg" alt="Curvas de acurácia para rede treinada com 30 épocas e 1000 imagens." width="70%" />
<p class="caption">
Figura 7.15: Curvas de acurácia para rede treinada com 30 épocas e 1000 imagens.
</p>
</div>

<div class="figure" style="text-align: center"><span id="fig:acur100"></span>
<img src="imagens/07-deepLearning/acuracia_100.jpg" alt="Curvas de acurácia para rede treinada com 100 épocas e 1000 imagens." width="70%" />
<p class="caption">
Figura 7.16: Curvas de acurácia para rede treinada com 100 épocas e 1000 imagens.
</p>
</div>
<p>Ao observar apenas a curva de erro se imagina que a rede esteja aprendendo até o final do treinamento, visto que o erro continua diminuindo. Entretanto, ao analisar as curvas de acurácia se identifica que a acurácia determinada pelos dados de validação aumenta rapidamente até uma determinada época, próximo de 40 no segundo caso, e em seguida fica estagnada. Assim, após a 40° época, a rede não está mais aprendendo a generalizar para os dados de validação, está ocorrendo “overfitting”, ou seja, o treinamento não está melhorando a capacidade da rede. Mesmo que a acurácia do treinamento esteja aumentando depois desta época, pode ser que a rede esteja apenas decorando os dados de treinamento, pois não está mais se atendo apenas às informações gerais, necessárias para reconhecer os números de forma geral<span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>.</p>
<p>Os casos mais comuns de se ocorrer “overfitting” é quando o número de dados do treinamento é muito baixo, como neste segundo caso com apenas 1000 imagens. Nesta situação, a rede tem poucos exemplos para extrair informações gerais, precisando muitas vezes aumentar o número de épocas de treinamento para que se alcance um desempenho mínimo. Quanto maior o número de épocas pode ser mais evidente o efeito de “overfiting”, por isso se recomenda observar quando a acurácia da validação começa a estagnar e a ficar muito distante da curva de treinamento<span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>.</p>
<p>Observar o comportamento da acurácia da validação é um dos métodos para definir até quando a rede deve ser treinada, ou seja, o número de épocas. Os dados de validação ajudam no teste de diferentes configurações de hiperparâmetros da rede, como épocas de treinamento, taxa de aprendizado e número de nós. Só depois de definir estes parâmetros e treinar a rede que se recomenda a utilização dos dados de teste para verificar realmente a acurácia da rede, utilizando dados que ela ainda não teve contato<span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>. Um teste com dados não conhecidos permite verificar se os parâmetros da rede podem ser aplicados em casos mais gerais ou se enquadram apenas em particularidades dos dados treinados. Por esta razão, na maioria dos casos os dados são divididos em três conjuntos - treinamento, validação e teste.</p>
</div>
</div>
</div>
<div id="redes-neurais-convolucionaiscnn" class="section level2">
<h2><span class="header-section-number">7.2</span> Redes neurais convolucionais(CNN)</h2>
<p>A área de deep learning tem conseguido ótimo desempenho em aplicações, principalmente pelo desenvolvimento da área e pelo aumento do poder computacional e da quantidade de dados disponíveis<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 431]</span>. Um dos tipos de redes neurais, conhecido como Redes Neurais Convolucionais(em inglês Convolutional Neural Networks(CNN)) tem também conseguido resultados ótimos, um dos motivos pelos quais foram adotadas em grande peso pela área de Visão Computacional, substituindo muitas das técnicas antigas, que por utilizarem algoritmos mais “estáticos” eram difíceis de serem aplicados em diferente áreas.</p>
<div id="blocos-de-construção-de-uma-cnn" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Blocos de construção de uma CNN</h3>
<p>Nas redes neurais mais simples usamos basicamente neurônios e conexões entre eles para realizar a construção de um modelo. As redes neurais convolucionais contam com algumas estruturas a mais que são a chave de sua eficiência no trabalho com imagens, veremos elas a seguir.</p>
<div id="operador-de-convolução" class="section level4">
<h4><span class="header-section-number">7.2.1.1</span> Operador de convolução</h4>
<p>A operação que dá nome a rede, a convolução é, como vista no tópico X, uma operação realizada entre duas funções. No nosso caso, como estamos trabalhando com imagens, usamos a convolução discreta.</p>
<p>Um ponto importante a se frisar é que matematicamente o que chamaremos de convolução é na verdade uma correlação, sendo que as duas são quase idênticas, a não ser pelo fato de que na convolução giramos o filtro(kernel) em 180º. A única vantagem que ganhamos em girar o filtro antes da operação é que ganhamos a propriedade comutativa, o que é útil matematicamente para derivação de provas mas não é importante na implementação de deep learning<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 332]</span>.</p>
<p>Na literatura e nas bibliotecas de deep learning, incluindo CNNs, se tornou comum chamar as duas operações de convolução<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 333]</span>, então aqui também usaremos essa convenção, utilizando a convolução sem girar o filtro(sendo então, uma correlação).</p>
<p>Relembrando do tópico X, a fórmula da correlação discreta é dada por:</p>
<p><span class="math display">\[g(x,y)=w(x,y)\bigstar f(x,y)=\sum_{s=-a}^a\sum_{t=-b}^bg(s,t)f(x+s,y+t)\]</span></p>
<p>Onde w é o nosso filtro(kernel) e f a nossa imagem. E relacionado a ela temos a fórmula da convolução:</p>
<p><span class="math display">\[g(x,y)=w(x,y)\ast f(x,y)=\sum_{s=-a}^a\sum_{t=-b}^bg(s,t)f(x-s,y-t)\]</span></p>
<p>Como podemos perceber observando as duas equações, essas operações são bem simples, sendo basicamente uma soma de produtos. Na figura <a href="deep-learning-em-visão-computacional.html#fig:cnnconv">7.17</a> temos uma representação de uma passo da convolução, onde podemos observar a seguinte operação:</p>
<p><span class="math display">\[
\text{w}\text{*f}\left(0,0\right)\text{=}\sum_{s}^{}\sum_{t}^{}\text{w}\left(s,t\right)\text{f}\left(0+s,0+t\right)\,\text{=}\,\\
\text{+w}\left(-1,-1\right)\text{f}\left(-1,-1\right)\text{+w}\left(-1,0\right)\text{f}\left(-1,0\right)\text{+w}\left(-1,1\right)\text{f}\left(-1,1\right)\\
\text{+w}\left(0,-1\right)\text{f}\left(0,-1\right)\text{+w}\left(0,0\right)\text{f}\left(0,0\right)\text{+w}\left(0,1\right)\text{f}\left(0,1\right)\\
\text{+w}\left(1,-1\right)\text{f}\left(1,-1\right)\text{+w}\left(0,1\right)\text{f}\left(0,-1\right)\text{+w}\left(1,1\right)\text{f}\left(-1,-1\right)\\
=\,1\cdot0+0\cdot0+\left(-1\right)\cdot0\\
+2\cdot0+0\cdot2+\left(-2\right)\cdot1\\
+1\cdot0+0\cdot9+\left(-1\right)\cdot3\\
=0\,-2-3\,=\,-5
\]</span>
(ref:cnnconv) Convolução de uma imagem de tamanho 5x5 com um kernel de tamanho 3x3 e seu respectivo resultado.</p>
<div class="figure" style="text-align: center"><span id="fig:cnnconv"></span>
<img src="imagens/07-deepLearning/cnn_conv.png" alt="(ref:cnnconv)" width="85%" />
<p class="caption">
Figura 7.17: (ref:cnnconv)
</p>
</div>
<p>O exemplo anterior(figura <a href="deep-learning-em-visão-computacional.html#fig:cnnconv">7.17</a>) foi bem simples, mas sabemos que em várias aplicações não teremos a imagem de entrada representada por apenas uma matriz(configurando uma imagem em tons de cinza) mas em grande parte das vezes estaremos utilizando imagens que contém 3 dimensões, ou seja, teremos uma imagem no modelo RGB, onde estarão presentes três matrizes, cada uma representando um canal de cor. Na figura <a href="deep-learning-em-visão-computacional.html#fig:cnnconv3d">7.18</a> há um exemplo de convolução em imagens RGB, podemos ver que agora nosso kernel é também formado por três matrizes. Uma coisa importante a se notar é que o número de camadas do filtro têm que ser igual ao número de canais da imagem para que a operação de convolução possa ser feita.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnconv3d"></span>
<img src="imagens/07-deepLearning/cnn_conv_3d.png" alt="Convolução de uma imagem de tamanho 5x5x5 com um kernel de tamanho 3x3x3 e seu respectivo resultado." width="85%" />
<p class="caption">
Figura 7.18: Convolução de uma imagem de tamanho 5x5x5 com um kernel de tamanho 3x3x3 e seu respectivo resultado.
</p>
</div>
<p>Na figura <a href="deep-learning-em-visão-computacional.html#fig:cnnconvblock">7.19</a> temos uma representação de uma camada de convolução com mais de um filtro, para cada um deles temos uma saída e como vemos, temos no resultado final um conjunto de dados onde o número de camadas de profundidade(também conhecidas como feature map) corresponderão ao número de filtros aplicados a entrada. Essa saída então pode ser enviada para frente na rede, passando mais vezes por convolução e tendo mais características extraídas.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnconvblock"></span>
<img src="imagens/07-deepLearning/cnn_conv_block.png" alt="Convolução com múltiplos kernels" width="85%" />
<p class="caption">
Figura 7.19: Convolução com múltiplos kernels
</p>
</div>
<p>Os dois exemplos anteriores(figura <a href="deep-learning-em-visão-computacional.html#fig:cnnconv">7.17</a> e <a href="deep-learning-em-visão-computacional.html#fig:cnnconvblock">7.19</a>) também servem para nos mostrar uma das características da convolução que a fazem ser uma boa escolha para se trabalhar com imagens, chamamos essa característica de iterações esparsas(também conhecida como conectividade esparsa)<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 335]</span>. Esse atributo evidencia o fato de que cada unidade da saída, ou pixel, é conectada a somente uma fração das unidades de entrada, no nosso exemplo anterior cada saída é conectada a uma região de 9x9x3=243 pixels da entrada. Isso é muito útil, pois nossa imagem pode ter milhões de pixels, e usando kernels de tamanhos menores, conseguimos detectar pequenas características, como bordas, quinas, etc<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 335]</span>. Nas camadas de convolução, os valores que a rede deverá aprender são os valores presentes nos filtros, então dessa maneira teremos menos parâmetros para aprender e armazenar. Em uma rede neural simples, como vimos no tópico x, uma imagem na entrada significa que cada pixel seria conectado a cada neurônio na próxima camada, resultando assim em uma rede excessivamente grande.</p>
<p>Uma outra característica importante é o de compartilhamento de parâmetros, já que o mesmo filtro é aplicado a diferentes regiões da imagem utilizando os mesmos valores, diferentemente de uma rede neural sem camadas de convolução, onde temos uma matriz com pesos que são usados para somente uma conexão. O compartilhamento de parâmetros nos proporciona uma outra característica, que é a invariância à translação, isso quer dizer que se movemos a posição de um objeto na imagem de entrada, sua representação também será movida na imagem resultante<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 339]</span>.</p>
<div id="padding" class="section level5">
<h5><span class="header-section-number">7.2.1.1.1</span> Padding</h5>
<p>Nos exemplos de convolução(fig x e x) vemos que conforme aplicamos o kernel na imagem de entrada, o tamanho da imagem de saída é reduzido. De fato, se convolucionais uma imagem de tamanho m x n com um filtro de tamanho <span class="math inline">\(k_m x k_n\)</span> a imagem resultante terá uma altura de <span class="math inline">\(m - k_m + 1\)</span> e um comprimento de <span class="math inline">\(n - k_n + 1\)</span>. Esse tipo de convolução, onde a imagem resultante é menor geralmente é chamada de “valid”(válida).</p>
<p>Se queremos a imagem de saída com o mesmo tamanho da imagem de entrada, temos que adicionar mais linhas e colunas em nossa imagem, isso é conhecido como padding. Nesse caso utilizamos a fórmula <span class="math inline">\(m + 2p - k_m + 1\)</span> e $ n + 2p - k_n + 1$ onde p representa o padding. Por exemplo, nas figuras anteriores(fig x e x), se quiséssemos uma saída de igual tamanho a entrada, teríamos que utilizar um padding de <span class="math inline">\(6 + 2p - 3 + 1 = 6 \Rightarrow p = 1\)</span>.</p>
</div>
<div id="stride" class="section level5">
<h5><span class="header-section-number">7.2.1.1.2</span> Stride</h5>
<p>Os exemplos de convolução que vimos anteriormente utilizavam passos de deslocamento de um em um, mas podemos também utilizar passos maiores, pois assim reduzimos o custo computacional ao realizar esses passos intervalados. Isso, claro, tem um impacto no resultado final, diminuindo sua resolução, mas em casos onde não precisamos extrair características finas isso se torna uma boa opção<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 348]</span>.</p>
<p>Quando utilizamos um valor de stride maior que um, isso também afetará o tamanho da saída, e será governado pela seguinte relação<span class="citation">[<a href="#ref-adrian2017" role="doc-biblioref">34</a>, p. 184]</span>:</p>
<p><span class="math display">\[\frac{m+2p-k_m}{s}+1 \  \times \ \frac{n+2p-k_n}{s}+1\]</span></p>
<p>Onde m e n são as dimensões da imagem, <span class="math inline">\(p\)</span> é o padding, <span class="math inline">\(k_m\)</span> e <span class="math inline">\(k_n\)</span> as dimensões do kenel e <span class="math inline">\(s\)</span> o stride. Na figura <a href="deep-learning-em-visão-computacional.html#fig:cnnstride">7.20</a> temos os um exemplo com os passos(Figura <a href="deep-learning-em-visão-computacional.html#fig:cnnstride">7.20</a> a-d) de um convolução com <span class="math inline">\(\text{stride} = 2\)</span> utilizando um kernel de tamanho 3x3 sobre uma imagem de tamanho 5x5 e <span class="math inline">\(\text{padding} = 0\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnstride"></span>
<img src="imagens/07-deepLearning/cnn_stride.png" alt="Representação de uma convolução com stride = 2. Adaptado de [35]." width="70%" />
<p class="caption">
Figura 7.20: Representação de uma convolução com stride = 2. Adaptado de <span class="citation">[<a href="#ref-dumoulin2016" role="doc-biblioref">35</a>]</span>.
</p>
</div>
</div>
</div>
<div id="pooling" class="section level4">
<h4><span class="header-section-number">7.2.1.2</span> Pooling</h4>
<p>Essa é uma camada muito importante, que tem como objetivo realizar a subamostragem(subsampling) da imagem, para reduzir seu tamanho, e consequentemente diminuir o total de memória, processamento e parâmetros necessários, além de refrear o risco de overfitting<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 442]</span> <span class="citation">[<a href="#ref-adrian2017" role="doc-biblioref">34</a>, p. 187]</span> <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 114]</span>.</p>
<p>Como nas camadas de convolução, cada unidade da saída é conectada a uma região de entrada, então também devemos levar em consideração o tamanho, stride e padding. Mas, diferentemente da convolução, o “kernel” ou em outras palavras, a região que nos conectará com a entrada, não terá pesos mas apenas realiza uma operação, sendo as mais comuns o máximo ou a média<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 442]</span>.</p>
<p>Na figura <a href="deep-learning-em-visão-computacional.html#fig:cnnpooling">7.21</a> temos um exemplo de max pooling onde podemos ver seu funcionamento nos passos(Figura <a href="deep-learning-em-visão-computacional.html#fig:cnnpooling">7.21</a>a-d). Este exemplo utiliza uma região de 2x2, o que é muito comum<span class="citation">[<a href="#ref-adrian2017" role="doc-biblioref">34</a>, p. 187]</span>, com um stride de 1.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnpooling"></span>
<img src="imagens/07-deepLearning/cnn_pooling_max.png" alt="Max pooling" width="70%" />
<p class="caption">
Figura 7.21: Max pooling
</p>
</div>
<p>Na Figura <a href="deep-learning-em-visão-computacional.html#fig:cnnpooling3d">7.22</a> temos outro exemplo de max pooling, mas desta vez realizado com uma entrada de maiores dimensões, podemos ver que a operação é realizada em cada uma das camadas do objeto de entrada, e que sua saída contém o mesmo número de camadas da entrada, sendo que é isso que tipicamente ocorre nesse tipo de operação<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 443]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnpooling3d"></span>
<img src="imagens/07-deepLearning/cnn_pooling_max_3d.png" alt="Max pooling em mais dimensões" width="70%" />
<p class="caption">
Figura 7.22: Max pooling em mais dimensões
</p>
</div>
<p>Apesar do pooling ser uma técnica muito difundida, podemos encontrar redes onde seus autores preferiram não utilizar pooling para realizar a subamostragem, mas utilizarem camadas de convolução com valores de stride e padding maiores para conseguir essa redução de dimensão<span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 117]</span> <span class="citation">[<a href="#ref-adrian2017" role="doc-biblioref">34</a>, p. 188]</span>. Essa maneira de trabalhar foi proposta por Springenberg et al. em seu artigo “Striving for Simplicity: The All Convolutional Net” de 2014, onde demonstram que mesmo redes sem camadas de pooling podem ter resultados bons em diferentes bases de dados, como o CIFAR-10 e ImageNet.</p>
</div>
<div id="camadas-totalmente-conectadas" class="section level4">
<h4><span class="header-section-number">7.2.1.3</span> Camadas totalmente conectadas</h4>
<p>As CNN’s geralmente tem várias camadas de convolução seguidas por camadas de ReLU que por sua vez são seguidas por camadas de pooling, e esse processo vai diminuindo as dimensões mxn e aumentando a profundidade, ou seja, a quantidade de camadas de características(conhecido como feature maps)<span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 119]</span> <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 446]</span>. Na Figura <a href="deep-learning-em-visão-computacional.html#fig:cnntypical">7.23</a> temos uma representação desse processo através da topologia da rede, e ao chegar ao final temos uma quantidade grande de camadas com as características extraídas da imagem de entrada e precisamos utilizar essas informações. Nessa mesma figura podemos ver que no final temos camadas totalmente conectadas(Fully connected) que é uma rede neural regular, uma MLP<span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 119]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:cnntypical"></span>
<img src="imagens/07-deepLearning/cnn_typical.png" alt="Típica arquitetura de uma rede neural convolucional[33, p. 447]" width="90%" />
<p class="caption">
Figura 7.23: Típica arquitetura de uma rede neural convolucional<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 447]</span>
</p>
</div>
<p>Na Figura <a href="deep-learning-em-visão-computacional.html#fig:cnnfc">7.24</a> temos dessa última etapa, onde recebemos o resultado das camadas de convolução, neste caso um bloco de dados de 5x5x40 que é então planificado(flattened) em um vetor contínuo de uma dimensão e dado como entrada a uma rede MLP que ao final tem uma camada Softmax que faz a classificação da imagem de entrada.</p>

<div class="figure" style="text-align: center"><span id="fig:cnnfc"></span>
<img src="imagens/07-deepLearning/cnn_fc.png" alt="Camada totalmente conectada[36, p. 120]" width="80%" />
<p class="caption">
Figura 7.24: Camada totalmente conectada<span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 120]</span>
</p>
</div>
</div>
</div>
<div id="por-que-usar-convoluções" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Por que usar convoluções</h3>
<p>Até agora entendemos os blocos de construção das CNN’s e os motivos pelos quais são usados. Devemos saber que a convolução não é apenas usada por ser mais eficiente no tratamento de imagens mas também que a ideia de utilizá-la teve inspiração em nosso próprio sistema visual.</p>
<div id="córtex-visual" class="section level4">
<h4><span class="header-section-number">7.2.2.1</span> Córtex visual</h4>
<p>Como as próprias redes neurais, as CNN’s foram bio-inspiradas em estudos sobre o córtex visual do cérebro que começaram a ocorrer desde 1980<span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 431]</span>, principalmente a partir dos trabalhos de David H. Hubel e Torsten Wiesel, onde foram realizados experimentos em animais, que permitiram aos dois pesquisadores deduzirem o funcionamento da estrutura do córtex visual.</p>
<p>De uma maneira simplificada, os sinais de luz recebidos pela retina são transmitidos ao cérebro através do nervo óptico, após isso chegam ao córtex visual primário que é formado principalmente por dois tipos de células<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 365]</span>:</p>
<ul>
<li><p>Células simples: essas células têm comportamentos que podem ser representados por funções lineares em uma imagem em uma pequena área conhecida como campo receptivo<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 365]</span> <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 432]</span>. Esse tipo de célula inspirou as unidades detectoras mais simples nas CNNs.</p></li>
<li><p>Células complexas: também respondem a características da imagem, como as células simples, mas são invariantes a posição, ou seja, não fazem grande distinção de onde a característica aparece. Esse tipo de célula inspirou as unidades de pooling, que veremos mais adiante<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 365]</span>.</p></li>
</ul>
<p>Anatomicamente, quanto mais nos aprofundamos nas camadas do cérebro, mais camadas análogas à convolução e pooling são passadas, e encontramos células mais especializadas que respondem a padrões específicos sem serem afetadas por transformações na entrada. Sendo que até chegar nessas camadas mais profundas, é realizado uma sequência de detecções seguidas de camadas de pooling<span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 365]</span>.</p>
</div>
</div>
<div id="redes-cnns-clássicas" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Redes CNN’s clássicas</h3>
<div id="lenet" class="section level4">
<h4><span class="header-section-number">7.2.3.1</span> LeNet</h4>
<p>Após estudar os principais blocos de construção de uma rede convolucional (CNN) - camada de convolução (<em>convolutional layer</em>), camada de redução (<em>pooling layer</em>) e camadas totalmente conectadas (<em>fully connected layer</em>) - torna-se mais fácil comparar as arquiteturas CNN’s e perceber que mesmo com as diferenças elas apresentam um padrão na combinação das camadas. Normalmente as arquiteturas CNN’s possuem uma sequência intercalada de camada de convolução, seguida por uma camada <em>pooling</em>, que se repete até a ponta da rede onde se encontram algumas camadas totalmente conectadas com estrutura semelhante às redes MLP. Esta estrutura básica pode ser vista na Figura <a href="deep-learning-em-visão-computacional.html#fig:lenet">7.25</a>.</p>
<p>À medida que são realizadas as operações ao longo da rede se percebe que os mapas ficam cada vez menores e que as camadas ficam mais profundas, ou seja, aumentam a quantidade de mapas em uma mesma camada. Como a camada de saída geralmente se apresenta como um vetor de probabilidades para as classes de predição, existe uma transição da representação dos dados em mapas para vetor, a partir do processo de <em>flatten</em>, que ocorre antes da primeira camada totalmente conectada.</p>

<div class="figure" style="text-align: center"><span id="fig:lenet"></span>
<img src="imagens/07-deepLearning/lenet.png" alt="Rede Convolucional LeNet - A entrada é uma imagem de um número escrito à mão e a saída um vetor com a probabilidade para cada um dos dez dígitos de 0 à 9 [37, p. 250]" width="100%" />
<p class="caption">
Figura 7.25: Rede Convolucional LeNet - A entrada é uma imagem de um número escrito à mão e a saída um vetor com a probabilidade para cada um dos dez dígitos de 0 à 9 <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 250]</span>
</p>
</div>
<p>A rede LeNet foi uma das primeiras CNN’s que demonstrou potencial de aplicação em visão computacional. A rede foi criada por Yann LeCun em 1998 com propósito de reconhecimento de números manuscritos. A LeNet foi posteriormente adaptada para reconhecer dígitos para os depósitos em máquinas ATM, e ainda existem caixas eletrônicos que executam o código desenvolvido por Yann e seu colega Leon Bottou <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 248]</span>. A rede também foi amplamente utilizada para reconhecimento de dígitos do dataset do MNIST <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 449]</span>, este dataset foi abordado no tópico de backpropagation.</p>
<p>Na Figura <a href="deep-learning-em-visão-computacional.html#fig:lenet">7.25</a> consideramos como entrada uma imagem padrão do MNIST, de tamanho 28 × 28 pixels e com um canal em escala de cinza. No esquema geral da rede LeNet (Figura <a href="deep-learning-em-visão-computacional.html#fig:lenet2">7.26</a>) temos uma visão mais clara da combinação das camadas, em que após a camada de entrada existem duas camadas convolucionais, intercaladas com duas camadas de <em>pooling</em> e na ponta três camadas totalmente conectadas.</p>
<p>Cada camada de convolução utiliza um filtro 5x5 e uma função de ativação sigmóide. A primeira camada de convolução tem seis canais ou mapas, enquanto a segunda tem 16. A operação de <em>pooling</em> envolve um filtro 2x2 que calcula a média, por isso é identificada como “AvgPool”, e utiliza stride 2 para que cada mapa da camada anterior tenha uma redução pela metade ao longo da largura e da altura, descartando 75% das ativações. O tamanho das três camadas totalmente conectadas (<em>Full Conect</em>) são respectivamente, 120, 84, e 10. A última camada, FC(10), corresponde ao número possível de classes, neste caso 10 em razão dos dígitos de 0 à 9. A função de ativação na última camada é uma função gaussiana.</p>

<div class="figure" style="text-align: center"><span id="fig:lenet2"></span>
<img src="imagens/07-deepLearning/lenet2.png" alt="Esquema geral das camadas na rede LeNet - Esquema da rede LeNet com a sequência de camadas convolucionais (Conv), pooling (AvgPool) e camadas totalmente conectadas (FC) [37, p. 252]" width="30%" />
<p class="caption">
Figura 7.26: Esquema geral das camadas na rede LeNet - Esquema da rede LeNet com a sequência de camadas convolucionais (Conv), pooling (AvgPool) e camadas totalmente conectadas (FC) <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 252]</span>
</p>
</div>
<p>Para compreender os efeitos de cada camada sobre o conjunto de dados apresentamos na Tabela <a href="#tab:tablelenet">7.1</a>, as dimensões das saídas de cada camada. Comentamos que de um bloco de convolução para o outro ocorre um aumento do número de canais (C) de 6 para 16, entre as camadas de <em>pooling</em> estes valores não são alterados, pois o processo só reduz a largura (W) e altura (H) dos canais. Nas camadas totalmente conectadas, as dimensões são reduzidas até se obter o tamanho do número de classes.</p>
<p>É comum nas redes CNN’s que a quantidade de canais praticamente dobre depois de uma camada <em>pooling</em> visto que ocorre uma redução pela metade nas dimensões dos mapas. Assim, é possível aumentar o número de mapas, tornando mais sensível a identificação de características de baixo nível, como bordas e texturas, sem aumentar drasticamente o número de parâmetros e recursos computacionais <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 258]</span>. Como a primeira camada de convolução aplica <em>padding</em> de 2, os mapas mantêm na saída a mesma dimensão que a imagem original (28x28), entretanto na segunda camada não se tem o <em>padding</em>, o que reduz em 4 pixels a largura e altura dos mapas.</p>
<p>Tabela (#tab:tablelenet) Tabela das configurações, parâmetros e informações das camadas na LeNet - Resumo das configurações das principais camadas da LeNet, como o número de canais e tamanho dos filtros. Apresentação de uma estimativa do número de parâmetros e da quantidade de memória para treinar a rede..</p>
<table>
<thead>
<tr class="header">
<th align="center">Layer</th>
<th align="center">Canais - C</th>
<th align="center">Tamanho - H e W</th>
<th align="center">Filtro - K</th>
<th align="center">Memória (kB)</th>
<th align="center">Parâmetros</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Inputs</td>
<td align="center">1</td>
<td align="center">28</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Convolucional 1</td>
<td align="center">6</td>
<td align="center">28</td>
<td align="center">5</td>
<td align="center">18</td>
<td align="center">156</td>
</tr>
<tr class="odd">
<td align="center">Avg Pooling 1</td>
<td align="center">6</td>
<td align="center">14</td>
<td align="center">2</td>
<td align="center">5</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Convolucional 2</td>
<td align="center">16</td>
<td align="center">10</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">2416</td>
</tr>
<tr class="odd">
<td align="center">Avg Pooling 2</td>
<td align="center">16</td>
<td align="center">5</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Flatten</td>
<td align="center">400</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">1.6</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center">Full Connect 1</td>
<td align="center">120</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">0.5</td>
<td align="center">48120</td>
</tr>
<tr class="even">
<td align="center">Full Connect 2</td>
<td align="center">84</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">0.3</td>
<td align="center">10164</td>
</tr>
<tr class="odd">
<td align="center">Full Connect 3</td>
<td align="center">10</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">0.04</td>
<td align="center">850</td>
</tr>
<tr class="even">
<td align="center">Total</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">33</td>
<td align="center">61706</td>
</tr>
</tbody>
</table>
<p>Ao longo dos anos surgiram variações deste modelo e a diferença mais evidente entre as redes é o número de camadas, que foi aumentando ao longo dos anos tornando as redes mais profundas. Ao aumentar o número de camadas se percebia que o desempenho das redes tendia a melhorar, entretanto foram surgindo algumas limitações. Quanto maior a quantidade de dados, mais memória computacional é exigida, sendo que esta capacidade depende dos requisitos do <em>hardware</em>.</p>
<p>Para avaliar a quantidade de memória utilizada no treinamento da rede LeNet vamos utilizar um cálculo aproximado com base na quantidade de elementos de saída em cada camada. O número de elementos é multiplicado pela quantidade de <em>bytes</em> necessária para armazenar cada elemento <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Considerando que os dados em ponto flutuante ocupam 32 <em>bit</em>, então são 4 <em>bytes</em> por elemento. Para facilitar a visualização dos resultados foram transformados para <em>kilobyte</em> (kB), e por isso foram divididos pelo fator 1024 (1 kB = 1024 B). Na equação a seguir exemplificamos o cálculo da quantidade de memória para a primeira camada de convolução da rede LeNet:</p>
<p>Número de elementos na saída = C x H x W = 6 x 28 x 28 = 4704</p>
<p><em>Bytes</em> por elemento = 4 (por 32-<em>bit</em> ponto flutuante)</p>
<p>kB = (Número de elementos) x (<em>Bytes</em> por elemento)/1024 = 4704 x 4/1024 = 18,38 kB</p>
<p>O parâmetro C identifica o número de canais ou mapas da camada e o termo H e W, a altura e largura do elemento de saída da camada, respectivamente. Como identificado na equação e na tabela, a quantidade aproximada de memória para a primeira camada seria 18 kB, e no total para a rede 33 kB. As primeiras camadas tendem a precisar de mais memória devido a maior dimensão (W e H) dos canais <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>
<p>Aumentar o número de camadas também exige que mais parâmetros sejam considerados, o que afeta tanto o tempo de treinamento quanto o seu desempenho, pois se não ocorrer uma otimização adequada dos parâmetros pode ser maior a probabilidade de ocorrer <em>overfitting</em> <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 230]</span>. Para determinar aproximadamente a quantidade de parâmetros relacionados com cada camada considerou-se os pesos relacionados aos filtros de cada mapa, calculados como o produto entre as dimensões do filtro (KxK), a quantidade de canais do elemento de entrada e a quantidade de canais de saída da camada <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Considerou-se também como parâmetros, os bias associados a cada canal de saída. Nas camadas totalmente conectadas, o número de parâmetros é determinado como produto da quantidade de elementos de entrada com a quantidade de elementos da saída da camada somado com o número de bias. A seguir exemplificamos os cálculos para a primeira camada de convolução:</p>
<p>Número de Pesos = <span class="math inline">\(C_{\text{saída}}\)</span> x <span class="math inline">\(C_{\text{entrada}}\)</span> x K x K = 6 x 1 x 5 x 5 = 150</p>
<p>Número de Bias = 6</p>
<p>Número de parâmetros = 150 + 6 = 156</p>
<p>Considerando que o filtro na primeira camada é de tamanho 5x5, que a entrada só apresenta um canal e que são 6 canais na camada de convolução, a primeira camada de convolução considera aproximadamente 156 parâmetros. Na tabela também está a quantidade de parâmetros relacionados com cada camada e o total aproximado de parâmetros para a rede LeNet é 61706. À medida que aumenta o número de canais na camada de convolução mais parâmetros são necessários. De maneira geral, a maior parte dos parâmetros se deve às camadas totalmente conectadas devido ao maior número de conexões <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>
</div>
<div id="alexnet" class="section level4">
<h4><span class="header-section-number">7.2.3.2</span> AlexNet</h4>
<p>Atualmente existem várias arquiteturas de redes CNNs utilizadas para aplicações na visão computacional. A evolução destas redes pode ser compreendida a partir dos resultados da competição <em>ImageNet Large Scale Visual Recognition Challenge</em> (ILSVRC). O principal objetivo da competição era avaliar algoritmos para detecção de objetos e classificação de imagens. A primeira edição da competição, em 2010, envolvia 1,2 milhões de imagens para o treinamento, sendo 1000 categorias de objetos. Nos dois primeiros anos de competição as redes CNNs ainda não tinham sido as primeiras colocadas, porém a partir de 2012 os modelos CNNs começaram a liderar a competição <span class="citation">[<a href="#ref-imagenet2020" role="doc-biblioref">39</a>]</span>. O progresso das redes pode ser avaliado com base na taxa de erro dos modelos que em sete anos caiu de aproximadamente 26%, no segundo ano da competição, para 2,3% na última edição da competição em 2017 <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>, como apresentado no gráfico da Figura <a href="deep-learning-em-visão-computacional.html#fig:imagenet">7.27</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:imagenet"></span>
<img src="imagens/07-deepLearning/imagenet.png" alt="Taxa de erro dos modelos de melhor desempenho na competição ImageNet - O desempenho dos modelos na competição ImageNet Large Scale Visual Recognition Challenge (ILSVRC) era avaliado principalmente pela taxa de erro. No gráfico são apresentados os modelos que venceram em cada edição da competição, que ocorreu de 2010 à 2017, e também redes que se tornaram populares como a VGG [38]" width="100%" />
<p class="caption">
Figura 7.27: Taxa de erro dos modelos de melhor desempenho na competição ImageNet - O desempenho dos modelos na competição ImageNet Large Scale Visual Recognition Challenge (ILSVRC) era avaliado principalmente pela taxa de erro. No gráfico são apresentados os modelos que venceram em cada edição da competição, que ocorreu de 2010 à 2017, e também redes que se tornaram populares como a VGG <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>
</p>
</div>
<p>Para conhecer um pouco dos diferentes modelos de redes CNN’s e perceber algumas diferenças, e estruturas que tiveram bom desempenho e permaneceram em modelos mais atuais, destacaremos a seguir quatro arquiteturas que ficaram bastante conhecidas e tiveram destaque na competição. A rede AlexNet foi a primeira CNN que venceu a competição ImageNet, no ano de 2012 com uma taxa de erro de 16,4%. A rede VGG não liderou a competição em 2014, porém é um dos modelos com bastante popularidade e que apresentou uma estrutura em blocos estabelecidos com base em regras. No ano de 2014, a CNN GoogLeNet que venceu a competição foi o ponto de partida para as redes <em>Inceptions</em>. A rede Residual (ResNet) em 2015 além de aproveitar as técnicas de maior desempenho das outras redes também implementou uma abordagem que possibilitou aumentar para mais de 100 camadas.</p>
<p>A rede AlexNet foi desenvolvida por Alex Krizhevsky, Ilya Sutskever, e Geoffrey Hinton <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 450]</span>. Esta rede é bem semelhante a LeNet-5, porém apresenta mais camadas. Por ser uma rede mais profunda, exigindo maior quantidade de memória, a rede original precisou ser distribuída entre dois GPU’s de 3 GB de forma física <span class="citation">[<a href="#ref-krizhevsky2012" role="doc-biblioref">40</a>]</span>. Desta forma, a rede foi desenhada como na Figura <a href="deep-learning-em-visão-computacional.html#fig:imagenet">7.27</a>, com uma estrutura de fluxo de dados duplo para que cada GPU recebesse metade do modelo.</p>

<div class="figure" style="text-align: center"><span id="fig:alexnet"></span>
<img src="imagens/07-deepLearning/alexnet.png" alt="Arquitetura da rede AlexNet - representada como a combinação de duas redes idênticas, pois originalmente o treinamento ocorreria com a distribuição dos dados entre duas GPU’s [40]" width="100%" />
<p class="caption">
Figura 7.28: Arquitetura da rede AlexNet - representada como a combinação de duas redes idênticas, pois originalmente o treinamento ocorreria com a distribuição dos dados entre duas GPU’s <span class="citation">[<a href="#ref-krizhevsky2012" role="doc-biblioref">40</a>]</span>
</p>
</div>
<p>Como esquematizado na Figura <a href="deep-learning-em-visão-computacional.html#fig:alexnet">7.28</a>, a AlexNet tem 5 camadas de convoluções, sendo as três primeiras intercaladas por camadas <em>pooling</em>. A diferença mais visível entre as arquiteturas AlexNet e LeNet são as três camadas de convolução a mais na rede AlexNet, que estão seguidas uma depois da outra sem camada <em>pooling</em> entre elas. Como as imagens de entrada são maiores que do dataset MNIST abordado na rede LeNet, os filtros de convolução na entrada são maiores (11x11) e se utiliza <em>stride</em> 4. Na segunda camada de convolução os filtros têm tamanho 5x5, e nas demais camadas de convolução se utilizam filtros 3x3.</p>
<p>Com a descoberta que as funções de ativação ReLUs nas camadas de convolução e que o <em>maxpooling</em> melhoram o desempenho das redes, a maioria dos modelos foram construídos utilizando estes artifícios <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 250]</span>. Os filtros <em>maxpooling</em> de tamanho 3x3 e <em>stride</em> 2 reduzem a dimensão dos canais com base no maior valor do campo de recepção. Com exceção da primeira camada, todas as demais camadas de convolução têm <em>padding</em> para que a dimensão dos canais não seja alterada após as convoluções.</p>
<p>As três últimas camadas são totalmente conectadas e apresentam respectivamente os tamanhos, 4096, 4096 e 1000. A camada de saída tem dimensão 1000 devido ao número de classes possíveis da competição ImageNet e a função de ativação é a Softmax.</p>

<div class="figure" style="text-align: center"><span id="fig:lenetalexnet"></span>
<img src="imagens/07-deepLearning/lenetalexnet.png" alt="Comparação das redes AlexNet e LeNet - a - Rede AlexNet; b - Rede LeNet. Estes esquemas gerais das camadas apresentam que a principal diferença das redes é que a AlexNet é mais profunda, com três camadas de convolução a mais do que a LeNet [37, p. 261]" width="40%" />
<p class="caption">
Figura 7.29: Comparação das redes AlexNet e LeNet - a - Rede AlexNet; b - Rede LeNet. Estes esquemas gerais das camadas apresentam que a principal diferença das redes é que a AlexNet é mais profunda, com três camadas de convolução a mais do que a LeNet <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 261]</span>
</p>
</div>
<p>O mesmo padrão para as dimensões dos elementos de saída das camadas visto em LeNet é visto na Tabela #tab:tablealexnet para a rede AlexNet. Enquanto o tamanho dos canais diminui entre uma camada de convolução para outra, a quantidade de canais cresce, sendo 96 na primeira, seguida por 256, 384, 384 e 256. Após o processo de <em>flatten</em>, a dimensão das camadas é reduzida até se estabelecer o tamanho do vetor de classes de predição.</p>
<p>Ao comparar a quantidade de memória e o número de parâmetros aproximados como foi descrito no tópico anterior (LeNet) observa-se que certamente a quantidade de memória exigida aumenta e o número de parâmetros também. Os cálculos aproximados indicam que enquanto a memória necessária para o treinamento da rede LeNet seria de 33 kB, na AlexNet seria aproximadamente 3 GB. O número de parâmetros calculados para LeNet foi 62 mil e para AlexNet 62 milhões. Mas de maneira geral nos dois modelos, as primeiras camadas demandam mais memória, enquanto que as camadas totalmente conectadas precisam de mais parâmetros.</p>
<p>Tabela (#tab:tablealexnet) Tabela das configurações, parâmetros e informações das camadas na AlexNet - Resumo das configurações das principais camadas da AlexNet, como o número de canais e tamanho dos filtros. Apresentação de uma estimativa do número de parâmetros e da quantidade de memória para treinar a rede.</p>
<table>
<thead>
<tr class="header">
<th align="center">Layer</th>
<th align="center">Canais - C</th>
<th align="center">Tamanho - H e W</th>
<th align="center">Filtro - K</th>
<th align="center">Memória (kB)</th>
<th align="center">Parâmetros</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Inputs</td>
<td align="center">3</td>
<td align="center">227</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Convolucional 1</td>
<td align="center">96</td>
<td align="center">55</td>
<td align="center">11</td>
<td align="center">1134</td>
<td align="center">35</td>
</tr>
<tr class="odd">
<td align="center">Max Pooling 1</td>
<td align="center">96</td>
<td align="center">27</td>
<td align="center">3</td>
<td align="center">273</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Convolucional 2</td>
<td align="center">256</td>
<td align="center">27</td>
<td align="center">5</td>
<td align="center">729</td>
<td align="center">615</td>
</tr>
<tr class="odd">
<td align="center">Max Pooling 2</td>
<td align="center">256</td>
<td align="center">13</td>
<td align="center">3</td>
<td align="center">169</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Convolucional 3</td>
<td align="center">384</td>
<td align="center">13</td>
<td align="center">3</td>
<td align="center">254</td>
<td align="center">885</td>
</tr>
<tr class="odd">
<td align="center">Convolucional 4</td>
<td align="center">384</td>
<td align="center">13</td>
<td align="center">3</td>
<td align="center">254</td>
<td align="center">1327</td>
</tr>
<tr class="even">
<td align="center">Convolucional 5</td>
<td align="center">256</td>
<td align="center">13</td>
<td align="center">3</td>
<td align="center">169</td>
<td align="center">885</td>
</tr>
<tr class="odd">
<td align="center">Max Pooling 3</td>
<td align="center">256</td>
<td align="center">6</td>
<td align="center">3</td>
<td align="center">36</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Flatten</td>
<td align="center">9216</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">36</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center">Full Connect 1</td>
<td align="center">4096</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">16</td>
<td align="center">37753</td>
</tr>
<tr class="even">
<td align="center">Full Connect 2</td>
<td align="center">4096</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">16</td>
<td align="center">16781</td>
</tr>
<tr class="odd">
<td align="center">Full Connect 3</td>
<td align="center">1000</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">4</td>
<td align="center">4097</td>
</tr>
<tr class="even">
<td align="center">Total</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">3090</td>
<td align="center">62378</td>
</tr>
</tbody>
</table>
</div>
<div id="vgg" class="section level4">
<h4><span class="header-section-number">7.2.3.3</span> VGG</h4>
<p>A rede VGG foi construída dentro do grupo Visual Geometry Group (VGG) na Universidade de Oxford pelos pesquisadores Karen Simonyan e Andrew Zisserman <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 265]</span>. Até agora vimos que as redes LeNet e AlexNet apresentam a sua estrutura em duas partes principais, uma com as camadas iniciais contendo uma combinação de camadas convolucionais e <em>pooling</em>, e na outra parte estão as camadas totalmente conectadas (<em>Full Conect</em>) na ponta da rede. Nestas redes geralmente é necessário selecionar individualmente vários parâmetros, por exemplo, nas camadas de convolução selecionam-se o número de canais, tamanho dos filtros, do <em>padding</em> e do <em>stride</em>. Na camada <em>pooling</em>, os hiperparâmetros são o tamanho do filtro e do <em>stride</em>. Em geral, estas duas redes não apresentam um guia geral de como selecionar os parâmetros, o que torna mais complexo desenhar novas redes e que sejam mais profundas.</p>
<p>O que se destacou na VGG em relação aos dois modelos anteriores é a introdução de princípios para estabelecer a estrutura da rede, o que permitiu a construção de modelos mais profundos <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 265]</span>. Outro aspecto característico da VGG é a estrutura em blocos na parte da rede com as camadas convolucionais, em que cada bloco apresenta camadas convolucionais em sequência e na ponta uma camada <em>pooling</em>. Enquanto o modelo AlexNet na Figura <a href="deep-learning-em-visão-computacional.html#fig:alexnetvgg">7.30</a> apresenta 5 camadas convolucionais, a VGG apresenta cinco blocos com número variável de camadas de convolução, mas que em geral os primeiros blocos apresentam menos camadas. Da mesma forma que a AlexNet, na ponta da rede estão as três camadas totalmente conectadas, com dimensões também iguais nos dois modelos, e uma função de ativação SoftMax na saída.</p>
<p>Na Figura <a href="deep-learning-em-visão-computacional.html#fig:alexnetvgg">7.30</a> está a representação da arquitetura VGG com 16 camadas, em que os primeiros dois blocos apresentam duas camadas convolucionais e os três últimos blocos possuem três camadas convolucionais. As camadas de convolução dobram de tamanho a cada bloco, sendo que cada camada no primeiro bloco tem 64 canais, no seguinte 128 e assim por diante até 512 no último bloco. A utilização da função de ativação ReLu na camada de convolução e <em>pooling</em> pelo valor máximo são estratégias que apresentaram bom desempenho na AlexNet e continuaram em outros modelos, como no VGG.</p>
<p>Os principais princípios de design da VGG estabelecem que todos os filtros de convolução são 3x3 e que utilizam <em>stride</em> 1 e <em>pad</em> 1 e que os filtros <em>maxpooling</em> são 2x2 e com <em>stride</em> 2. Após cada camada pooling, o número de canais dobra na camada de convolução. A ideia de fixar o tamanho dos filtros convolucionais partiu da percepção de que a combinação de 2 filtros 3x3 apresenta um campo receptivo equivalente a um filtro 5x5, e que 3 filtros 3x3 equivalem a um de 7x7 <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 212]</span>. Fixando o tamanho dos filtros e estabelecendo com <em>pad</em> e <em>stride</em> 1 que a dimensão dos canais não se altere entre as camadas convolucionais, o único hiperparâmetro que precisa ser otimizado é a quantidade de camadas em cada bloco.</p>
<p>Ao utilizar filtros 3x3 menores porém em maior quantidade que os utilizados na AlexNet (11x11 e 5x5) se inclui mais não linearidade, permitindo que a rede aprenda mais características de baixo nível <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 212]</span>. Aumentando a profundidade da rede com mais camadas de convolução são incluídas mais funções não lineares de ativação. Mesmo sendo redes mais profundas, esta estratégia de utilizar filtros menores diminui o número de parâmetros. Considerando que duas camadas em sequência tem C canais cada uma, ao utilizar dois filtros 3x3, o número total de parâmetros é 2x3x3x<span class="math inline">\(C^2\)</span> = 18<span class="math inline">\(C^2\)</span>, que é menor ao comparar na situação de um único filtro 5x5 com 25<span class="math inline">\(C^2\)</span> parâmetros <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>
<p>Certamente dobrar o número de canais entre os blocos deve fazer com que o número de parâmetros cresça rapidamente, e por isso se padronizou os filtros <em>maxpooling</em> para que se reduza as dimensões dos canais pela metade. Controlando o número de ativações que passam para as próximas camadas é possível manter aproximadamente constante o número de operações. Avaliando superficialmente que o número de operações é dado como a quantidade total de multiplicações e adições, podemos calcular para cada camada como produto de quatro parâmetros <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>: tamanho do filtro (KxK), as dimensões do canais de entrada (HxW), a quantidade de canais de entrada (<span class="math inline">\(C_{\text{entrada}}\)</span>) e canais na saída (<span class="math inline">\(C_{\text{saída}}\)</span>):</p>
<p>Número de operações = Número de elementos de saída x Operações por elemento de saída
= (<span class="math inline">\(C_{\text{saída}}\)</span> x H x W) x (<span class="math inline">\(C_{\text{entrada}}\)</span> x K x K)
= (2C x HW) x (2C x 3 x 3) = 36 HW<span class="math inline">\(C^2\)</span></p>
<p>No caso de duas camadas de convolução com filtros 3x3 e separadas por um <em>pooling</em>, com redução pela metade da dimensão dos canais (2Hx2W <span class="math inline">\(\rightarrow\)</span> HxW) e dobrando o número de canais (C <span class="math inline">\(\rightarrow\)</span> 2C), a quantidade de pesos aumenta de 9<span class="math inline">\(C^2\)</span> para 36<span class="math inline">\(C^2\)</span>, porém o número de operações se mantêm em 36HW<span class="math inline">\(C^2\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:alexnetvgg"></span>
<img src="imagens/07-deepLearning/alexnetvgg.png" alt="Comparação das redes VGG e AlexNet - Comparação das redes VGG e AlexNet com base na estrutura geral das camadas. Enquanto a parte final das redes é semelhante em relação às camadas totalmente conectadas, a VGG se diferencia por ser mais profunda e apresentar um padrão das camadas convolucionais organizadas em blocos [38]" width="70%" />
<p class="caption">
Figura 7.30: Comparação das redes VGG e AlexNet - Comparação das redes VGG e AlexNet com base na estrutura geral das camadas. Enquanto a parte final das redes é semelhante em relação às camadas totalmente conectadas, a VGG se diferencia por ser mais profunda e apresentar um padrão das camadas convolucionais organizadas em blocos <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>
</p>
</div>
</div>
<div id="googlenet-e-inception" class="section level4">
<h4><span class="header-section-number">7.2.3.4</span> GoogLenet e Inception</h4>
<p>Ao acompanhar a evolução das CNN’s podemos perceber que a principal estratégia para aumentar o desempenho na classificação das imagens foi aumentar o número de camadas que guardam os pesos das redes. As redes AlexNet e VGG-16 foram desenvolvidas com 8 e 16 camadas, respectivamente. À medida que as redes se tornavam mais profundas surgiu o dilema de como tornar os algoritmos mais eficientes, visto que mais camadas significava maior quantidade de parâmetros e operações, exigindo maior recursos computacionais. Comparando as redes na Figura <a href="deep-learning-em-visão-computacional.html#fig:neuralevolution">7.31</a> é possível verificar a acurácia das redes, a quantidade de parâmetros e o número de operações. Verifica-se que para a rede VGG-16 alcançar resultados melhores que a rede AlexNet foi necessário mais do que duplicar a quantidade de parâmetros, de aproximadamente 65 milhões na AlexNet para um pouco mais que 130 milhões na VGG-16.</p>
<p>Na competição da ImageNet de 2014, um grupo de pesquisa da Google liderado por Christian Szegedy propôs a arquitetura GoogLeNet que deveria ao mesmo tempo garantir boa performance e ser mais eficiente que os modelos existentes <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 452]</span>. O modelo não só ganhou a competição como atendeu os seus requisitos, pois mesmo sendo uma rede com 22 camadas, mais que a VGG-16, precisou de 12 vezes menos parâmetros, 13 milhões em vez de 138 milhões <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 217]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:neuralevolution"></span>
<img src="imagens/07-deepLearning/neuralevolution.png" alt="Gráfico da evolução das redes neurais CNN’s - O desempenho das redes é avaliado pela acurácia versus o número de operações necessárias para uma única etapa forward. O raio dos círculos é proporcional ao número de parâmetros, sendo que a legenda no canto inferior direito indica uma referência de 5x\(10^6\) à 155x\(10^6\) [41]" width="70%" />
<p class="caption">
Figura 7.31: Gráfico da evolução das redes neurais CNN’s - O desempenho das redes é avaliado pela acurácia versus o número de operações necessárias para uma única etapa forward. O raio dos círculos é proporcional ao número de parâmetros, sendo que a legenda no canto inferior direito indica uma referência de 5x<span class="math inline">\(10^6\)</span> à 155x<span class="math inline">\(10^6\)</span> <span class="citation">[<a href="#ref-canziani2016" role="doc-biblioref">41</a>]</span>
</p>
</div>
<p>Para compreender a rede GoogLenet podemos dividi-la em três partes (Figura <a href="deep-learning-em-visão-computacional.html#fig:googlenet">7.32</a>), na primeira parte, as camadas da entrada são semelhantes às redes AlexNet e VGG, na segunda parte, são os blocos inceptions característicos desta rede, e a última parte se refere a estrutura de classificação. A primeira parte contém dois blocos com uma sequência de camadas convolucionais intercaladas com <em>pooling</em> 3x3. No primeiro bloco têm apenas uma camada de convolução 7x7, com <em>stride</em> 2 e <em>padding</em> 3, e uma camada <em>pooling</em> com <em>stride</em> 2. Ao final destas duas camadas, o elemento tem 64 canais e teve uma redução por 4 em sua dimensão (H e W). No segundo bloco são duas camadas de convolução, a primeira com filtro 1x1 e 64 canais e a segunda 3x3 com 192 canais, sendo que apenas o <em>pooling</em> 3x3 na ponta do bloco altera as dimensões dos canais pela metade.</p>
<p>O principal papel destes dois blocos é reduzir de maneira considerável as dimensões da imagem, visto que a maior parte da memória exigida se deve às primeiras camadas <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Considerando que nesta etapa ocorre uma redução em 8 vezes das dimensões da imagem, uma entrada 224x224 ao se reduzir para 28x28 utilizará aproximadamente 7,5 MB de memória enquanto a mesma redução no VGG-16 precisa de 42,9 MB, quase 6 vezes mais que a GoogLenet <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Também ao passar para as próximas camadas uma imagem de menor dimensão também se reduz o número de operações e a quantidade de parâmetros para treinar a rede.</p>

<div class="figure" style="text-align: center"><span id="fig:googlenet"></span>
<img src="imagens/07-deepLearning/googlenet.png" alt="A estrutura geral da rede GoogLenet pode ser dividida em três partes: Parte A - Semelhante a AlexNet e LeNet, contém uma sequência de camadas convolucionais e pooling para reduzir as dimensões da imagem; Parte B - Módulos Inceptions separados por camadas pooling; Parte C - Camada de pooling global e uma Full Conect par classificação [36, p. 224]" width="50%" />
<p class="caption">
Figura 7.32: A estrutura geral da rede GoogLenet pode ser dividida em três partes: Parte A - Semelhante a AlexNet e LeNet, contém uma sequência de camadas convolucionais e pooling para reduzir as dimensões da imagem; Parte B - Módulos Inceptions separados por camadas pooling; Parte C - Camada de pooling global e uma Full Conect par classificação <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 224]</span>
</p>
</div>
<p>Outra técnica para tornar a rede mais eficiente foi incluir uma camada de AvgPool global antes da camada de classificação <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 455]</span>. Nos modelos CNN’s anteriores era comum incluir um <em>flattering</em> para converter os dados em um vetor, perdendo a informação espacial, para ser compatível com as camadas totalmente conectadas que faziam a classificação. Estas últimas camadas acabam sendo as responsáveis pela maior parte dos parâmetros. No modelo VGG-16, por exemplo, as 3 camadas totalmente conectadas geram aproximadamente 123,6 milhões de parâmetros, quase 90% dos parâmetros totais <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>
<p>Em vez de adotar o <em>flatering</em>, o GoogLenet utiliza um filtro de média de mesma dimensão do elemento de entrada, retornando a média dos mapas para cada posição do vetor. Como o vetor de saída já tem um tamanho reduzido é necessário incluir apenas uma camada totalmente conectada com as 1000 classes. Como a camada de média global não precisa de parâmetros, e considerando que retorna um vetor 1024, são necessários aproximadamente 1 milhão de parâmetros na camada totalmente conectada, 100 vezes menos que na VGG <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Na última camada, assim como no VGG, associa-se uma ativação Softmax, enquanto nas camadas de convolução é a ReLu.</p>
<p>Já foi comentado sobre a primeira parte e a última da rede GoogLenet, a seção intermediária que estudaremos inclui os módulos <em>Inceptions</em> que se tornaram elementos característicos das redes mais modernas intituladas <em>Inceptions</em>. Cada módulo se assemelha aos blocos do VGG, em que estão presentes algumas camadas convolucionais em sequência e na ponta uma camada <em>pooling</em>. No caso do VGG se viu que para reduzir o número de hiperparâmetros se fixou o tamanho dos filtros em 3x3, e o parâmetro variável ficou sendo o número de camadas convolucionais. A ideia dos <em>Inceptions</em> (Figura <a href="deep-learning-em-visão-computacional.html#fig:inceptionmodule">7.33</a>) é não se preocupar nem com tamanho dos filtros e nem com o número de camadas no módulo, pois cada módulo consiste em uma combinação de filtros de diferentes tamanhos arranjados de uma forma fixa <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 217]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:inceptionmodule"></span>
<img src="imagens/07-deepLearning/inceptionmodule.png" alt="Módulo Incepetion da rede GoogLenet - A parte intermediária da rede GoogLenet é formada por uma sequência de módulos Inceptions separados por camadas pooling. Cada módulo apresenta quatro caminhos para o mesmo dado de entrada, e na saída, os resultados são concatenados [37, p. 274]" width="100%" />
<p class="caption">
Figura 7.33: Módulo Incepetion da rede GoogLenet - A parte intermediária da rede GoogLenet é formada por uma sequência de módulos Inceptions separados por camadas pooling. Cada módulo apresenta quatro caminhos para o mesmo dado de entrada, e na saída, os resultados são concatenados <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 274]</span>
</p>
</div>
<p>A partir da entrada do módulo <em>inception</em>, as cópias do elemento de entrada seguem ao mesmo tempo por quatro caminhos. Ao final destes caminhos não se altera o tamanho da imagem, porém o número de canais é alterado de formas diferentes, sendo a escolha do número de canais de cada camada um hiperparâmetro. Na saída do módulo ocorre uma concatenação de todos estes canais, formando um único elemento de mesma dimensão que na entrada e com número de canais que é soma de todos que resultaram de cada caminho.</p>
<p>O primeiro caminho tem apenas uma convolução 1x1, conhecida como <em>bottleneck</em>, que tem como principal função preservar as dimensões (altura e largura) mas diminuir o número de canais, o que reduz o custo computacional e o número de parâmetros <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 220]</span>. Como esta convolução inclui mais não linearidade com baixo custo, inclui-se também nas camadas de entrada uma convolução 1x1, contribuindo para uma otimização na primeira parte da rede <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 453]</span>. Esta mesma camada foi acrescentada no início de cada um dos caminhos 2 e 3 para reduzir a complexidade do modelo. Após reduzir o número de camadas se inclui filtros maiores, possibilitando processar as informações em diferentes escalas, sendo que no segundo caminho os filtros são 3x3 e no quarto caminho 5x5 <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 274]</span>.</p>
<p>Todos os caminhos, até mesmo o quarto que inclui uma camada de MaxPool, apresentam apropriados <em>padding</em> para manter a mesma dimensão dos canais que na entrada. Como o MaxPool não altera o número de canais é incluído no final do quarto caminho uma convolução 1x1, reduzindo o volume.</p>
<p>Ao concatenar no final todos os canais de cada caminho, o módulo <em>inception</em> segue a hipótese de que a informação visual pode ser processada em várias escalas e que os resultados agregados permitem ao próximo nível extrair várias características de diferentes escalas ao mesmo tempo <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 222]</span>. Na rede GoogLenet da Figura <a href="deep-learning-em-visão-computacional.html#fig:googlenet">7.32</a> se vê três agrupamentos de módulos <em>inception</em> intercalados por Maxpooling 3x3, totalizando 9 módulos.</p>
<p>O diagrama anterior da GoogLenet é uma das representações mais simplificadas do modelo, pois como é visto na Figura <a href="deep-learning-em-visão-computacional.html#fig:googlenet2">7.34</a>, a arquitetura original inclui dois classificadores que correm paralelamente com os outros blocos descritos anteriormente, um que se inicia depois do terceiro módulo inception e o outro depois do sexto módulo <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 456]</span>. Cada classificador funciona parecido com a parte final da rede, em que ocorre a classificação <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. Os classificadores são formados por uma camada AvgPooling, seguida por uma de convolução, duas camadas totalmente conectadas e na saída uma função de ativação Softmax.</p>
<p>Existe uma peculiaridade ao treinar as redes mais profundas, pois na retropropagação dos erros, as taxas reduzem a valores muito próximo de zeros, dificultando a convergência do algoritmo. A técnica adotada pelo GoogLenet para garantir a convergência foi incluir o cálculo do gradiente dos erros destas classificações intermediárias na retropropagação do erro <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 456]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:googlenet2"></span>
<img src="imagens/07-deepLearning/googlenet.png" alt="Arquitetura da rede GoogLenet com classificadores intermediários - A rede GoogLenet com dois classificadores, um no terceiro módulo inception e o outro no sexto módulo. Estes classificadores intermediários reduzem o efeito de desaparecimento dos gradiente do erro [42]" width="50%" />
<p class="caption">
Figura 7.34: Arquitetura da rede GoogLenet com classificadores intermediários - A rede GoogLenet com dois classificadores, um no terceiro módulo <em>inception</em> e o outro no sexto módulo. Estes classificadores intermediários reduzem o efeito de desaparecimento dos gradiente do erro <span class="citation">[<a href="#ref-szegedy2015" role="doc-biblioref">42</a>]</span>
</p>
</div>
</div>
<div id="resnet" class="section level4">
<h4><span class="header-section-number">7.2.3.5</span> ResNet</h4>
<p>A Rede Neural Residual (ResNet) venceu a competição ImageNet em 2015 com uma taxa de erro de 3,6%. A rede desenvolvida por um grupo de pesquisa da Microsoft inclui várias técnicas de otimização e regularização dos modelos anteriores, principalmente da GoogLenet. Enquanto o ponto característico da rede GoogLenet são os módulos <em>inceptions</em>, na ResNet as unidades residuais permitiram treinar redes ainda mais profundas. Entre os principais modelos da ResNet são encontradas redes com 50, 101, 152 camadas de pesos <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 230]</span>, mais que o dobro do número da GoogLenet com 22 camadas.</p>
<p>Da mesma forma que a GoogLenet, a rede ResNet pode ser divida em três partes, sendo que a primeira e a última parte seguem a arquitetura da GoogLenet, e as camadas intermediárias incluem as unidades residuais. As camadas de entrada que permitem uma redução considerável da dimensão da imagem incluem uma camada de convolução com 64 canais e filtros 7x7 de <em>stride</em> 2, seguida por uma camada 3x3 de MaxPooling com <em>stride</em> 2. A última parte da rede, a estrutura de classificação, apresenta as mesmas camadas da GoogLenet, a camada Global de Pooling média de dimensão 1024 e apenas uma camada totalmente conectada com 1000 unidades representando as classes. Lembrando que a substituição do processo de <em>flatering</em> pela média global permitiu reduzir a proporção de parâmetros nas últimas camadas.</p>

<div class="figure" style="text-align: center"><span id="fig:resnet"></span>
<img src="imagens/07-deepLearning/resnet.png" alt="Arquitetura da rede ResNet - A estrutura geral da rede ResNet também pode ser dividida em três partes: Parte A - Semelhante a GoogLenet, contém uma camada convolucional e pooling para reduzir as dimensões da imagem; Parte B - Módulos com unidades residuais; Parte C - Camada de pooling global e uma Full Conect par classificação [33, p. 458]" width="100%" />
<p class="caption">
Figura 7.35: Arquitetura da rede ResNet - A estrutura geral da rede ResNet também pode ser dividida em três partes: Parte A - Semelhante a GoogLenet, contém uma camada convolucional e pooling para reduzir as dimensões da imagem; Parte B - Módulos com unidades residuais; Parte C - Camada de pooling global e uma Full Conect par classificação <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 458]</span>
</p>
</div>
<p>Como descrito no tópico (rede GoogLenet) ao acrescentar mais camadas, o número de parâmetros pode aumentar de forma que dificulte o treinamento, não só pelas limitações de recursos computacionais, mas também pela possibilidade de <em>overfitting</em> e demora de convergência do algoritmo. O modelo GoogLenet adotou várias abordagens de otimização e regularização, incluindo camadas de classificação intermediária para garantir que o modelo pudesse convergir, evitando o efeito de desaparecimento dos pesos (<em>vanishing gradients</em>), que em certos momentos tendiam a zero <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 456]</span>.</p>
<p>Uma das técnicas do ResNet que colaboraram para acelerar a convergência no treinamento foi a normalização em batch. Este método se tornou referência para vários modelos de CNN, pois até aquela época cada modelo adotava abordagens diferentes no treinamento, como as classes intermediárias do GoogLenet <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. A normalização é aplicada individualmente por camada, sendo que a normalização dos dados ocorre com base nas estatísticas dos agrupamentos <em>minibatch</em> adotados no treinamento, que foram vistos rapidamente no tópico MLP <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 280]</span>. Nas redes ResNet a normalização é aplicada nos dados no momento de transição entre a camada de convolução e a de ativação.</p>
<p>Mesmo conseguindo convergir redes mais profundas identificou-se nos modelos que ocorria uma degradação da acurácia ao se acrescentar mais camadas <span class="citation">[<a href="#ref-he2016" role="doc-biblioref">43</a>]</span>. Antigamente se tinha a intuição de que ao se partir de um modelo com número reduzido de camadas para uma rede mais profunda, o erro não deveria aumentar, pois no mínimo a rede mais profunda teria uma parte das suas camadas copiadas da outra rede e a outra parte funcionaria como funções de identidade. Com mais camadas, a otimização de um modelo se torna mais complexa mesmo para problemas facilmente mapeados em redes menores <span class="citation">[<a href="#ref-he2016" role="doc-biblioref">43</a>]</span>. A ideia na ResNet para lidar com esta complexidade foi forçar que as camadas realizassem o mapeamento da saída incluindo como referência a própria entrada, ou seja, um mapeamento residual <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 457]</span>.</p>
<p>No mapeamento comum das camadas na <a href="deep-learning-em-visão-computacional.html#fig:residual">7.36</a> se imagina que a partir de uma entrada x se busca com o treinamento estabelecer uma resposta esperada <span class="math inline">\(h(x)\)</span>. No caso do mapeamento residual se inclui um desvio do dado de entrada (<span class="math inline">\(x\)</span>) na saída para forçar um modelo <span class="math inline">\(h(x) - x\)</span>. Com isso se espera que seja mais fácil otimizar um modelo com referência, o mapeamento residual, e que este consiga rapidamente se ajustar a funções identidade, garantindo que as camadas no mínimo estabeleçam a performance das camadas anteriores <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 288]</span>. Os desvios, conexões residuais, também minimizam os efeitos de desaparecimento dos pesos, pois permitem um fluxo alternativo dos gradientes, contribuindo para a retropropagação do erro <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 231]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:residual"></span>
<img src="imagens/07-deepLearning/residual.png" alt="Mapeamento convencional e o residual da rede ResNet - a - Mapeamento tradicional da entrada e saída das camadas; b - Mapeamento em uma unidade residual da rede ResNet. No mapeamento comum, para uma entrada x se busca estabelecer uma resposta esperada \(h(x)\). No modelo residual se inclui um desvio da entrada (\(x\)) na saída para forçar uma resposta \(h(x) - x\) [33, p. 457]" width="100%" />
<p class="caption">
Figura 7.36: Mapeamento convencional e o residual da rede ResNet - a - Mapeamento tradicional da entrada e saída das camadas; b - Mapeamento em uma unidade residual da rede ResNet. No mapeamento comum, para uma entrada x se busca estabelecer uma resposta esperada <span class="math inline">\(h(x)\)</span>. No modelo residual se inclui um desvio da entrada (<span class="math inline">\(x\)</span>) na saída para forçar uma resposta <span class="math inline">\(h(x) - x\)</span> <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 457]</span>
</p>
</div>
<p>A conexão residual parte da entrada de uma unidade residual em direção a saída da mesma, somando a informação de entrada com a resposta antes da camada de ativação ReLu (Figura <a href="deep-learning-em-visão-computacional.html#fig:residualblock">7.37</a>). Cada unidade residual tem duas camadas de convolução 3x3, com <em>stride</em> 1 e configuração de <em>padding</em> para manter as dimensões. Entre as duas camadas existe uma camada de normalização antecedendo uma ativação ReLu.</p>

<div class="figure" style="text-align: center"><span id="fig:residualblock"></span>
<img src="imagens/07-deepLearning/residualblock.png" alt="Unidade residual da rede ResNet - Na parte intermediária da rede ResNet existe uma sequência de módulos com unidades residuais. Cada unidade residual tem duas camadas de convolução intercaladas por um pooling, e as normalizações são aplicadas na saída das convoluções [33, p. 457]" width="90%" />
<p class="caption">
Figura 7.37: Unidade residual da rede ResNet - Na parte intermediária da rede ResNet existe uma sequência de módulos com unidades residuais. Cada unidade residual tem duas camadas de convolução intercaladas por um pooling, e as normalizações são aplicadas na saída das convoluções <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 457]</span>
</p>
</div>
<p>Após uma sequência de unidades residuais de mesma configuração, entre os módulos, é dobrado o número de canais e ao mesmo tempo se reduz pela metade a largura e altura dos canais. Entre as unidades residuais não são utilizadas camadas <em>pooling</em>, assim, para reduzir as dimensões dos canais é utilizado <em>stride</em> 2 na convolução que encerra um módulo. Desta forma, na última unidade residual dos módulos, a entrada tem dimensão diferente da saída. Para poder somar os valores é necessário corrigir as dimensões da entrada com uma convolução 1x1 de <em>stride</em> 2 e de mesmo número de canais da saída da unidade <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 289]</span>.</p>
<p>A estrutura dos módulos com as unidades residuais lembram os blocos da rede VGG, em que cada bloco apresenta uma sequência de camadas de convolução com as mesmas dimensões, e sempre filtro 3x3. De um bloco para o outro no VGG, o número de canais também dobra e a altura e a largura são reduzidas pela metade, neste caso pelo efeito do MaxPooling. O ResNet apresenta 4 módulos, em que o número de unidades residuais varia de modelo a modelo como indicado na Figura - Arquitetura padrão da ResNet. O ResNet-34, por exemplo, com 34 camadas de pesos contém respectivamente nos quatro módulos, três unidades residuais com saída de 64 canais, quatro unidades com 128 canais, seis unidades com 256 canais e três unidades com 512 canais <span class="citation">[<a href="#ref-he2016" role="doc-biblioref">43</a>]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:resnetdefault"></span>
<img src="imagens/07-deepLearning/resnetdefault.png" alt="Arquitetura padrão da ResNet - Existem várias versões da ResNet, sendo que a parte inicial e a final das redes são semelhantes, e o que geralmente muda é quantidade de unidades residuais em cada um dos quatro módulos, na parte intermediária da rede [33, p. 457]" width="30%" />
<p class="caption">
Figura 7.38: Arquitetura padrão da ResNet - Existem várias versões da ResNet, sendo que a parte inicial e a final das redes são semelhantes, e o que geralmente muda é quantidade de unidades residuais em cada um dos quatro módulos, na parte intermediária da rede <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 457]</span>
</p>
</div>
</div>
</div>
<div id="aprendizado-por-transferência" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Aprendizado por transferência</h3>
<p>Treinar uma rede neural do zero é uma tarefa complicada, pois necessita de uma enorme quantidade de dados e poder computacional <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 242]</span>. Em muitos casos podemos não ter uma quantidade suficiente de algum dos dois, mas isso não impede que criemos nossos modelos. Para isso contamos com uma técnica conhecida como aprendizagem por transferência, que consiste basicamente em utilizar-se modelos já treinados na resolução de nossos problemas. Nesse caso assumimos que muitos dos fatores que explicam o contexto <span class="math inline">\(P_1\)</span>(situação na qual o modelo foi treinado) também podem explicar nossa novo contexto <span class="math inline">\(P_2\)</span> <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 536]</span>.</p>
<p>Conseguimos utilizar essa técnica pois mesmo modelos treinados em escopos diferentes acabam aprendendo a detectar características parecidas nas camadas mais iniciais, se especificando mais nas suas camadas mais profundas. Podemos ver isso na imagem <a href="deep-learning-em-visão-computacional.html#fig:differentscnn">7.39</a>, onde temos a representação de quatro redes diferentes sendo a primeira voltada a trabalhar com imagem de pessoas, a segunda com carros, a terceira com elefantes e a última com cadeiras. Mesmo esses sendo objetos completamente diferentes, as redes aprendem, nas camadas iniciais a detectar bordas e quinas, que são características compartilhadas por todos os itens.</p>

<div class="figure" style="text-align: center"><span id="fig:differentscnn"></span>
<img src="imagens/07-deepLearning/differentscnn.png" alt="Diferentes CNN’s e seus mapas de características em diferentes camadas - Na primeira coluna temos as características aprendidas por uma rede especializada no trabalho com rostos humanos, na segunda coluna com carros, na terceira com elefantes e na quarta com cadeiras [36, p. 253]" width="100%" />
<p class="caption">
Figura 7.39: Diferentes CNN’s e seus mapas de características em diferentes camadas - Na primeira coluna temos as características aprendidas por uma rede especializada no trabalho com rostos humanos, na segunda coluna com carros, na terceira com elefantes e na quarta com cadeiras <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 253]</span>
</p>
</div>
<p>Existem três principais maneiras de se realizar a transferência de aprendizado <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 254]</span>, sendo que cada uma delas se enquadra melhor em um cenário ou em outro, são elas:</p>
<ul>
<li><p><strong>Usar uma rede pré treinada como classificador:</strong> Esse método se enquadra bem se nosso problema tem um domínio muito parecido com a rede já treinada que iremos utilizar, sendo que temos apenas que encaixá-la em nosso uso, não sendo necessário treinamento adicional.</p></li>
<li><p><strong>Usar uma rede pré treinada como um extrator de características:</strong> Esse método é útil quando estamos resolvendo um problema que tem várias características comuns com uma rede pré treinada mas não de maneira que possamos utilizar como na aproximação anterior. O que fazemos então é “congelar” as camadas da rede que extraem características(ou seja, as camadas de convolução) e substituir a rede totalmente conectada do final, colocando em seu lugar outra rede que classifique conforme nossa necessidade e realizamos seu treinamento.</p></li>
</ul>

<div class="figure" style="text-align: center"><span id="fig:vgg16"></span>
<img src="imagens/07-deepLearning/vgg16.png" alt="Rede VGG16 - Temos, de cima para baixo, as camadas congeladas, as camadas que serão removidas e a camada que será adicionada e treinada [36, p. 257]" width="50%" />
<p class="caption">
Figura 7.40: Rede VGG16 - Temos, de cima para baixo, as camadas congeladas, as camadas que serão removidas e a camada que será adicionada e treinada <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 257]</span>
</p>
</div>
<p>Na figura <a href="deep-learning-em-visão-computacional.html#fig:vgg16">7.40</a> temos um exemplo do livro de Mohamed Elgendy <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>]</span>, onde se queria realizar a classificação com cachorros e gatos, então essa rede foi escolhida pois foi treinada na base de dados ImageNet, que contém muitos exemplos de cachorros e gatos, com isso ela poderia se adaptar bem ao nosso problema.</p>
<ul>
<li><strong>Usar uma rede pré treinada para ajuste fino:</strong> Os métodos anteriores são usados quando temos um domínio parecido, já este terceiro método se enquadra em problemas que têm propriedades bem diferentes. Mesmo nestes casos conseguimos utilizar redes pré-treinadas, pelos motivos já ditos anteriormente, que as redes, principalmente em suas camadas iniciais, aprendem a detectar características parecidas, ficando mais específicas conforme sua profundidade <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 259]</span>.
Neste método temos algumas possibilidades, uma delas é “congelar” as primeiras camadas convolucionais e retreinar o resto da rede, o que estaremos fazendo então é um ajuste fino(também conhecido como fine-tune) nas camadas mais profundas da rede, para que consigam se adaptar ao nosso problema atual.
Também temos a possibilidade de não “congelar” nenhuma camada e retreinar a rede toda, neste caso, mesmo tendo que gastar um tempo maior e ser necessário uma maior quantidade de dados, a rede tende a convergir mais rapidamente a uma solução ótima, em comparação a uma iniciação randômica <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 259]</span>.</li>
</ul>
<p>No livro “Deep Learning for Vision Systems” de Mohamed Elgendy são apresentadas algumas dicas de como escolher a melhor tecnica de transferencia de aprendizado para alguns tipos de cenários, sendo elas apresentadas na tabela #tab:tabelaaprendizado:</p>
<p>Tabela (#tab:tabelaaprendizado) Tabela com os diferentes cenários onde podemos utilizar a transferência de aprendizado <span class="citation">[<a href="#ref-elgendy2020" role="doc-biblioref">36</a>, p. 262]</span></p>
<table style="width:100%;">
<colgroup>
<col width="25%" />
<col width="23%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Cenário</th>
<th align="center">Quantidade de dados disponíveis</th>
<th align="center">Similaridade entre a rede pré treinada e a nova base de dados</th>
<th align="center">Método</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">Pequena</td>
<td align="center">Similar</td>
<td align="center">Usar uma rede pré treinada como classificador</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">Grande</td>
<td align="center">Similar</td>
<td align="center">Ajuste fino na rede totalmente conectada</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">Pequena</td>
<td align="center">Muito diferente</td>
<td align="center">Ajuste fino em uma parte da rede</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">Grande</td>
<td align="center">Muito diferente</td>
<td align="center">Ajuste fino em toda a rede</td>
</tr>
</tbody>
</table>
</div>
<div id="redes-neurais-na-prática" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Redes neurais na prática</h3>
<p>Ao estudar os diferentes modelos de CNN’s percebemos que o principal fator para melhorar a acurácia das redes foi o aumento no número de camadas, tornando as redes mais profundas. Destaca-se que a construção de modelos mais profundos só foi possível pela evolução da performance de hardware, principalmente memória e unidades de processamento, e desenvolvimento de softwares mais específicos, conhecidos como frameworks para deep learning <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 36]</span>.</p>
<p>O treinamento das redes envolve milhares de operações com elementos multidimensionais, ou seja, <em>arrays</em> n-dimensionais com número arbitrário de eixos, conhecidos como tensores <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 42]</span>. Tensores com apenas uma dimensão correspondem matematicamente aos vetores, enquanto os de duas dimensões são as matrizes. No caso das redes CNN’s, muitas vezes as entradas são imagens coloridas que podem ser interpretadas como tensores de três dimensões, a altura, a largura e o volume da imagem (canais RGB). Dentro da rede ocorre uma série de multiplicações e somas a partir do tensor de entrada, resultando em diferentes tensores tridimensionais.</p>
<p>A realização de todas as operações só é possível devido às unidades de processamento das máquinas, que permitem a interação da unidade lógica aritmética (ALU) com a memória. Quando as primeiras redes foram desenvolvidas, o principal recurso de processamento eram as CPU’s, um processador de propósito geral em que a ALU realiza apenas um cálculo por vezes. Como possui aplicações mais gerais precisa de acesso constante à memória para leitura de instruções e armazenamento de dados, o que é uma desvantagem em relação ao tempo de processamento conhecida como “gargalo de von Neumann” <span class="citation">[<a href="#ref-googlecloud2021" role="doc-biblioref">44</a>]</span>. Geralmente para tornar mais rápido o acesso a memória são integradas tecnologias mais sofisticadas aos caches, porém de tamanho limitado devido ao custo <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 260]</span>.</p>
<p>As limitações de armazenamento e processamento relacionados a uma única CPU inviabilizam o treinamento de redes mais profundas. O que permitiu a evolução das redes foi a adaptação de unidades de processamento com aplicações mais específicas. A maioria das redes modernas são implementadas com base em unidades de processamento gráfico (GPU’s). Originalmente estes componentes foram desenvolvidos para aplicações gráficas, principalmente para renderização de videogames <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 439]</span>. Entretanto, grande parte da abordagem de processamento da GPU também se mostrou compatível com os cálculos necessários para treinar as redes neurais.</p>
<p>A GPU consegue maior capacidade de processamento que uma CPU, pois integra várias ALUs em um único processador, o que permite a realização de milhares de operações simultaneamente <span class="citation">[<a href="#ref-googlecloud2021" role="doc-biblioref">44</a>]</span>. Esta estratégia é ideal para aplicações que se adequam ao processamento paralelo, como multiplicação de matrizes de uma rede neural, que envolvem operações independentes <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 440]</span>. As GPU’s foram construídas para realizar operações mais simples, sem envolver muitas ramificações como geralmente é necessário no fluxo de trabalho da CPU. A maior parte dos cálculos do treinamento das redes são previsíveis, com algoritmos que não necessitam de controles sofisticados <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 440]</span>.</p>
<p>Se por um lado as operações das redes não exigem grande complexidade computacional, por outro, demandam grande quantidade de memória. Durante o treinamento é preciso armazenar os parâmetros, os valores de ativação, e os gradientes, ou seja, uma quantidade de dados além do limite da cache associada à CPU <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 440]</span>. Neste sentido, além de permitir processamento paralelo, diminuindo o tempo de treinamento, a GPU também oferece maior quantidade de memória.</p>
<p>A utilização da GPU para o treinamento das redes se tornou ainda mais comum após a adaptação para propósitos mais gerais. Um dos principais modelos de GPU, da NVIDIA, suporta uma programação CUDA para o desenvolvimento de códigos arbitrários semelhantes à linguagem C <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 440]</span>. Assim, a GPU pode executar diferentes rotinas que não estejam só associadas a sub rotinas de renderização.</p>
<p>Para garantir códigos de GPU de alta performance foram construídos ao longo dos anos diferentes bibliotecas que lidam com computação numérica, principalmente envolvendo convoluções e outras operações com tensores <span class="citation">[<a href="#ref-goodfellow2016" role="doc-biblioref">22</a>, p. 441]</span>. Assim, no desenvolvimento das redes não é necessário se preocupar com a programação no nível do CUDA, que é mais complexa e envolve computação paralela e distribuída. Geralmente estas bibliotecas, ou frameworks, têm suporte tanto em GPU quanto CPU. As primeiras gerações de muitos frameworks foram desenvolvidas por meio de parcerias entre Universidades e grandes empresas com interesse em Deep Learning. Algumas das bibliotecas mais comuns destacadas na Figura <a href="deep-learning-em-visão-computacional.html#fig:frameworks">7.41</a>, são o PyTorch e Caffe2 mantidos pelo Facebook, o TensorFlow pela Google, o MXNet pela Amazon e o CNTK pela Microsoft <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:frameworks"></span>
<img src="imagens/07-deepLearning/frameworks.png" alt="Principais frameworks para deep learning - Para facilitar a manipulação de tensores e a utilização de GPU’s foram desenvolvidos diferentes frameworks. A maior parte das primeiras gerações dos frameworks surgiram como parcerias entre universidades e grandes empresa [38]" width="100%" />
<p class="caption">
Figura 7.41: Principais frameworks para deep learning - Para facilitar a manipulação de tensores e a utilização de GPU’s foram desenvolvidos diferentes frameworks. A maior parte das primeiras gerações dos frameworks surgiram como parcerias entre universidades e grandes empresa <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>
</p>
</div>
<p>Mesmo com o aumento do desempenho da GPU em relação a CPU para treinar as redes, em muitos casos uma única máquina não é suficiente para todo o processamento. Com a possibilidade do paralelismo se tornou comum distribuir a carga de trabalho entre várias máquinas. No entanto, montar uma rede própria de GPU é um investimento alto, e a tecnologia também iria depreciar em poucos anos, pois os modelos de GPU estão evoluindo rapidamente para acompanhar as redes cada vez maiores <span class="citation">[<a href="#ref-wangenheim2018" role="doc-biblioref">45</a>]</span>. Uma alternativa para as redes locais é utilizar servidores de GPU em nuvem oferecidos por provedores como Amazon, Google, Microsoft entre outros.</p>
<p>Nos últimos anos aumentou a oferta de serviços na nuvem para treinar CNN’s, sendo que os custos são com base no tipo de tecnologia da GPU disponível, no tempo de processamento e na quantidade de armazenamento <span class="citation">[<a href="#ref-wangenheim2018" role="doc-biblioref">45</a>]</span>. Alguns provedores oferecem alguns serviços gratuitos. As redes podem ser desenvolvidas diretamente em plataformas oferecidas pelos próprios provedores, que geralmente apresentam uma IDE baseada em Jupyter Notebooks. Cada plataforma apresenta facilidades para determinados frameworks, geralmente para aquelas bibliotecas que também prestam suporte <span class="citation">[<a href="#ref-wangenheim2018" role="doc-biblioref">45</a>]</span>. O GoogleCloud, por exemplo, apresenta várias ferramentas para integrar com o TensorFlow.</p>
<p>O GoogleCloud oferece além dos serviços de GPU a alternativa de acesso às TPU’s como recursos de computação em nuvem. A TPU (unidades de processamento de tensor) foi projetada especificamente para trabalhar com redes neurais e funciona como processador de matriz especializado. Como a sequência de cálculos já é previamente conhecida, foram conectados vários multiplicadores e somadores, formando uma matriz de operadores, denominada matriz sistólica <span class="citation">[<a href="#ref-googlecloud2021" role="doc-biblioref">44</a>]</span>. Uma vantagem da TPU em relação a CPU e a GPU é a redução do gargalo de von Neumann, pois os dados são todos carregados de uma única vez da memória, não precisando o acesso durante os cálculos <span class="citation">[<a href="#ref-googlecloud2021" role="doc-biblioref">44</a>]</span>.</p>
<p>No tópico sobre backpropagation foi apresentado um exemplo de aplicação de rede MLP para reconhecimento de dígitos em que implementamos na linguagem Python. No código foi desenvolvido manualmente a estrutura da rede, as etapas do treinamento e as funções, utilizando principalmente bibliotecas que lidam com cálculos em Arrays Multidimensionais, como o Numpy. No exemplo MLP, ao manipular os dados como array do numpy não surgiram inconsistências e também não nos deparamos com operações muito complexas, pois até aquele momento as aplicações não envolviam redes muito densas, que exigissem várias otimizações e regularizações. Em modelos de redes maiores, a utilização do algoritmo backpropagation é um desafio maior e muitas vezes pode ser difícil garantir a convergência apenas utilizando as bibliotecas tradicionais de computação numérica (adrian2017, pg. 153).</p>
<p>Por outro lado, os frameworks que acabamos de apresentar, como o TensorFlow e o Pythorch, foram construídos especificamente para lidar com deep learning, podendo, por exemplo, disponibilizar automaticamente várias funções de otimização e regularização. Os frameworks de deep learning geralmente apresentam uma estrutura de dados multidimensionais, tratados como uma classe tensor, que substitui o “ndarray” do numpy <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 44]</span>. Esta estrutura de dados de tensor apresenta algumas vantagens em relação ao tradicional array do numpy que se adequam melhor a manipulação de dados nas redes. O primeiro ponto é que o numpy apresenta por default uma precisão de 64 bit, enquanto os tensores geralmente usam 32 bit. Como a precisão de 32 bit é suficiente para trabalhar com as redes, ao se reduzir o tamanho dos dados se ocupa menos memória e torna as operações mais rápidas <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 474]</span>.</p>
<p>Por default a manipulação das variáveis são direcionadas para os cálculos na CPU, entretanto, quando existe disponível diferentes dispositivos de CPU e GPU pode ser interessante distribuir os cálculos e armazenamento <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 219]</span>. Diferente do Numpy, os frameworks de deep learning possuem suporte para lidar com vários dispositivos ao mesmo tempo <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 44]</span>. Anteriormente destacamos que o propósito da utilização das GPU’s nas redes é acelerar o treinamento, principalmente adotando processamento distribuído e paralelo, porém se a manipulação dos dados não for adequada, precisando ocorrer muitas transferências entres dispositivos, o tempo de processamento pode aumentar <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 222]</span>. Geralmente, ao se utilizar os frameworks configurados com as GPU’s e CPU’s, não é necessário se preocupar na forma como ocorrem estas transferências, pois os processamentos são otimizados com base em gráficos computacionais <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 368]</span>.</p>
<p>O que tem popularizado ainda mais as CNN’s mesmo fora dos meios acadêmicos é o desenvolvimento de interfaces de programação que tornam mais simples a construção e o treinamento das redes neurais. Keras, um dos mais conhecidos API para deep learning em alto nível, foi desenvolvido por François Chollet como um projeto de pesquisa, que foi disponibilizado como software livre em 2015 <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 292]</span>. Foi escrito em Python e funciona como um mecanismo de computação subjacente que roda sob diferentes frameworks de backend, alguns identificados na Figura - Implementações do API Keras, os quais oferecem o suporte de computação numérica, como TensorFlow, Theano, CNTK e MxNet <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 292]</span>.</p>
<p>Grande parte do trabalho com as redes neurais pode ser implementado direto com o API de alto nível, incluindo o desenho da arquitetura, as funções, algoritmo de treinamento, otimizadores e módulos de regularização. Para implementações que necessitam de maiores customizações é necessário recorrer aos frameworks de mais baixo nível, como o TensorFlow. O TensorFlow, criado por um grupo de pesquisa da Google e liberado como software livre em 2015, tem a sua própria implementação de keras, o tf.keras <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 368]</span>.</p>
<p>O API tf.keras tem como backend apenas o TensorFlow (Figura <a href="deep-learning-em-visão-computacional.html#fig:keras">7.42</a>), porém apresenta maiores facilidades para integrar com customizações do backend. A inclusão de novos códigos pode ser feita a partir da linguagem python e automaticamente o API converte para funções do tipo TensorFlow que possuem base no C++ <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 398]</span>. Muitas vezes essas adaptações ocorrem quando se precisa de um controle maior, ou customizar funções, camadas, módulos de regularização, processos de inicialização, ou medidas de avaliação <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 398]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:keras"></span>
<img src="imagens/07-deepLearning/keras.png" alt="Implementações do API Keras - a - Implementação geral do API keras-team que é compatível com diferentes backends, como TensorFlow, Theano, CNTK e MxNet. b - O API tf.keras tem como backend apenas o TensorFlow e comporta aplicações específicas deste framework [33, p. 292]" width="90%" />
<p class="caption">
Figura 7.42: Implementações do API Keras - a - Implementação geral do API keras-team que é compatível com diferentes backends, como TensorFlow, Theano, CNTK e MxNet. b - O API tf.keras tem como backend apenas o TensorFlow e comporta aplicações específicas deste framework <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 292]</span>
</p>
</div>
<p>No momento a última versão estável do TensorFlow é a 2.4, sendo que a primeira versão do TensorFlow 2.0 foi lançada em março de 2019 <span class="citation">[<a href="#ref-tensorflow2020" role="doc-biblioref">46</a>]</span>. Enquanto que nas versões 1.0, o módulo simbólico é a parte central <span class="citation">[<a href="#ref-geron2019" role="doc-biblioref">33</a>, p. 396]</span>, principalmente pela preferência da abordagem por gráficos estáticos, a partir das versões 2.0 se começou a ter uma programação mais imperativa de forma semelhante ao PytTorch, e que apresenta como default gráficos dinâmicos <span class="citation">[<a href="#ref-zhang2020dive" role="doc-biblioref">37</a>, p. 516]</span>. O módulo estático busca maior otimização do código, sendo que após a definição de todo o processo e construção do gráfico não se tem mais a dependência do código <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>. No formato dinâmico a construção e a execução dos gráficos não podem ser desvinculadas, pois ocorrem simultaneamente. Geralmente, os gráficos dinâmicos são mais fáceis de debugar e apresentam menores inconsistências <span class="citation">[<a href="#ref-johnson2019" role="doc-biblioref">38</a>]</span>.</p>
</div>
</div>
<div id="redes-neurais-siamesas" class="section level2">
<h2><span class="header-section-number">7.3</span> Redes Neurais Siamesas</h2>
<p>O primeiro trabalho com redes neurais siamesas em 1994 tinha a proposta de comparar duas assinaturas manuscritas, verificando se as duas eram originais ou se uma delas era falsa <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 75]</span>. Grande parte das redes neurais para classificação com imagens utiliza a estratégia de apresentar o objeto para identificar o que é, enquadrando nas classes definidas no treinamento. As redes siamesas apresentam o problema de uma forma diferente, em vez de descobrir o que foi apresentado, a ideia é comparar duas entradas, verificando se fazem parte da mesma classe ou não.</p>
<p>Após a utilização das redes siamesas para reconhecimento de assinaturas surgiram diferentes aplicações para análise de imagens com objetivo de avaliar a similaridade entre elas. Na área da saúde, as redes siamesas já foram testadas por diferentes trabalhos para detecção de doenças a partir da análise de imagens de exames <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 81]</span>. Já foram apresentados trabalhos na área da farmácia para comparação de composições químicas, e na biologia, para avaliar sequências de DNA e classificar cromossomos <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 78]</span>. Existem adaptações destas redes em diferentes setores, como na robótica, no desenvolvimento de software e no sensoriamento remoto <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 82]</span>.</p>
<p>Uma das aplicações que mais teve repercussão foi na verificação de faces, como no trabalho de Chopra et al. (2005) <span class="citation">[<a href="#ref-chopra2005" role="doc-biblioref">48</a>]</span>. Taigman et al. (2014) apresentaram uma rede para verificação de faces a partir de fotos, como um projeto do Facebook <span class="citation">[<a href="#ref-taigman2014" role="doc-biblioref">49</a>]</span>. Sabri e Kurita (2018) trabalharam com reconhecimento de faces e expressões faciais <span class="citation">[<a href="#ref-sabri2018" role="doc-biblioref">50</a>]</span>. As redes neurais siamesas também ganharam popularidade no reconhecimento de imagem <em>one-shot</em>, em particular pelo estudo de Koch et al. (2021) <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 81]</span>.</p>
<p>Diferente dos modelos tradicionais de redes para classificação que precisam de uma grande quantidade de entradas de treinamento para cada classe, nas redes siamesas <em>one-shot</em>, geralmente é necessário apenas um exemplo para cada classe <span class="citation">[<a href="#ref-lambda2019" role="doc-biblioref">51</a>]</span>. O propósito da rede não é reconhecer a entrada a partir de diferentes classes, o que exige muitos dados de treinamento, e sim, comparar a similaridade entre duas imagens. Por exemplo, em um reconhecimento facial dos funcionários de uma empresa, em vez de treinar uma rede com várias fotos de cada pessoa para que aprenda a classificar um por um, pode ser interessante utilizar uma siamesa <em>one-shot</em> que compara uma entrada com as referências verificando se corresponde a algum funcionário.</p>
<p>Considerando que os vídeos podem ser tratados como uma sequência de imagens, muitas das técnicas de processamento de imagens com redes siamesas foram bem adaptadas para aplicações com vídeos. Além de aspectos estáticos presentes nas imagens, os vídeos podem oferecer informações de movimento e do estado de objetos em diferentes contextos e momentos <span class="citation">[<a href="#ref-walker2021" role="doc-biblioref">47</a>, p. 83]</span>. Por exemplo, com vídeos é possível não só reconhecer um objeto em uma imagem isolada, como também rastreá-lo ao longo dos quadros de gravações <span class="citation">[<a href="#ref-zhang2018" role="doc-biblioref">52</a>]</span>. Estas redes conseguiram reconhecer pessoas até mesmo com base no estilo de caminhada, como mostrou um estudo com imagens de vídeos de segurança <span class="citation">[<a href="#ref-zhang2016" role="doc-biblioref">53</a>]</span>.</p>
<p>O uso das redes com dados de vídeos tem mostrado várias oportunidades na área de segurança, além do monitoramento de pessoas também existem linhas de pesquisa para o setor de veículos e trânsito. O sistema PROVID produzido por Liu et al. (2017) é um exemplo do uso das redes siamesas para identificação de veículos com base nas placas <span class="citation">[<a href="#ref-liu2017" role="doc-biblioref">54</a>]</span>. Os estudos de redes siamesas para detecção dos limites das estradas e de contornos em vídeos na perspetiva da direção de veículos pode ajudar o desenvolvimento de automóveis autônomos <span class="citation">[<a href="#ref-wang2017" role="doc-biblioref">55</a>]</span>.</p>
<p>As possibilidades das redes siamesas se estendem além do processamento de imagens e da visão computacional, já foram apresentados estudos para análise de áudios e no processamento de linguagem natural. Um exemplo de aplicação com áudio foi a rede siamesa de Zhang et al. (2018), desenvolvida para avaliar a similaridade entre sons originais e imitações de vozes para poder diferenciá-las <span class="citation">[<a href="#ref-zhang2018" role="doc-biblioref">52</a>]</span>. A área de processamento de linguagem natural se propõe gerar e compreender automaticamente a linguagem humana a partir de dados de comunicação, como em documentos com textos e mensagens. Por exemplo, na proposta de González et al. (2019), as redes siamesas extraem as informações mais importantes de um texto para poder resumi-lo <span class="citation">[<a href="#ref-gonzalez2019" role="doc-biblioref">56</a>]</span>. No trabalho de Das et al. (2016) foram avaliados questões de diferentes <em>websites</em> de perguntas e respostas, como <em>Yahoo Answers</em> e <em>Stack Overflow</em>, para poder detectar as semelhanças entre as mensagens <span class="citation">[<a href="#ref-das2016" role="doc-biblioref">57</a>]</span>.</p>
<div id="arquitetura" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Arquitetura</h3>
<p>Como dito no tópico anterior, o objetivo de se utilizar uma rede com arquitetura siamesa era comparar diferentes entradas e ter como resultado um valor que representasse sua semelhança ou diferença. O nome “siamesa” por si só já nos dá uma boa ideia de como será sua arquitetura, que consiste basicamente na utilização de duas redes idênticas, como na Figura <a href="deep-learning-em-visão-computacional.html#fig:siamesa-bromley">7.43</a>, onde temos a rede proposta por Bromley et al. (1993) em seu artigo sobre verificação de assinaturas <span class="citation">[<a href="#ref-bromley1993" role="doc-biblioref">58</a>]</span>. Nessa arquitetura temos uma camada de entrada de 200 x 8 unidades e como saída um bloco de dados de 16 x 19 unidades, o método utilizado para realizar a comparação entre os vetores de saída das duas redes foi calcular o cosseno do ângulo entre eles.</p>

<div class="figure" style="text-align: center"><span id="fig:siamesa-bromley"></span>
<img src="imagens/07-deepLearning/siamesa-bromley.png" alt="Arquitetura utilizada por Bromley et al. (1993) com duas redes recebendo os dados sobre assinaturas após seu pré processamento. A rede contém uma camada de entrada com 200x8 unidades e uma saída de 16x19 unidades. [58]" width="40%" />
<p class="caption">
Figura 7.43: Arquitetura utilizada por Bromley et al. (1993) com duas redes recebendo os dados sobre assinaturas após seu pré processamento. A rede contém uma camada de entrada com 200x8 unidades e uma saída de 16x19 unidades. <span class="citation">[<a href="#ref-bromley1993" role="doc-biblioref">58</a>]</span>
</p>
</div>
<p>Na Figura <a href="deep-learning-em-visão-computacional.html#fig:siamesa-chopra">7.44</a> temos um exemplo de outra arquitetura, nesse caso a apresentada por Chopra et al. (2005). Um aspecto importante que devemos notar, é que tanto essa rede quanto a anterior tem os mesmo pesos em ambas as suas redes internas. Nesse caso se optou por utilizar a distância euclidiana entre os dois vetores de características resultantes das duas subredes.</p>

<div class="figure" style="text-align: center"><span id="fig:siamesa-chopra"></span>
<img src="imagens/07-deepLearning/siamesa-chopra.png" alt="Arquitetura utilizada por Chopra et al. [48]. Essa arquitetura de rede siamesa tem os parâmetros da rede compartilhados entre as duas. \(X_1\) e \(X_2\) são as duas entradas da rede e \(E_w\) é a saída da diferença euclidiana entre as saídas das duas redes(\(||G_w(X_1)-G_w(X_2)||\))." width="50%" />
<p class="caption">
Figura 7.44: Arquitetura utilizada por Chopra et al. <span class="citation">[<a href="#ref-chopra2005" role="doc-biblioref">48</a>]</span>. Essa arquitetura de rede siamesa tem os parâmetros da rede compartilhados entre as duas. <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> são as duas entradas da rede e <span class="math inline">\(E_w\)</span> é a saída da diferença euclidiana entre as saídas das duas redes(<span class="math inline">\(||G_w(X_1)-G_w(X_2)||\)</span>).
</p>
</div>
<p>Das redes neurais que apresentamos para aplicações no processamento de imagens e visão computacional, como MLP, CNN’s e siamesas, as abordagens de desenvolvimento, treinamento e operação seguem um padrão semelhante ao se pôr em prática. A utilização de <em>frameworks</em> tem facilitado as aplicações em redes neurais, disponibilizando comandos para criar camadas, montagem da arquitetura, funções de regularização e otimização, medidas de avaliação, algoritmos de treinamento, sendo que a maior parte dos recursos é facilmente adaptável para diferentes modelos de redes. Geralmente, a maior parte do trabalho de quem desenvolve essas redes está na obtenção e preparo dos dados de entrada.</p>
<p>Dependendo do tipo da rede, a imagem de entrada deve ser disponibilizada como um vetor, como na MLP, ou como um tensor, como na CNN’s, e no caso das siamesas pode ser que a entrada seja duas ou mais imagens ao mesmo tempo. Na maioria das vezes será necessário, dimensionar as imagens, estabelecer as classes, alterar o formato dos dados, e dividir as amostras de treinamento, teste e validação. Cada aplicação pode exigir uma abordagem diferente para que a rede aprenda o problema, o que muitas vezes exige que os dados sejam pré configurados e organizados de forma que se adequem ao tipo de algoritmo adotado no treinamento.</p>
<p>Nas redes estudadas utiliza-se o método supervisionado, especificamente o “<em>backpropagation</em>” , ou seja, durante o treinamento devem ser apresentados tanto as entradas do problema como as respostas correspondentes que são previamente conhecidas. Desta forma, o modelo consegue corrigir os parâmetros da rede considerando as diferenças entre as saídas e as respostas esperadas. No algoritmo de <em>backpropagation</em>, esta diferença é tratada por uma função de erro que o treinamento busca minimizar. No tópico <a href="deep-learning-em-visão-computacional.html#backpropagation">backpropagation</a> foi descrita uma das funções de erro que foram utilizadas pelos primeiros modelos de redes neurais, o erro quadrático (MSE). Com o aprimoramento das redes, a substituição por outras funções erros demonstrou melhores resultados no treinamento, como a entropia cruzada ou <em>Cross-Entropy</em> <span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>.</p>
<p>Dado a expressão da função custo entropia cruzada e de sua derivada:</p>
<p><span class="math display" id="eq:entropia-cruzada">\[
C = -\frac{1}{n} \sum_{x}[y \ln a + (1 - y) \ln(1 - a)] 
\tag{7.1}
\]</span></p>
<p><span class="math display" id="eq:derivada-entropia">\[
\frac{\partial C}{\partial w_j} = \frac{1}{n} \sum_{x} x_j (\sigma(z) - y)
\tag{7.2}
\]</span></p>
<p>se consegue avaliar que a derivada aumenta com o tamanho do erro (<span class="math inline">\(\sigma(z) - y\)</span>). Considerando este comportamento e que o algoritmo de treinamento fica mais rápido quanto maior for a derivada da função custo, o método da entropia cruzada tende a demonstrar menores problemas com a desaceleração do aprendizado se comparado ao erro quadrático <span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>. No erro quadrático, como a derivada da função custo depende diretamente da derivada da função de ativação, o treinamento poderá ficar estagnado nos pontos em que a ativação for próximo dos limites, particularmente se as funções de ativações forem sigmóides <span class="citation">[<a href="#ref-nielsen2015" role="doc-biblioref">31</a>]</span>.</p>
<p>Geralmente, as funções erro quadrático e entropia cruzada são aplicadas em modelos que se pretende que a saída corresponda a um vetor de probabilidades em relação às classes. Desta forma, é possível associar cada valor a uma classe, ou seja, dado a entrada se determina qual a saída mais provável dentro dos exemplos estabelecidos no treinamento. Outras funções de custo mais comuns nas redes siamesas são as por técnica de ranqueamento, como a perda contrastiva e as funções <em>triplets</em>. No ranqueamento, a saída da rede é estabelecida pelas distâncias entre os dados de entrada de acordo com uma distribuição dos exemplos apresentados no treinamento <span class="citation">[<a href="#ref-gomez2019" role="doc-biblioref">59</a>]</span>. Neste caso, as entradas que pertencem a uma mesma classe tendem a ficar mais próximas, ganhando uma posição de maior prioridade que elementos com menor semelhança, os quais vão se afastando.</p>
<p>O primeiro modelo de rede siamesa, que comparava as assinaturas <span class="citation">[<a href="#ref-bromley1993" role="doc-biblioref">58</a>]</span>, utilizou a função erro contrastiva <span class="citation">[<a href="#ref-hadsell2006" role="doc-biblioref">60</a>]</span>:</p>
<p><span class="math display" id="eq:erro-contrastivo">\[
L(W, Y, X_1, X_2) = (1 - Y)\frac{1}{2}(E_w)^2 + (Y) \frac{1}{2} {max(0,m - E_w)}^2
\tag{7.3}
\]</span></p>
<p>em que são apresentadas pelo menos duas entradas na rede com propósito de comparar (ou contrastar) e determinar a similaridade. Quando o par de entrada é semelhante (<span class="math inline">\(X_1, X_2\)</span>) o parâmetro <span class="math inline">\(Y\)</span> recebe valor 0, e quando são de classes diferentes (<span class="math inline">\(X_1, X’_2\)</span>), <span class="math inline">\(Y = 1\)</span>. Os pesos da rede (<span class="math inline">\(W\)</span>) são corrigidos de forma que a distância (<span class="math inline">\(E_w\)</span>) entre dados semelhantes fique menor e os mais diferentes se afastem <span class="citation">[<a href="#ref-hadsell2006" role="doc-biblioref">60</a>]</span>. Para isto é estabelecido uma margem (<span class="math inline">\(m\)</span>) de ajuste, a qual garante uma distância mínima separando o que pertence ou não a um mesmo grupo, <span class="math inline">\(E_W (X_1, X_2) + m &lt; E_W (X_1, X’_ 2 )\)</span>.</p>
<p>A função de erro <em>Triplet</em> tem demonstrado bons desempenhos em alguns estudos com redes siamesas. Diferente da função contrastiva, na rede <em>Triplet</em> se utiliza três dados como entrada no treinamento, ou seja, uma siamesa com três sub-redes idênticas em vez de duas. Neste método, compara-se ao mesmo tempo o valor âncora (<span class="math inline">\(x^a\)</span>), ou valor de referência, com um padrão semelhante (caso positivo <span class="math inline">\(x^p\)</span>), e com um padrão diferente (caso negativo <span class="math inline">\(x^n\)</span>) de acordo com a função erro <span class="citation">[<a href="#ref-schroff2015" role="doc-biblioref">61</a>]</span>:</p>
<p><span class="math display" id="eq:erro-triplet">\[
C = \sum_{i}\left[||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \alpha\right]
\tag{7.4}
\]</span>
Considerando que as funções <span class="math inline">\(f(x) \in R\)</span> podem ser interpretadas como a aplicação de cada sub-rede, o algoritmo de treinamento deve buscar ordenar as entradas com base na similaridade relativa. Assim, a distância entre a âncora e o padrão positivo deve ser menor do que em relação ao caso negativo, como demonstrado na Figura <a href="deep-learning-em-visão-computacional.html#fig:erro-triplet">7.45</a>. <span class="citation">[<a href="#ref-schroff2015" role="doc-biblioref">61</a>]</span>. Da mesma forma que no método contrastivo é aplicada uma margem (<span class="math inline">\(\alpha\)</span>) de ajuste.</p>

<div class="figure" style="text-align: center"><span id="fig:erro-triplet"></span>
<img src="imagens/07-deepLearning/erro-triplet.png" alt="Comportamento da função erro Triplet. A função Triplet reduz a distância entre os exemplos similares, ou seja, entre a âncora e o padrão positivo, e aumenta a distância entre o padrão diferente (caso negativo) e a âncora [61]." width="40%" />
<p class="caption">
Figura 7.45: Comportamento da função erro <em>Triplet</em>. A função <em>Triplet</em> reduz a distância entre os exemplos similares, ou seja, entre a âncora e o padrão positivo, e aumenta a distância entre o padrão diferente (caso negativo) e a âncora <span class="citation">[<a href="#ref-schroff2015" role="doc-biblioref">61</a>]</span>.
</p>
</div>

<!-- nixon2019feature -->
<!-- davies2017 -->
</div>
</div>
</div>
<h3>Refêrencias</h3>
<div id="refs" class="references">
<div id="ref-goodfellow2016">
<p>[22] I. Goodfellow, Y. Bengio, e A. Courville, <em>Deep Learning</em>. MIT Press, 2016.</p>
</div>
<div id="ref-img:deepblue">
<p>[23] A. Chiang, “IBM Deep Blue at Computer History Museum”. 2020, [Online]. Disponível em: <a href="https://commons.wikimedia.org/wiki/File:IBM_Deep_Blue_at_Computer_History_Museum_(9361685537).jpg">https://commons.wikimedia.org/wiki/File:IBM_Deep_Blue_at_Computer_History_Museum_(9361685537).jpg</a>.</p>
</div>
<div id="ref-cajal">
<p>[24] S. R. y Cajal, <em>Comparative study of the sensory areas of the human cortex</em>. Clark University, 1899.</p>
</div>
<div id="ref-mcculloch1943">
<p>[25] W. S. McCulloch e W. Pitts, “A logical calculus of the ideas immanent in nervous activity”, <em>The bulletin of mathematical biophysics</em>, vol. 5, nº 4, p. 115–133, 1943.</p>
</div>
<div id="ref-img:coloredNeuralNetwork">
<p>[26] Glosser.ca, “Artificial neural network with layer coloring”. 2013, [Online]. Disponível em: <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg">https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg</a>.</p>
</div>
<div id="ref-russell2016">
<p>[27] S. Russell e P. Norvig, “Artificial intelligence: a modern approach”, 2016.</p>
</div>
<div id="ref-haykin1999">
<p>[28] H. Simon, <em>Redes Neurais: Princípios e prática</em>, 2º ed. São Paulo: Bookman, 1999.</p>
</div>
<div id="ref-img:neuronCS">
<p>[29] cs231, “Cartoon drawing of a biological neuron”. 2021, [Online]. Disponível em: <a href="https://cs231n.github.io/neural-networks-1/">https://cs231n.github.io/neural-networks-1/</a>.</p>
</div>
<div id="ref-rateke1999">
<p>[30] T. Rateke, “Técnicas Subsimbólicas: Redes Neurais”. LAPIX (Image Processing; Computer Graphics Lab)/Universidade Federal de Santa Catarina, Florianópolis, [Online]. Disponível em: <a href="http://www.lapix.ufsc.br/ensino/reconhecimento-de-padroes/tecnicas-sub-simbolicas-redes-neurais/">http://www.lapix.ufsc.br/ensino/reconhecimento-de-padroes/tecnicas-sub-simbolicas-redes-neurais/</a>.</p>
</div>
<div id="ref-nielsen2015">
<p>[31] M. A. Nielsen, <em>Neural Networks and Deep Learning</em>. Determination Press, 2015.</p>
</div>
<div id="ref-hertz2018">
<p>[32] J. A. Herts, A. S. Krogh, e R. G. Palmer, <em>Introduction to the theory of neural computation</em>. New York: CRC Press, 2018.</p>
</div>
<div id="ref-geron2019">
<p>[33] A. Géron, <em>Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems</em>. O’Reilly Media, 2019.</p>
</div>
<div id="ref-adrian2017">
<p>[34] R. D. Adrian, “Deep Learning for Computer Vision with Python”. PyImageSearch, 2017.</p>
</div>
<div id="ref-dumoulin2016">
<p>[35] V. Dumoulin e F. Visin, “A guide to convolution arithmetic for deep learning”, <em>arXiv preprint arXiv:1603.07285</em>, 2016.</p>
</div>
<div id="ref-elgendy2020">
<p>[36] M. Elgendy, <em>Deep Learning for Vision Systems</em>. Manning Publications, 2020.</p>
</div>
<div id="ref-zhang2020dive">
<p>[37] A. Zhang, Z. C. Lipton, M. Li, e A. J. Smola, <em>Dive into Deep Learning</em>. 2020.</p>
</div>
<div id="ref-johnson2019">
<p>[38] J. Johnson, “Deep Learning for Computer Vision - Fall 2019”. Website do curso EECS 498-007 / 598-005 na Universidade de Michigan, Michigan, Estados Unidos, 2019, [Online]. Disponível em: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/</a>.</p>
</div>
<div id="ref-imagenet2020">
<p>[39] P. U. Stanford Vision Lab Stanford University, “ImageNet Large Scale Visual Recognition Challenge (ILSVRC)”. Website da competição ImageNet Large Scale Visual Recognition Challenge (ILSVRC), 2020, [Online]. Disponível em: <a href="https://image-net.org/challenges/LSVRC/">https://image-net.org/challenges/LSVRC/</a>.</p>
</div>
<div id="ref-krizhevsky2012">
<p>[40] A. Krizhevsky, I. Sutskever, e G. E. Hinton, “Imagenet classification with deep convolutional neural networks”, <em>Advances in neural information processing systems</em>, vol. 25, p. 1097–1105, 2012.</p>
</div>
<div id="ref-canziani2016">
<p>[41] A. Canziani, A. Paszke, e E. Culurciello, “An analysis of deep neural network models for practical applications”, <em>arXiv preprint arXiv:1605.07678</em>, 2016.</p>
</div>
<div id="ref-szegedy2015">
<p>[42] C. Szegedy <em>et al.</em>, “Going deeper with convolutions”, in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, p. 1–9.</p>
</div>
<div id="ref-he2016">
<p>[43] K. He, X. Zhang, S. Ren, e J. Sun, “Deep residual learning for image recognition”, in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, p. 770–778.</p>
</div>
<div id="ref-googlecloud2021">
<p>[44] G. Developers, “Guia para iniciantes da Cloud TPU”. Website da Google Cloud, 2021, [Online]. Disponível em: <a href="https://cloud.google.com/tpu/docs/beginners-guide?hl=pt-br">https://cloud.google.com/tpu/docs/beginners-guide?hl=pt-br</a>.</p>
</div>
<div id="ref-wangenheim2018">
<p>[45] A. von Wangenheim, “Deep Learning: Introdução ao Novo Coneccionismo”. LAPIX (Image Processing; Computer Graphics Lab)/Universidade Federal de Santa Catarina, Florianópolis, 2018, [Online]. Disponível em: <a href="http://www.lapix.ufsc.br/ensino/reconhecimento-de-padroes/tecnicas-sub-simbolicas-redes-neurais/">http://www.lapix.ufsc.br/ensino/reconhecimento-de-padroes/tecnicas-sub-simbolicas-redes-neurais/</a>.</p>
</div>
<div id="ref-tensorflow2020">
<p>[46] G. Developers, “Versões da API TensorFlow”. Website do TensorFlow, 2020, [Online]. Disponível em: <a href="https://www.tensorflow.org/versions">https://www.tensorflow.org/versions</a>.</p>
</div>
<div id="ref-walker2021">
<p>[47] J. M. Walker, <em>Artificial Neural Networks</em>, 3º ed. Nova Iorque, Estados Unidos: Humana/Springer Nature, 2021.</p>
</div>
<div id="ref-chopra2005">
<p>[48] S. Chopra, R. Hadsell, e Y. LeCun, “Learning a similarity metric discriminatively, with application to face verification”, in <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</em>, 2005, vol. 1, p. 539–546.</p>
</div>
<div id="ref-taigman2014">
<p>[49] Y. Taigman, M. Yang, M. Ranzato, e L. Wolf, “Deepface: Closing the gap to human-level performance in face verification”, in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2014, p. 1701–1708.</p>
</div>
<div id="ref-sabri2018">
<p>[50] M. Sabri e T. Kurita, “Facial expression intensity estimation using Siamese and triplet networks”, <em>Neurocomputing</em>, vol. 313, p. 143–154, 2018.</p>
</div>
<div id="ref-lambda2019">
<p>[51] H. Lamba, “One Shot Learning with Siamese Networks using Keras”. Website Towards Data Science, Florianópolis, 2019, [Online]. Disponível em: <a href="https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d">https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d</a>.</p>
</div>
<div id="ref-zhang2018">
<p>[52] H. Zhang, W. Ni, W. Yan, J. Wu, H. Bian, e D. Xiang, “Visual tracking using Siamese convolutional neural network with region proposal and domain specific updating”, <em>Neurocomputing</em>, vol. 275, p. 2645–2655, 2018.</p>
</div>
<div id="ref-zhang2016">
<p>[53] C. Zhang, W. Liu, H. Ma, e H. Fu, “Siamese neural network based gait recognition for human identification”, in <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2016, p. 2832–2836.</p>
</div>
<div id="ref-liu2017">
<p>[54] X. Liu, W. Liu, T. Mei, e H. Ma, “Provid: Progressive and multimodal vehicle reidentification for large-scale urban surveillance”, <em>IEEE Transactions on Multimedia</em>, vol. 20, nº 3, p. 645–658, 2017.</p>
</div>
<div id="ref-wang2017">
<p>[55] Q. Wang, J. Gao, e Y. Yuan, “Embedding structured contour and location prior in siamesed fully convolutional networks for road detection”, <em>IEEE Transactions on Intelligent Transportation Systems</em>, vol. 19, nº 1, p. 230–241, 2017.</p>
</div>
<div id="ref-gonzalez2019">
<p>[56] J.-Á. González, E. Segarra, F. Garcı́a-Granada, E. Sanchis, e L.-F. Hurtado, “Siamese hierarchical attention networks for extractive summarization”, <em>Journal of Intelligent &amp; Fuzzy Systems</em>, vol. 36, nº 5, p. 4599–4607, 2019.</p>
</div>
<div id="ref-das2016">
<p>[57] A. Das, H. Yenala, M. Chinnakotla, e M. Shrivastava, “Together we stand: Siamese networks for similar question retrieval”, in <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2016, p. 378–387.</p>
</div>
<div id="ref-bromley1993">
<p>[58] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, e R. Shah, “Signature verification using a" siamese" time delay neural network”, <em>Advances in neural information processing systems</em>, vol. 6, p. 737–744, 1993.</p>
</div>
<div id="ref-gomez2019">
<p>[59] R. Gómez, “Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names”. Blog Raúl Gómez, 2019, [Online]. Disponível em: <a href="https://gombru.github.io/2019/04/03/ranking_loss/">https://gombru.github.io/2019/04/03/ranking_loss/</a>.</p>
</div>
<div id="ref-hadsell2006">
<p>[60] R. Hadsell, S. Chopra, e Y. LeCun, “Dimensionality reduction by learning an invariant mapping”, in <em>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)</em>, 2006, vol. 2, p. 1735–1742.</p>
</div>
<div id="ref-schroff2015">
<p>[61] F. Schroff, D. Kalenichenko, e J. Philbin, “Facenet: A unified embedding for face recognition and clustering”, in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, p. 815–823.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="segmentação.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="separação-plano-de-fundo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": null
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/covap-utfpr/pdi/edit/master/07-deep_learning.Rmd",
"text": "Editar "
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"citation_package": "biblatex"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
