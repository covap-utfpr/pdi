[["index.html", "Material introdutório de Processamento Digital de Imagens(PDI) Inicio", " Material introdutório de Processamento Digital de Imagens(PDI) Inicio Esse site foi desenvolvido pelo grupo COVAP (Computação Visual Aplicada) visando oferecer um material introdutório da disciplina de PDI para alunos e pessoas interessadas no assunto. Os conteúdos se encontram dividos na seguinte estrutura: Princípios básicos Formação da imagem Tipos de arquivo Espaço de cores Transformações Compressão de imagens Filtros Detectores Morfologia matemática Lembramos ainda que todo o material está hospedado no Github, agradeçemos então todo apontamento de erros e sugestões, para que assim consigamos sempre disponibilizar um conteudo revisado e livre de quaisquer erros. "],["intro.html", "Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica 1.2 Aplicações Processamento Digital de Imagens 1.3 Etapas do Processamento e Análise de Imagens", " Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica A visão desempenha um papel importante na vida das pessoas, pois com ela é possível uma percepção incrivelmente rica do mundo ao seu redor. Para tentar reproduzir as capacidades visuais humanas por sistemas autônomos manipulados por computadores foram desenvolvidas pelo menos três grandes áreas [1, p. 2]: Processamento Digital de Imagens (PDI), Visão Computacional (VC) e a Computação Gráfica(CG), apresentados na Figura 1.1. Essas áreas, apesar de serem correlacionadas, têm objetivos e métodos diferentes, por isso a importância de distingui-las. Figura 1.1: Processos Computacionais com Imagens [1, p. 2]. O Processamento Digital de Imagens (PDI) busca realizar o pré-processamento das imagens, utilizando para isso técnicas de tratamento, como a correção da iluminação, eliminação de ruído, e a segmentação. O foco da Visão Computacional (VC) é a análise das imagens, identificando os seus componentes e obtendo informações. Diferente da VC, em que as imagens são os objetos de estudo, na Computação Gráfica (CG), as imagens são o resultado do processo. Na CG são geradas representações visuais seguindo descrições e especificações geométricas [1, p. 3]. A Tabela 1.1 apresenta de forma resumida as diferenças entre PDI, VC e CG. Na segunda linha da tabela está uma descrição simples de cada área, e na terceira linha um esquema identificando o objeto e o produto de cada processo. Tabela 1.1: Processos Computacionais com Imagens. Computação Gráfica (CG) Visão Computacional (VC) Processamento Digital de Imagens (PDI) Cria e altera imagens a partir de dados. Análise de imagem para criação de modelos. Transformação de imagem (tratamento). modelo → imagem imagem → modelo imagem → imagem As imagens tratadas em PDI têm como uma das finalidades servir de material para a Visão Computacional, como identificado na Figura 1.1. Muitas vezes as áreas de Visão Computacional e PDI são confundidas devido a dificuldade em se definir em que ponto uma termina e a outra começa. Mesmo não existindo uma linha clara entre os limites destas duas áreas é possível utilizar um paradigma que considera três níveis de processamento [2, p. 2]: Baixo nível A nível de pixel, realiza operações de pré-processamento, sendo utilizada, por exemplo, na redução de ruído, aumento de contraste e restauração. Médio nível Operações mais complexas, como segmentação, partição e reconhecimento de objetos individuais. Entrada é uma imagem mas a saída pode ser um conjunto contendo os atributos extraídos das imagens. Alto nível Interpretação do conteúdo da imagem e análise. Baseado nesses níveis, iremos considerar que o processamento de imagem atua nos primeiros dois níveis, ou seja, envolve o pré-processamento e processos de extração de elementos de imagens até o reconhecimento de componentes individuais. Como o foco deste material é o Processamento de Imagens Digitais (PDI), estes dois níveis serão apresentados com detalhes nos próximos tópicos, mas primeiro vejamos alguns exemplos de aplicações do PDI. 1.2 Aplicações Processamento Digital de Imagens As primeiras tarefas de processamento de imagens tiveram aplicações significativas por volta da década de 1960, quando se desenvolveram computadores com potencial suficiente para realizá-las. O programa espacial americano também foi um forte impulso para o contínuo desenvolvimento e aprimoramento das técnicas de processamento digital de imagem (PDI), já que imagens, como as obtidas da Lua através de sondas e transmitidas à terra, continham distorções provenientes das câmeras utilizadas. Era necessário então, a utilização de métodos para corrigir essas alterações [2, p. 4]. Outra área que também faz uso extensivo do processamento de imagens e impulsionou seu desenvolvimento é a área médica. Nessa área, o uso de imagens auxiliou no diagnóstico de doenças através de exames visuais como os de raio-x [2, p. 4]. A utilização do processamento de imagens para melhorar informações visuais, ajudando na interpretação humana, expandiu-se para diferentes setores. No sensoriamento remoto, o pré-processamento contribui para uma melhor análise de imagens aéreas e de satélite, aumentando a compreensão da superfície terrestre. Na arqueologia e nas artes, métodos de processamento de imagens podem restaurar fotografias com registros únicos de objetos raros, pinturas, documentos antigos e conteúdos em vídeos [3, p. 2]. Na física e em áreas da biologia, técnicas computacionais realçam imagens de experimentos em áreas como plasmas de alta energia e microscopia eletrônica [2, p. 5]. Com o aumento da automatização de tarefas, o processamento de imagens tem se destacado na aquisição de dados de imagens visando a percepção automática por máquinas [3, p. 3]. Técnicas de identificação de padrões podem ser aplicados no reconhecimento automático de caracteres, de impressões digitais, de faces, e de placas de veículos, contribuindo com setores de segurança. Na automação industrial tem sido utilizado no sistema de visão computacional para inspeção e montagem de produtos. Na área militar, pode ser aplicado na identificação e rastreamento de alvos em imagens de satélites, e na navegação de veículos autônomos. Nas áreas de medicina e biologia, rastreamentos automáticos em imagens radiográficas e amostras de sangue têm contribuído para os exames e testes [3, p. 3]. O processamento computacional de imagens aéreas e de satélites também é utilizado na previsão do tempo e em avaliações ambientais [2, p. 5]. Este variado campo de aplicações pode ser justificado pela capacidade dos aparelhos de processamento de imagens trabalharem com imagens de diversas fontes. Diferentemente dos seres humanos, que são limitados à banda visual do espectro eletromagnético (EM), o processamento computacional cobre todo o EM, variando de ondas gama a ondas de rádio [2, p. 1]. No processamento digital ainda é possível trabalhar com imagens geradas por fontes que os humanos não estão acostumados a associar com imagens. Essas fontes incluem acústica, ultrassom, microscopia eletrônica e imagens geradas por computador [2, p. 13]. Uma das formas mais fáceis de desenvolver uma compreensão básica da extensão das aplicações do processamento de imagens é categorizar as imagens de acordo com sua fonte. Na Figura 1.2 temos uma representação do EM, iremos a seguir explorar cada uma dessa faixas, apresentando algumas das áreas onde podem ser utilizados: Figura 1.2: Espectro eletromagnético [4]. Imagens formadas por raios gama As imagens formadas a partir de raios gama têm diferentes utilidades, sendo muito utilizadas na medicina e astronomia [2, p. 6]. Na medicina, existem procedimentos onde se injetam isótopos radioativos no paciente e por meio dos detectores de raio gama é formada uma imagem, como exemplo, escaneamento ósseo e tomografia por emissão de pósitrons (PET-scan). Na astronomia, ela pode ser utilizada para se conseguir ver detalhes astronômicos que estão presentes na faixa eletromagnética dos raios gama. Imagens formadas por raios X Imagens formadas a partir de raio X têm uma ampla gama de aplicações, desde seu uso na medicina até seu uso no meio industrial [2, p. 6]. Na indústria, pode ser utilizado para se encontrar defeitos de fabricação em produtos, e na medicina, vêm se utilizando muito o processamento de imagem e a visão computacional para ajudar no diagnóstico de doenças, como por exemplo, artérias obstruídas, fraturas e tumores. Imagens na banda ultravioleta O espectro ultravioleta também tem inúmeras aplicações, como a inspeção industrial, microscopia, imagens biológicas e observações astronômicas [2, p. 8]. Imagens na banda visível e infravermelho Essas duas bandas possuem uma gama extremamente ampla de aplicações, sendo utilizadas juntas ou separadas. Na banda visível, existem diversas aplicações, como em processos industriais, detecção de faces, detecção de placas de carros, etc [2, p. 11]. A banda infravermelho também possui inúmeras aplicações, sendo uma delas imagens a partir de satélites, onde o infravermelho nos permite ver inúmeros detalhes que somente com a banda visível não seria possível [2, p. 9]. Imagens na banda de micro-ondas e rádio Na banda de micro-ondas o melhor exemplo que temos é o radar. Essa banda tem uma peculiaridade de ser extremamente penetrante, podendo gerar imagens através de nuvens, vegetação, etc [2, p. 12]. Já a banda de rádio é muito utilizada na medicina, como exemplo na ressonância magnética e na astronomia [2, p. 12]. Como podemos observar, existem inúmeras maneiras de se conseguir imagens além da clássica imagem no espectro visível, isso nos dá a possibilidade de utilizar o PDI em inúmeras áreas e problemas. Na Figura 1.3 temos uma nebulosa observada a partir de diferentes bandas dos EM, sendo possível observar detalhes que passariam despercebidos se usássemos somente alguma delas. Figura 1.3: Nebulosa CRAB em diferentes frequências [5]. 1.3 Etapas do Processamento e Análise de Imagens Um dos objetivos deste material é servir de referência para o estudo inicial de Visão computacional, assim, para compreender as relações entre as etapas de processamento e análise de imagens apresentamos na Figura 1.4 uma sequência dos principais passos utilizados em uma aplicação de PDI. Neste material nos deteremos nos conteúdos de processamento, desde a aquisição de imagens, pré-processamento até segmentação. Figura 1.4: Etapas de aplicação de PDI [3, p. 4]. Vale ressaltar que essas etapas não são fixas e podem ser modificadas, sendo que uma base de conhecimentos é importante para orientar em uma aplicação específica [3, p. 4]. Aquisição da imagem Captura a imagem por meio de um dispositivo ou sensor e a converte em uma imagem digitalizada [3, p. 3]. Podemos citar como exemplo as câmeras fotográficas, tomógrafos médicos, satélites e scanners. Na Figura 1.5, temos um exemplo de aquisição de imagens do satélite Landsat, neste caso estão identificadas as bandas vermelha, verde e azul visíveis e o infravermelho próximo. Os detalhes sobre a aquisição de imagens serão discutidos no tópico Formação de Imagem. Figura 1.5: Aquisição de imagens de satélite.(a) a (d) mostram quatro imagens espectrais de satélite da cidade de Washington, D.C., banda vermelha, verde e azul visíveis e o infravermelho próximo, respectivamente. (e) Imagem colorida como combinação RGB de (a), (b) e (c). (f) Imagem colorida obtida pela combinação de (b), (c) e (d)[2, p. 279]. Pré processamento Essa etapa busca realizar mudanças e ajustes na imagem visando melhorar seu uso nas etapas futuras [3, p. 3]. Como exemplo temos casos onde não precisamos das cores de uma imagem, podendo então realizar a conversão para grayscale(tons de cinza), ou precisamos gerar imagens coloridas como na Figura 1.5 em que são combinadas as bandas espectrais. Além disso, podemos realizar cortes ou realces, isolando somente a parte de maior interesse na imagem como na Figura 1.6, ou também atenuar o ruído na imagem, além de outras técnicas que serão abordadas em outros tópicos. Figura 1.6: Subtração de imagens para realce de diferenças.(a) Imagem da área de Washington D.C em infravermelho. (b) Resultado ao zerar o bit menos significativo de todos os pixels de (a). (c) Diferença entre as duas imagens ajustada para a faixa [0, 255], sendo que valores em preto (0) indicam pontos nos quais não há nenhuma diferença [2, p. 49]. Segmentação Nessa etapa as informações de interesse são extraídas da imagem, geralmente, pela detecção de descontinuidades (bordas) ou de similaridades na imagem [3, p. 4]. Na Figura 1.7 é mostrado o resultado de um exemplo de segmentação por similaridade, em que o elemento de maior interesse é o rio. Figura 1.7: Extração de características de uma imagem segmentada.(a) Imagem na banda infravermelha da área de Washington, D.C. (b) Segmentação da imagem por limiarização. (c) O maior componente conexo de (b). Técnica de representação por esqueleto de (c) [2, p. 544]. Representação e Descrição Armazenar e manipular objetos de interesse extraídos da imagem. O processo de descrição visa a extração de características para discriminar classes de objetos [3, p. 4]. Na Figura 1.8, o objetivo é determinar o tamanho das ramificações do rio, para isto considerou-se que o tamanho de cada ramificação no esqueleto seria uma boa aproximação [2, p. 545]. O esqueleto é uma representação do rio, e seus elementos são discriminados dentro do maior componente conexo da imagem segmentada. Reconhecimento e Interpretação Essa etapa examina as informações produzidas na etapa anterior e classifica cada objeto como sendo de interesse ou não, atribuindo significado ao conjunto de objetos reconhecidos pelos rótulos [3, p. 3]. Uma aplicação de análise de imagens inclui a classificação de áreas em uma imagem multiespectral como na Figura 1.8. Neste exemplo utilizou-se o método bayesiano, em que cada pixel da imagem foi avaliado em relação a três classes (água, desenvolvimento urbano e vegetação). Nas Figuras 1.8, pontos pretos representam pontos classificados incorretamente, enquanto pontos brancos foram classificados corretamente [2, p. 579]. Figura 1.8: Classificação bayesiana em uma imagem multiespectral. Resultado (em branco) da classificação na classe água, desenvolvimento urbano e vegetação, da esquerda para a direita [2, p. 579]. Refêrencias "],["formação-da-imagem.html", "Capítulo 2 Formação da imagem 2.1 Câmera pinhole e geometria 2.2 Lentes 2.3 Sensor 2.4 Amostragem e Quantização 2.5 Definição de imagem digital 2.6 Resolução espacial e de intensidade 2.7 Pixels", " Capítulo 2 Formação da imagem Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é do espectro eletromagnético, mas podendo ser também, a partir da energia mecânica (ultrassom), feixe de elétrons, etc.. Cada fonte necessita de um método específico de captura, para algumas pode ser uma câmera fotográfica, porém a outras é necessário que o computador sintetize a imagem, como o microscópio eletrônico. Como já mencionado no tópico de introdução, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensitivos a essa faixa de luzes, que vêm da luz solar e nos ajuda a realizar nossas atividades. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas, como a ultravioleta[6, p. 2]. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas[7, p. 8]. A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de 0,43 a 0,79 \\(\\mu m\\). Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar[2, p. 28]. Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir. 2.1 Câmera pinhole e geometria Na figura 2.1 temos um esquema básico de como geralmente ocorre a aquisição de imagens, primeiramente a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida, sendo após isso capturada por um dispositivo, como uma câmera. Figura 2.1: Representação de uma típica captura de imagem [7, p. 8]. Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, este é conhecido como câmara pinhole(do inglês buraco de alfinete) ou câmara escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na figura 2.2, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruim. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores. Figura 2.2: Introdução de barreira para captura de imagem [7, p. 11]. Na figura 2.2 percebemos que a imagem resultante acaba invertida, isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir. Figura 2.3: Geometria de uma câmera pinhole [8, p. 5]. Na figura 2.3, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância \\(Z\\) da abertura e a uma distância \\(Y\\) o eixo óptico, podemos definir a altura \\(y\\) e a largura \\(x\\) da projeção do objeto utilizando a simetria de triângulos: \\[-\\frac{y}{f}=\\frac{Y}{Z}\\Leftrightarrow y=-f\\frac{Y}{Z} \\text{ e } -\\frac{x}{f}=\\frac{x}{f} \\Leftrightarrow x=-f\\frac{X}{Z}\\] A variável \\(f\\) nessa equação se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera. Os sinais negativos das equações significam que a imagem projetada está rotacionada a 180º verticalmente e horizontalmente, como podemos confirmar na imagem acima. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens, como precisar de um longo tempo de exposição para captura da imagem. 2.2 Lentes As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso. Figura 2.4: Ação de uma lente sobre os raios de luz [7, p. 12]. Como podemos ver na figura 2.4, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando-se o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada. O ponto \\(F\\) onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância \\(f\\), que vai do centro óptico \\(O\\) até \\(F\\) é conhecida como Distância Focal. Definindo a distância do objeto real até a lente como g e a distância até a formação da imagem após passa pela lente como b temos que: \\[\\frac{1}{g}+\\frac{1}{b}=\\frac{1}{f}\\] Como \\(f\\) e \\(b\\) estão normalmente entre 1 mm e 100mm isso mostra que \\(\\frac{1}{g}\\) não tem quase nenhum impacto na equação e significa que \\(b = f\\). Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal. Outro ponto importante das lentes é conhecido como zoom óptico. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, \\(B\\), aumenta quando \\(f\\) aumenta. Podemos representar isso na seguinte equação, onde \\(g\\) é o tamanho real do objeto: \\[\\frac{b}{B}=\\frac{g}{G}\\] Na prática \\(f\\) é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos. Se o \\(f\\) for constante, quando alteramos a distância do objeto, no caso \\(g\\), sabemos que \\(b\\) também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos \\(b\\) temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando \\(b\\) para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco. Figura 2.5: Uma imagem focada e desfocada [7, p. 11]. A figura 2.5 ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos. Figura 2.6: Profundidade de campo [7, p. 13]. A figura 2.6 apresenta outro ponto muito importante, chamado Profundidade de Campo(Depth of field), que representa a soma das distâncias \\(g_l\\) e \\(g_r\\), que representam o quando os objetos podem ser movidos e permanecerem em foco. Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão(Field of View ou FOV) que representa a área observável de uma câmera. Na figura 2.7 essa área observável é denotada pelo ângulo \\(V\\). O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as equações seguintes para o FOV vertical e horizontal: \\[FOV_x = 2*\\tan^{-1}\\left(\\frac{\\frac{comprimento\\ do\\ sensor}{2}}{f}\\right) \\text{ e } FOV_y = 2*\\tan^{-1}\\left(\\frac{\\frac{altura\\ do\\ sensor}{2}}{f}\\right)\\] Figura 2.7: Campo de visão [7, p. 14]. Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de 14mm, altura de 10mm e uma distância focal de 5mm temos: \\[FOV_x=2*tan^{-1}\\left(\\frac{7}{5}\\right)=108.0^{\\circ} \\text{ e } FOV_y=2*tan^{-1}(1)=90^{\\circ}\\] Isso significa que essa câmera tem uma área observal de 108.9º horizontalmente e 90º verticalmente. Na figura 2.8 temos o mesmo objeto fotografado com diferentes profundidades de campo: Figura 2.8: Diferentes profundidades de campo [7, p. 15]. Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris no olho humano, ela controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para capturar a imagem. 2.3 Sensor Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD, que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS, usado em casos mais gerais, como câmeras de celulares. Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na figura 2.9, conhecido como PDA(Photodiode Array): Figura 2.9: Sensor(area matricial de celulas), Single Cell(uma única celula sensora) [7, p. 17]. Como podemos ver, o sensor consiste em várias pequenas células, cada uma um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na figura 2.10 podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta(correctly exposed), logo em seguida temos uma que sofreu de superexposição(overexposed) e na terceira temos uma com subexposição(under exposed). Por último temos uma imagem que sobre com o movimento do objeto que estava sendo capturado, oque ocasionou o borramento(motion blur). Figura 2.10: Diferentes níveis de exposição [7, p. 17]. Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas em imagens coloridas, como são capturadas? Imagens coloridas utilizam, especialmente, o formato RGB, que significa Red-Green-Blue, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na figura 2.11 podemos ver uma imagem com seus componentes separados: Figura 2.11: Imagem colorida separada em seus três componentes [7, p. 28]. Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na figura 2.12. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo. Figura 2.12: Captura de imagem com três sensores [9, p. 242]. Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel, isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na figura 2.13: Figura 2.13: Filtro Bayer [7, p. 29]. Podemos perceber que ocorre uma maior ocorrência das cores verdes, isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase a sua captura. Na figura 14 temos uma esquematização de como cada pixel recebe informação de somente uma cor, por meio da filtragem, onde a luz que entra(Incoming light) é filtrada e somente a cor de interesse consegue passar, após isso ela chega a malha de sensores(sensor array): Figura 2.14: Sensores com padrão Bayer [10]. Vemos na figura 2.14 que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos. 2.4 Amostragem e Quantização Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto. 2.4.1 Amostragem Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura 2.15, na qual a figura 2.15 (a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto. A figura 2.15 (b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha AB. Nas extremidades do gráfico na figura 2.15 (b), a intensidade é mais alta devido a parte branca da imagem, já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na figura 2.15 (c). Esse procedimento de amostragem, na prática, é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo teríamos que ter um número infinito de pixels, como isso não é possível recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem [2]. Figura 2.15: Filtro Bayer [2, p. 34]. 2.4.2 Quantização Na figura 2.15 (c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na figura 2.15 (d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na figura 2.15 (c). Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital[8, p. 8]. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo[11, p. 9]. No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos \\(\\{0,\\dots, L-1\\}\\), onde \\(L\\) é uma potência de dois, ou seja, \\(L = 2_k\\) [11, p. 10]. Isso significa que L é o número de tons de cinza que podem ser representados com uma quantidade k de bits. Em muitas situações é utilizado \\(k = 8\\), ou seja, temos 256 níveis de cinza. Ao realizar a quantização e a amostragem linha por linha no objeto da figura 2.16 (a) é produzida uma imagem digital bidimensional como na figura 2.16. Figura 2.16: Filtro Bayer [2, p. 35]. 2.5 Definição de imagem digital Uma imagem pode ser definida como uma função bidimensional, \\(f(x, y)\\), em que \\(x\\) e \\(y\\) são coordenadas espaciais (plano), e a amplitude de \\(f\\) em qualquer par de coordenadas \\((x, y)\\) é chamada de intensidade ou nível de cinza da imagem nesse ponto [2]. Quando \\(x\\), \\(y\\) e os valores de intensidade de f são quantidades finitas e discretas, chamamos de imagem digital. A função \\(f(x, y)\\) pode ser representada na forma de uma matriz (M x N) como na Figura, em que as M linhas são identificadas pelas coordenadas em \\(x\\), e as N colunas em \\(y\\). Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou pel. O formato numérico da matriz, imagem 2.17, é apropriado para o desenvolvimento de algoritmos, particularmente quando se escreve a equação da matriz (M x N): \\[f(x,y) = \\begin{bmatrix} f(0,0) &amp; f(0,1) &amp; \\cdots &amp; f(0,N-1) \\\\ f(1,0) &amp; f(1,1) &amp; \\cdots &amp; f(1, N-1) \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ f(M-1,0) &amp; f(M-1, 1) &amp; \\cdots &amp; f(M-1, N-1) \\end{bmatrix}\\] Figura 2.17: Representações da imagem digital [2, p. 36]. Na figura 2.17 (a) temos a representação da imagem em 3D, onde a intensidade de cada pixel é representada no eixo z, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na figura 2.17 (b), formato que seria visualizado em um monitor ou uma fotografia [2]. Em cada ponto da figura 2.17 (a), o nível de cinza é proporcional ao valor da intensidade \\(f\\), assumindo valores 0, 0,5 ou 1. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco. Note que na Figura, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo x positivo direcionado para baixo e o eixo y positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez [2]. De acordo com o tamanho da matriz (M x N) e dos níveis discretos de tons de cinza (\\(L = 2^k\\)) que os pixels podem assumir é possível determinar o número, \\(b\\), de bits necessários para armazenar uma imagem digitalizada: \\[b = M × N × k\\] Quando uma imagem pode ter \\(2^k\\) níveis de intensidade, geralmente ela é denominada como uma “imagem de k bits”. Por exemplo, uma imagem com 256 níveis discretos de intensidade é chamada de uma imagem de 8 bits. A figura 2.18 mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (N x N) para diferentes valores de N e k. O número de níveis de intensidade (L) correspondente a cada valor de k é mostrado entre parênteses. Observa-se na figura 2.18 que uma imagem de 8 bits com dimensões 1.024 × 1.024 exigiria aproximadamente 1MB para armazenamento. Figura 2.18: Número de bits de armazenamento para vários valores de N e k [2, p. 38]. 2.6 Resolução espacial e de intensidade Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (M x N) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente dots per inch (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2.400 dpi [2]. A figura 2.19 mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A figura 2.19 (a) tem resolução 512 x 512, e a resolução das demais 2.19 (b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução. Figura 2.19: Efeitos da redução da resolução espacial [3, p. 20]. A resolução de intensidade ou profundidade corresponde ao número de bits (k) utilizados para estabelecer os níveis de cinza da imagem (\\(L=2^k\\)). Por exemplo, em uma imagem cuja intensidade é quantizada em L= 256 níveis, a profundidade é de k = 8 bits por pixel. Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura. A imagem (a) apresenta 256 níveis de cinza (k = 8). As imagens (b) e (c) foram geradas pela redução do número de bits k = 4 e k = 2, respectivamente, mas mantendo a mesma dimensão. Figura 2.20: Efeitos da redução de profundidade [7, p. 19]. 2.7 Pixels A topologia digital da imagem desempenha muita importância na especificação, localização e relação entre as coordenadas da imagem, facilitando sua manipulação. A tipologia de uma imagem digital contém as seguintes propriedades dos pixels: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas. Para especificar, localizar, e relacionar topologicamente uma imagem digitalizada, consideramos: p,q denotando os pontos e, \\(p(x,y)\\), \\(q(x,y)\\), \\(u(x,y)\\) coordenadas dos pontos denotados, expressaremos \\(V\\) Conjunto de valores em uma imagem binária \\(V=\\{0,1\\}\\). 2.7.1 Vizinhança Vizinhança 4 - \\(\\left[N_4(p)\\right]\\) \\(N_4(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na horizontal outros dois na vertical suas coordenadas, ou seja, é o conjunto de pixels ao redor de p, sem considerar as diagonais [12, p. 15]. Exemplo na figura 2.21 (a). \\[p(x,y): p(x+1, y), p(x-1, y), p(x, y+1), p(x, y-1)\\] Vizinhança D - \\(\\left[N_D(p)\\right]\\) \\(N_D(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na diagonais superiores (direita, esquerda ) outras duas na diagonais inferiores (direita, esquerda) suas coordenadas, ou seja o conjunto de pixels ao redor de \\(p\\), considerando apenas as diagonais [12, p. 15].Exemplo na figura 2.21 (b). \\[p(x,y): p(x+1,y+1), p(x+1, y-1), p(x-1, y+1), p(x-1, y-1)\\] Vizinhança 8 - \\(\\left[N_8(p)\\right]\\) \\(N_8(p)\\) em \\(p(x,y)\\) possui 8 vizinhos, quatro \\(N_4(p)\\) e outros 4 \\(N_D(p)\\)suas coordenadas ou seja o conjunto de pixels ao redor de \\(p\\), considerando união das vizinhanças-4 e vizinhança-8 [12, p. 15]. Exemplo na figura 2.21 (c). \\[p(x,y): N_8(p) = N_4(p) \\cup N_D(p)\\] Figura 2.21: Vizinhanças [12]. 2.7.2 Conectividade Conceito importante, usado no estabelecimento limite das bordas de objetos e, identifica componentes das regiões da imagem (obtenção de propriedades específicas do objeto para o processamento de mais alto nível ). Dois pixels \\(p(x,y)\\), \\(q(x,y)\\) estão conectados se: São de alguma forma vizinhos (\\(N_4\\),\\(N_D\\) ou \\(N_8\\)). Seus níveis de cinza satisfazem algum critério de similaridade (\\(V = \\{ \\dots \\}\\)). Conectividade de 4: Os pixels p e q, assumindo valores em &amp;V&amp; , são conectados de 4 somente se q pertence ao conjunto \\(N_4(p)\\). Exemplo em figura 2.22 (a). \\[C4_{p,q} \\text{ em } V \\Leftrightarrow q \\in N_4(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C4_{p.q} \\text{ verdadeiro}\\] Conectividade de m(conectividade mista): Dois pixels \\(p\\) e \\(q\\), assumindo valores em \\(V\\), são conectados de m somente se: \\(q\\) pertence ao conjunto \\(N_4(p)\\) \\(q\\) pertence ao conjunto \\(N_D(p)\\) e a interseção entre \\(N_4(p)\\) e \\(N_4(q)\\) for vazia. \\[Cm_{p,q} \\text{ em } V\\Leftrightarrow (q \\in N_4(p) \\vee (q \\in N_D(p) \\wedge N_4(p) \\cap N_4(q) = \\{\\})) \\vee f(p) e f(q) \\in V \\] \\[V = \\{0\\} \\to Cm_{p.q}\\text{ falso}\\] Exemplo em figura 2.22 (b). Conectividade de 8: Os pixels \\(p\\) e \\(q\\), assumindo valores em V, são conectados de 8 somente se \\(q\\) pertence ao conjunto \\(N_8(p)\\). Exemplo em figura 2.22 (c). \\[C8_{p,q} \\text{ em } V\\Leftrightarrow q \\in N_8(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C8_{p.q}\\text{ verdadeiro}\\] Figura 2.22: Conectividades [12]. 2.7.3 Adjacência Dois pixels \\(p\\) e \\(q\\), com valores pertencendo a \\(V\\) são: Adjacentes-4 se \\(q\\) estiver no conjunto \\(N_4(p)\\). Adjacentes-8 se \\(q\\) estiver no conjunto \\(N_8(p)\\). Adjacentes-m, \\(p\\) e \\(q\\) subconjuntos de pixels onde \\(\\{(pq) \\vee (pp) \\vee (q q) \\}\\), são ditos adjacentes se pegamos um pixel do primeiro conjunto for adjacente a um pixel do segundo. Na figura 2.23 temos exemplos de 1. e 2. (em 2.23 (a)) e de 3. em 2.23 (b). Figura 2.23: Adjacências [12]. 2.7.4 Componente Conexa Dois pixels \\(p\\) e \\(q\\) de um subconjunto de pixels \\(V\\) da imagem são ditos conexos em \\(V\\) se existir um caminho de pa qinteiramente contido em \\(V\\). Para qualquer pixel \\(p\\) em \\(V\\), o conjunto de pixels em \\(V\\) que são conexos a pé chamado um componente conexo de \\(V\\). Note que em uma componente conexo qualquer dois pixels deste componentes são conexos entre si. Em componentes conexos distintos os pixels são disjuntos (não conectados). 2.7.5 Medidas de Distância Para pixels \\(p\\), \\(q\\) e \\(z\\) com coordenadas \\(p(x,y)\\), \\(q(s,t)\\) e \\((u,v)\\), respectivamente, \\(D\\) é uma função distância ou métrica se: \\(D(p,q) &gt;= 0 (D(p,q) = 0 ) \\Leftrightarrow p = q\\) \\(D(p,q) = D(q,p)\\) \\(D(p,z) &lt;= D(p,q) + D(q,u)\\) A distância entre dois pontos quaisquer pode ser definida por: \\[D_e(p,q) = \\sqrt{(x-s)^2 + (y-t)^2}\\] conhecida como Distância Euclidiana. Distância \\(D_4\\)(City Block ou Quarteirão) entre \\(p(x,y)\\) e \\(q(s,t)\\)é definida por: \\[D4(p,q)=|x-s|+|y-t|\\] Distância D8(Distância Xadrez) entre \\(p\\) e \\(q\\) é definida como: \\[D_8 = max(|x-s|,|y-t|)\\] 2.7.6 Operações Lógico-aritméticas As operações entre pixels são computadas pixel a pixel, considerando p e qpodemos efetuar as seguintes operações aritméticas e lógicas. Operações Aritméticas: Adição: \\(p+q\\). O uso ocorre ao se fazer a média para redução de ruído. Subtração: \\(p-q\\). É usada para remover informação estática de fundo, Detecção de diferenças entre imagens. Multiplicação: \\(p\\cdot q\\). Calibração de brilho. Divisão: \\(p\\div q\\) As operações Aritmética de Multiplicação e Divisão são usadas para corrigir sombras em níveis de cinza, produzidas em não uniformidades da iluminação ou no sensor utilizado para a aquisição da imagem. Operações Lógicas: Conjunção: \\(p\\wedge q\\) Disjunção: \\(p\\vee q\\) Complementar: \\(\\neg q(\\bar{q})\\) As operações lógicas podem ser combinadas para formar qualquer outra operação lógica, são aplicadas apenas em imagens binárias, tem seu uso no mascaramento, detecção de características e análise de forma. Refêrencias "],["transformacões-geométricas.html", "Capítulo 3 Transformacões Geométricas 3.1 Definição 3.2 Representação Vetorial e Matricial da Imagem 3.3 Sistema de coordenadas objetos (2D e 3D) 3.4 Transformações em pontos e objetos 3.5 Transformações Lineares 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação 3.9 Transformação Cisalhamento (Shearing ou Skew) 3.10 Transformação de Reflexão 3.11 Características especias 3.12 Transformação Isométrica 3.13 Transformação Semelhança 3.14 Transformação Afim 3.15 Transformação Projetiva 3.16 Transformação Inversão 3.17 Classificação das Projeções Geométricas", " Capítulo 3 Transformacões Geométricas Nessa seção abordaremos conceitos importantes que detalham processos de transformações geométricas na imagem digitalizada, definiremos a seção proposta; representação vetorial e matricial da Imagem; sistema de coordenadas objetos (2D e 3D); transformações em pontos e objetos; transformações (lineares, translação, escala, rotação, cisalhamento (Shearing ou Skew), reflexão )); características especiais transformação (isométrica, semelhança, afim, projetiva, inversão); classificação das projeções geométricas. 3.1 Definição As transformações geométricas são operações que podem ser utilizadas sobre uma imagem, visando alteração de características como: posição , orientação, forma ou tamanho da imagem apresentada. As operações (transformações geométricas) não alteram a topologia (pixels) da imagem operada, apenas possibilitam a projeção da imagem no espaço determinado. 3.2 Representação Vetorial e Matricial da Imagem Podemos definir um vetor como segmento de reta orientada (sentido e direção). Pensado em um vetor 2D, representada por V, coordenadas, para o ponto P(x,y), tendo assim um sentido e uma direção, um sentido e um comprimento especificado. Podemos então calcular o comprimento do vetor 2D, pela fórmula: |V| = {sqr{x^2 + y^2}} 3.3 Sistema de coordenadas objetos (2D e 3D) 3.4 Transformações em pontos e objetos 3.5 Transformações Lineares 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação 3.9 Transformação Cisalhamento (Shearing ou Skew) 3.10 Transformação de Reflexão 3.11 Características especias 3.12 Transformação Isométrica 3.13 Transformação Semelhança 3.14 Transformação Afim 3.15 Transformação Projetiva 3.16 Transformação Inversão 3.17 Classificação das Projeções Geométricas "],["transformações-radiométricas.html", "Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência 4.4 Processamento de histograma 4.5 Equalização do histograma 4.6 Especificação de histograma", " Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência 4.4 Processamento de histograma 4.5 Equalização do histograma 4.6 Especificação de histograma "],["filtros.html", "Capítulo 5 Filtros 5.1 Convolução 5.2 Média 5.3 Mediana 5.4 Gaussiano", " Capítulo 5 Filtros Filtros são uma poderosa ferramenta para se realizar operações em imagens. Diferente das operações de ponto, que operam sobre um único pixel, as operações utilizando filtros levam em consideração os pixels próximos ao pixel atualmente em modificação. Isso nos permite realizar alterações muito mais complexas do que as realizadas anteriormente, como a operação sharpen (aguçamento) e blur (suavizar), que podem ser observados na figura 5.1 (a) e figura 5.1 (b), respectivamente. Figura 5.1: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] 5.1 Convolução Convolução é uma operação muito utilizada no PDI, a qual tem suas origens na matemática, onde ela é definida como uma operação realizada entre duas funções e que resulta numa terceira, ou, em outras palavras, ela recebe dois sinais de entrada e gera um sinal de saída. No caso do PDI, podemos imaginar os sinais de entrada como sendo a nossa imagem e o filtro (kernel), e a nossa saída como sendo a imagem filtrada. Quando dizemos kernel, estamos nos referindo a uma função ou, no caso do processamento de imagens, a uma matriz, que é aplicada em nossa imagem e produz como saída o objeto de entrada com modificações. Na tabela 5.1, temos exemplos de alguns tipos de kernels que podem ser utilizados na convolução e seus respectivos resultados. Além dos efeitos mais comuns, como o de desfoque (blur), podemos utilizar kernels que extraem informações mais complexas das imagens, como os detectores de borda, que serão discutidos mais a fundo nos próximos tópicos. Tabela 5.1: Exemplo de kernels (adaptada de [13]). Operação Kernel Resultado Identidade (Imagem Original) \\[\\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] Detecção de borda \\[\\begin{bmatrix} -1 &amp; -1 &amp; -1\\\\ -1 &amp; 8 &amp; -1\\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}\\] Média (box blur) \\[\\frac{1}{9}\\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\] Gaussian blur \\[\\frac{1}{16}\\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\ 2 &amp; 4 &amp; 2\\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\] Agora, veremos operação de convolução mais detalhadamente. Essa operação geralmente é representada por \\(*\\) e pode ser descrita, em poucas palavras, como uma soma de produtos que é realizada com um deslizamento sobre a função de entrada. A figura 5.2 representa de maneira visual a convolução em um cenário de uma dimensão, através do sinal de entrada (signal) e o kernel. Podemos ainda perceber que o kernel está rotacionado em 180º, isso se deve a definição de convolução. Figura 5.2: Convolução em uma dimensão [14] Algo interessante que podemos observar na imagem 5.2 é que nosso sinal de entrada é quase totalmente 0 e contém um único ponto 1, isso faz com que nosso resultado seja uma cópia do kernel. A partir disso, conseguimos imaginar o por que temos como resultado a própria imagem quando aplicado um kernel identidade, como mostrado na tabela 5.1. Antes de irmos mais adiante no assunto, é importante esclarecermos alguns conceitos para que não se tornem confusos. Existe outra operação matemática extremamente parecida com a convolução chamada correlação, sendo que ela também realiza a soma de multiplicações com a diferença de que ela não rotaciona o kernel. Para entendermos bem essa diferença, podemos observar a figura 5.3, onde temos um exemplo de correlação e convolução sendo executados em um espaço unidimensional. Temos uma função \\(f\\) e um filtro \\(w\\) na figura 5.3 (a) e (b), na sequência, de (b) e (j), temos as funções e os filtros prontos para se realizar a correlação e a convolução. Nas etapas (c) e (k), podemos ver o preenchimento com zeros, isso ocorre porque há partes das funções que não se sobrepõem, dessa forma, permite que \\(w\\) percorra todos os pixels de \\(f\\). Após isso, é realizado o primeiro passo da correlação e convolução, onde podemos observar que o resultado é 0 já que \\(w\\) está sobreposto por somente zeros, logo a soma da multiplicação de cada item de \\(w\\) por \\(f\\) será nulo. Deslocamos então o filtro \\(w\\) em uma unidade a direita, onde o resultado novamente será 0, sendo que o primeiro resultado não nulo se dará no terceiro deslocamento, sendo 8 para a correlação e \\(1\\) para a convolução. Temos o resultado de ambas operações em (g) e (o) e o resultado recortado em (h) e (p), recorte este que remove os zeros até o tamanho ficar igual ao da \\(f\\) inicial. Figura 5.3: Ilustração de correlação e convolução unidimensional [2, p. 96] Vamos extender agora essas duas operações à aplicação em duas dimensões. Uma representação disso pode ser vista na figura 5.4, onde temos novamente o kernel \\(w\\) e a função \\(f\\). Percebe-se outra vez o efeito de se aplicar o kernel em uma imagem com apenas o número 1 no meio, nos dois casos temos como saída a cópia do kernel, com a diferença que na correlação ele sai rotacionado. Assim, nota-se que se pré-rotacionarmos o filtro e realizarmos a correlação teremos no final uma convolução. Já que a correlação e convolução são iguais, quais delas devo utilizar? Segundo Gonzalez, [2, p. 98], isso é uma questão de preferência e qualquer uma das duas operações conseguem realizar a outra com uma simples rotação do kernel. Essa questão se torna ainda menos relevante quando utilizamos filtros que são simétricos, pois, como antes e após a rotação temos o mesmo kernel, tanto correlação ou convolução nos darão o mesmo resultado; já em kernels assimétricos, temos resultados diferentes. Ainda, segundo Moeslund, [7, p. 87], quando trabalhamos com filtros de desfoque, detectores de borda, entre outros, o processo de se aplicar o kernel é comumente chamado de convolução mesmo quando na prática se é implementada a correlação. Figura 5.4: Ilustração de correlação e convolução bidimensional [2, p. 98] 5.1.1 Definção matemática de convolução Vamos explorar um pouco das notações matemáticas utilizadas para representar a convolução e a correlação, assim também poderemos consolidar a idéia de que ambas são muito correlacionadas. Como dito no início desta seção, geralmente a convolução é identificada por \\(*\\), já a correlação costuma ser identificada por ☆. A correlação em duas dimensões segue a seguinte equação (Equação X): \\(g(x,y) = w(x,y)☆f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x+s,y+t)\\) Onde \\(w\\) é nosso \\(kernel\\) e \\(f\\) nossa imagem, podemos perceber que ambas são funções de duas variáveis, \\(x\\) e \\(y\\), pois estamos trabalhando em duas dimensões. Os limites dos somatórios são dados por \\(a=(m-1)/2\\) e \\(b=(n-1)/2\\). E o que essa função faz é andar em cada posição da imagem, ou seja, \\((x,y)\\), e substituir o píxel atual pela soma de produtos da multiplicação dos valores do \\(kernel\\) pelos valores dos píxels da imagem. Já a convolução tem uma equação bem similar, sendo diferente apenas pelos sinais negativos em \\(f\\), o que evidência a rotação do \\(kernel\\). Podemos notar que os sinais inversos estão em \\(f\\) e não em \\(w\\); segundo [2, p. 98], isso é usado para fins de simplicidade de notação e não alteram o resultado. \\(g(x,y) = w(x,y)*f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x-s,y-t)\\) Uma das melhores maneiras de entender bem as equações é ver um exemplo prático. Veremos isso a seguir, onde temos um exemplo passo-a-passo de correlação: \\[ \\text{w}\\text{*f}\\left(0,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(-1,-1\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot0+0\\cdot2+\\left(-2\\right)\\cdot1\\\\ +1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ =0\\,-2-3\\,=\\,-5 \\] \\[ \\text{w}\\text{*f}\\left(0,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,2\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot2+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ =0\\,+4+8=\\,12 \\] \\[ \\text{w}\\text{*f}\\left(0,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,3\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot1+0\\cdot0+\\left(-2\\right)\\cdot0\\\\ +1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ =0\\,+2+3=\\,5 \\] \\[ \\text{w}\\text{*f}\\left(1,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,1\\right)\\\\ =\\,1\\cdot0+0\\cdot2+\\left(-1\\right)\\cdot1\\\\ +2\\cdot0+0\\cdot9+\\left(-2\\right)\\cdot3\\\\ +1\\cdot0+0\\cdot5+\\left(-1\\right)\\cdot4\\\\ =\\left(-1\\right)\\,+\\left(-6\\right)+\\left(-4\\right)=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(1,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,2\\right)\\\\ =\\,1\\cdot2+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot9+0\\cdot3+\\left(-2\\right)\\cdot1\\\\ +1\\cdot5+0\\cdot4+\\left(-1\\right)\\cdot2\\\\ =2\\,+16+3=\\,21 \\] \\[ \\text{w}\\text{*f}\\left(1,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,3\\right)\\\\ =\\,1\\cdot1+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot3+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot4+0\\cdot2+\\left(-1\\right)\\cdot0\\\\ =1\\,+6+4=\\,11 \\] \\[ \\text{w}\\text{*f}\\left(2,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,0+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,1\\right)\\\\ =\\,1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ +2\\cdot0+0\\cdot5+\\left(-2\\right)\\cdot4\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =\\left(-3\\right)+\\left(-8\\right)+0=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(2,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,2\\right)\\\\ =\\,1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ +2\\cdot5+0\\cdot4+\\left(-2\\right)\\cdot2\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =8+6+0=\\,14\\\\ \\] \\[ \\text{w}\\text{f}\\left(2,2\\right)\\text{=}\\sum{s}^{}\\sum{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,2+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,3\\right)\\\\ =,1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot4+0\\cdot2+\\left(-2\\right)\\cdot0\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =3+8+0=11 \\] Fonte: Autoria própria inspirado em http://www.songho.ca/dsp/convolution/convolution2d_example.html e https://arxiv.org/abs/1603.07285 O exemplo utiliza um \\(kernel\\) assimétrico, então, como dito anteriormente, os resultados da correlação e convolução são diferentes. Deixaremos isso como um exercício ao leitor, realizar a convolução para provar a diferença. O caso anterior tem também uma característica que é o complemento das bordas com 0’s para que o \\(kernel\\) pudesse ser aplicado sobre toda a imagem. Essa é uma das maneiras de lidar com o problema das bordas, sendo que temos essa e mais algumas possibilidades [8, p. 125] [15, p. 60]: Preenchimento com constante (constant padding): Método onde as laterais são preenchidas com um valor constante, comumente 0. Preenchimento com vizinho (nearest neighbor): Se realiza o preenchimento das bordas adicionais com os valores dos vizinhos mais próximos. Reflexão (reflect): Os pixels das bordas da imagem são repetidos nas bordas adicionais. Repetição (ou wrap): A imagem é repetida nas bordas. Além dos métodos apresentados existem outras técnicas que podem ser utilizadas, sendo que em cada caso se escolhe a que melhor se ajusta à tarefa realizada. O último ponto a ser explorado é a escolha do tamanho ideal do preenchimento que deve ser aplicado à imagem. De forma geral, se temos uma imagem (\\(f\\)), um kernel (\\(w\\)) e um preenchimento (\\(p\\)), podemos escrever a seguinte relação1: \\[saída = (f_{vertical} - w_{vertical} + p_{vertical} + 1) \\times (f_{horizontal} - w_{horizontal} + p_{horizontal} + 1)\\] Isso nos leva a perceber que o tamanho da imagem de saída aumenta conforme o preenchimento. Na maioria dos casos, queremos que a imagem de entrada e saída tenha o mesmo tamanho, então usamos \\(p_{vertical} = k_{vertical} - 1\\) e \\(p_{horizontal} = k_{horizontal} - 1\\). E como, geralmente, o kernel tem tamanho ímpar, utilizamos \\(\\frac{p_{vertical}}{2}\\) e \\(\\frac{p_{horizontal}}{2}\\). 5.2 Média Um filtro de média é um tipo de filtro que utiliza a média dos valores dos píxels próximos ao píxel central. Como esse tipo de filtro realiza uma operação linear, ele é classificado como um filtro linear de suavização. Essa suavização se dá exatamente pelo tipo da operação utilizada, a média dos píxels vizinhos, que diminui a nitidez pela redução das transições abruptas nos níveis de intensidade. Um dos problemas que podem ocorrer é que as bordas também são mudanças abruptas, então podem ser comprometidas pelo filtro. Na figura 5.5, temos um exemplo do efeito desse filtro: Figura 5.5: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] A máscara (kernel) de um filtro de média pode ser representada por: \\[\\begin{bmatrix} \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9} \\end{bmatrix} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\] A primeira forma é a soma de todos os valores divididos por 9, o qual é o tamanho do filtro. A segunda forma, onde todos os coeficientes do kernel são 1’s, é mais eficiente computacionalmente, pois realizamos todas as somas e multiplicações antes de dividirmos. Esse tipo de filtro é muitas vezes chamado de box filter. Figura 5.6: Imagem com diferentes tamanhos de filtro de média [2, p. 102] Na figura 5.6, temos um exemplo onde foram aplicados filtros de média com diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Podemos notar que com o menor filtro, o de tamanho 3, temos um leve borramento na imagem toda, mas que as partes que tem o mesmo tamanho ou são menores que o filtro tem um borramento maior. Isso exemplifica umas das importantes aplicações dos filtros de suavização, que é a de desfocar os objetos menores e deixar os maiores em maior evidência. Figura 5.7: Exemplo de uso do desfoque [2, p. 103] Na figura 5.7, podemos ver como o desfoque pode ser utilizado para encontrar os detalhes principais da imagem. Nesta imagem obtida a partir do telescópio Hubble foi aplicado o desfoque para diminuir a visibilidade dos objetos menores e dar maior ênfase aos da frente. E, então, foi limiarizado o resultado a fim de destacar esses objetos. 5.3 Mediana A mediana é um filtro não linear que utiliza como princípio a própria técnica estatística, que consiste em ordenar um conjunto de dados em ordem e selecionar o valor central. Esse tipo de filtro é muito eficiente na remoção de ruído, principalmente o tipo de ruído conhecido como ruído sal e pimenta, que tem uma característica aparência de pixels pretos e brancos. Como dito anteriormente que a mediana consiste em ordenar o conjunto de dados, nesse caso os valores da vizinhança do píxel. Se usarmos uma vizinhança de tamanho 3x3, a mediana será o quinto maior valor, pois consideramos também o valor do píxel central. Na figura 5.8, tem-se um exemplo mostrando uma imagem de raios X de uma placa de circuitos com ruído sal e pimenta (a). As respectivas aplicações de um filtro de média (b) e do filtro da mediana (c), ambos com dimensões 3x3. A partir dela, é visto o quanto o filtro da mediana se sai melhor na remoção do ruído. Figura 5.8: Remoção de ruído [2, p. 103] 5.4 Gaussiano Filtro de Gauss é um filtro simétrico (isotrópico) usado para suavizar imagens. Ele funciona análogo a um filtro de média ponderada, mas com um padrão, isto é, dá ênfase ao pixel central da máscara (kernel) e menor ênfase a medida que os demais se distanciam dele, seguindo um gradiente muito similar à distribuição gaussiana. O “similar” se deve somente ao fato que será necessário truncamentos, assim, havendo erros e por ser desconsiderado a constante multiplicadora, o que melhora o perfil da função para aplicação no PDI. Função gaussiana: \\[g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Função gaussiana modificada: \\[g(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Note que a função tem formato de sino, que varia de acordo com os valores atribuídos ao, \\(\\sigma\\), o qual dependerá da ênfase desejada no cálculo. A medida que \\(\\sigma\\) aumenta, maior é o peso dado às “caldas”, no caso, aos pixels mais distantes do pixel central. Característica representada no gráfico 5.9. A fim de que o pixel central da máscara esteja em \\((0,0)\\), considera-se \\(\\mu=0\\). Figura 5.9: Função gaussiana com \\(\\mu=0\\) e diferentes \\(\\sigma\\) O filtro gaussiano 2D é gerado através da convolução de dois vetores da mesma função - a imagem se refere ao subconjunto do contradomínio -, sendo estas, normalmente, oriundas da mesma função gaussiana modificada. De forma alternativa, ele também pode ser construído a partir da i-ésima linha do Triângulo de Pascal que corresponde ao tamanho do kernel desejado. Como o método é o mesmo para as duas fontes, explicaremos ele utilizando como fonte a função gaussiana modificada. O procedimento primário é obter a imagem da função, mas para obter um filtro com intuito gaussiano se deve selecionar a amostragem dos pontos no intervalo de \\(\\pm 2.5\\sigma\\) ou \\(\\pm 3.5\\sigma\\) [8, p. 114]. Assim, optando-se pela primeira opção, procura-se \\(g(-3)\\), \\(g(-2)\\), \\(g(-1)\\), \\(g(0)\\), \\(g(1)\\), \\(g(2)\\) e \\(g(3)\\). Entretanto, como alguns valores têm infinitas casas decimais, devemos truncá-los, no qual foi preferido com cinco casas decimais. \\(g(-3) = g(3) = 0,01110\\) \\(g(-2) = g(2) = 0,13533\\) \\(g(-1) = g(1) = 0,60653\\) \\(g(0) = 1\\) Então, considera-se o menor valor como 1, no caso \\(f(3)\\) e \\(f(-3)\\), e interpolamos os demais, truncando-os com propósito de obter a parte inteira. A divisão por 224 é oriunda da soma dos pesos da média. Pronto, temos a máscara gaussiana 1D necessária: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) Com a máscara em mãos, convoluciona-se os seguintes fatores: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) \\(\\frac{1}{224}\\begin{bmatrix}1 \\\\ 12 \\\\ 54 \\\\ 90 \\\\ 54 \\\\ 12 \\\\ 1\\end{bmatrix}\\) é a transposta da máscara 1D \\(f(x,y)\\) é a imagem de entrada Nessa propriedade há uma determinada ordem que é mais eficiente para o sistema digital, que é a convolução entre \\(f(x,y)\\) e uma das máscaras 1D e, depois, a convolução desse resultado com a transposta desta máscara. Pronto, como saída teremos o filtro gaussiano aplicado a imagem. Figura 5.10: Imagem com diferentes tamanhos de filtro gaussiano. (Adaptado:[2, p. 102]) Nesse exemplo, temos um exemplo similar ao usado no filtro de média, porém, usando-se o filtro gaussiano. A imagem de entrada (a) tem tamanho 500x500 onde foram aplicados filtros de diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Note que, ao contrário do exemplo anterior, há uma maior preservação da nítidez, todavia não seria um filtro eficiente para redução de ruído. Essa característica é devido ao maior peso dado ao pixel central do kernel. Refêrencias "],["segmentação.html", "Capítulo 6 Segmentação 6.1 Detecção por descontinuidade 6.2 Detecção de bordas 6.3 Transformada de Hough 6.4 Detecção de blobs 6.5 Detecção de junções ou cantos 6.6 Segmentação por limiarização 6.7 método de Otsu 6.8 Segmentação usando watersheds morfológicas", " Capítulo 6 Segmentação 6.1 Detecção por descontinuidade 6.1.1 Detecção de ponto 6.1.2 Detecção de linha 6.2 Detecção de bordas 6.2.1 Modelos de Bordas 6.2.2 Método do gradiente ( Roberts, Prewitt, Sobel). 6.2.3 Método Marr-Hildreth 6.2.4 Método Canny 6.3 Transformada de Hough 6.3.1 Transformada de Hough para detecção de linhas 6.3.2 Transformada de Hough para detecção de círculos 6.4 Detecção de blobs 6.4.1 LoG 6.4.2 DoG 6.4.3 DoH 6.5 Detecção de junções ou cantos 6.6 Segmentação por limiarização 6.7 método de Otsu 6.8 Segmentação usando watersheds morfológicas "],["refêrencias.html", "Refêrencias", " Refêrencias "]]
