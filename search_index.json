[["index.html", "Material introdutório de Processamento Digital de Imagens e Visão Computacional Início", " Material introdutório de Processamento Digital de Imagens e Visão Computacional Início Esse site foi desenvolvido pelo grupo COVAP (Computação Visual Aplicada) que visa oferecer um material introdutório da disciplina de PDI (Processamento Digital de Imagens) e VC (Visão Computacional) para alunos e entusiastas no assunto. Os conteúdos se encontram organizados na seguinte estrutura: Introdução Formação da imagem Transformações geométricas Transformações radiométricas Filtros Segmentação Lembramos ainda que todo o material está hospedado no Github, agradeçemos então todo apontamento de erros e sugestões, para que assim consigamos sempre disponibilizar um conteudo revisado e livre de quaisquer erros. "],["intro.html", "Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica 1.2 Aplicações Processamento Digital de Imagens 1.3 Etapas do Processamento e Análise de Imagens", " Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica A visão desempenha um papel importante na vida das pessoas, pois com ela é possível uma percepção incrivelmente rica do mundo ao seu redor. Para tentar reproduzir as capacidades visuais humanas por sistemas autônomos manipulados por computadores foram desenvolvidas pelo menos três grandes áreas [1, p. 2]: Processamento Digital de Imagens (PDI), Visão Computacional (VC) e a Computação Gráfica(CG), apresentados na Figura 1.1. Essas áreas, apesar de serem correlacionadas, têm objetivos e métodos diferentes, por isso a importância de distingui-las. Figura 1.1: Processos Computacionais com Imagens [1, p. 2]. O Processamento Digital de Imagens (PDI) busca realizar o pré-processamento das imagens, utilizando para isso técnicas de tratamento, como a correção da iluminação, eliminação de ruído, e a segmentação. O foco da Visão Computacional (VC) é a análise das imagens, identificando os seus componentes e obtendo informações. Diferente da VC, em que as imagens são os objetos de estudo, na Computação Gráfica (CG), as imagens são o resultado do processo. Na CG são geradas representações visuais seguindo descrições e especificações geométricas [1, p. 3]. A Tabela 1.1 apresenta de forma resumida as diferenças entre PDI, VC e CG. Na segunda linha da tabela está uma descrição simples de cada área, e na terceira linha um esquema identificando o objeto e o produto de cada processo. Tabela 1.1: Processos Computacionais com Imagens. Computação Gráfica (CG) Visão Computacional (VC) Processamento Digital de Imagens (PDI) Cria e altera imagens a partir de dados. Análise de imagem para criação de modelos. Transformação de imagem (tratamento). modelo → imagem imagem → modelo imagem → imagem As imagens tratadas em PDI têm como uma das finalidades servir de material para a Visão Computacional, como identificado na Figura 1.1. Muitas vezes as áreas de Visão Computacional e PDI são confundidas devido a dificuldade em se definir em que ponto uma termina e a outra começa. Mesmo não existindo uma linha clara entre os limites destas duas áreas é possível utilizar um paradigma que considera três níveis de processamento [2, p. 2]: Baixo nível A nível de pixel, realiza operações de pré-processamento, sendo utilizada, por exemplo, na redução de ruído, aumento de contraste e restauração. Médio nível Operações mais complexas, como segmentação, partição e reconhecimento de objetos individuais. Entrada é uma imagem mas a saída pode ser um conjunto contendo os atributos extraídos das imagens. Alto nível Interpretação do conteúdo da imagem e análise. Baseado nesses níveis, iremos considerar que o processamento de imagem atua nos primeiros dois níveis, ou seja, envolve o pré-processamento e processos de extração de elementos de imagens até o reconhecimento de componentes individuais. Como o foco deste material é o Processamento de Imagens Digitais (PDI), estes dois níveis serão apresentados com detalhes nos próximos tópicos, mas primeiro vejamos alguns exemplos de aplicações do PDI. 1.2 Aplicações Processamento Digital de Imagens As primeiras tarefas de processamento de imagens tiveram aplicações significativas por volta da década de 1960, quando se desenvolveram computadores com potencial suficiente para realizá-las. O programa espacial americano também foi um forte impulso para o contínuo desenvolvimento e aprimoramento das técnicas de processamento digital de imagem (PDI), já que imagens, como as obtidas da Lua através de sondas e transmitidas à terra, continham distorções provenientes das câmeras utilizadas. Era necessário então, a utilização de métodos para corrigir essas alterações [2, p. 4]. Outra área que também faz uso extensivo do processamento de imagens e impulsionou seu desenvolvimento é a área médica. Nessa área, o uso de imagens auxiliou no diagnóstico de doenças através de exames visuais como os de raio-x [2, p. 4]. A utilização do processamento de imagens para melhorar informações visuais, ajudando na interpretação humana, expandiu-se para diferentes setores. No sensoriamento remoto, o pré-processamento contribui para uma melhor análise de imagens aéreas e de satélite, aumentando a compreensão da superfície terrestre. Na arqueologia e nas artes, métodos de processamento de imagens podem restaurar fotografias com registros únicos de objetos raros, pinturas, documentos antigos e conteúdos em vídeos [3, p. 2]. Na física e em áreas da biologia, técnicas computacionais realçam imagens de experimentos em áreas como plasmas de alta energia e microscopia eletrônica [2, p. 5]. Com o aumento da automatização de tarefas, o processamento de imagens tem se destacado na aquisição de dados de imagens visando a percepção automática por máquinas [3, p. 3]. Técnicas de identificação de padrões podem ser aplicados no reconhecimento automático de caracteres, de impressões digitais, de faces, e de placas de veículos, contribuindo com setores de segurança. Na automação industrial tem sido utilizado no sistema de visão computacional para inspeção e montagem de produtos. Na área militar, pode ser aplicado na identificação e rastreamento de alvos em imagens de satélites, e na navegação de veículos autônomos. Nas áreas de medicina e biologia, rastreamentos automáticos em imagens radiográficas e amostras de sangue têm contribuído para os exames e testes [3, p. 3]. O processamento computacional de imagens aéreas e de satélites também é utilizado na previsão do tempo e em avaliações ambientais [2, p. 5]. Este variado campo de aplicações pode ser justificado pela capacidade dos aparelhos de processamento de imagens trabalharem com imagens de diversas fontes. Diferentemente dos seres humanos, que são limitados à banda visual do espectro eletromagnético (EM), o processamento computacional cobre todo o EM, variando de ondas gama a ondas de rádio [2, p. 1]. No processamento digital ainda é possível trabalhar com imagens geradas por fontes que os humanos não estão acostumados a associar com imagens. Essas fontes incluem acústica, ultrassom, microscopia eletrônica e imagens geradas por computador [2, p. 13]. Uma das formas mais fáceis de desenvolver uma compreensão básica da extensão das aplicações do processamento de imagens é categorizar as imagens de acordo com sua fonte. Na Figura 1.2 temos uma representação do EM, iremos a seguir explorar cada uma dessa faixas, apresentando algumas das áreas onde podem ser utilizados: Figura 1.2: Espectro eletromagnético [4]. Imagens formadas por raios gama As imagens formadas a partir de raios gama têm diferentes utilidades, sendo muito utilizadas na medicina e astronomia [2, p. 6]. Na medicina, existem procedimentos onde se injetam isótopos radioativos no paciente e por meio dos detectores de raio gama é formada uma imagem, como exemplo, escaneamento ósseo e tomografia por emissão de pósitrons (PET-scan). Na astronomia, ela pode ser utilizada para se conseguir ver detalhes astronômicos que estão presentes na faixa eletromagnética dos raios gama. Imagens formadas por raios X Imagens formadas a partir de raio X têm uma ampla gama de aplicações, desde seu uso na medicina até seu uso no meio industrial [2, p. 6]. Na indústria, pode ser utilizado para se encontrar defeitos de fabricação em produtos, e na medicina, vêm se utilizando muito o processamento de imagem e a visão computacional para ajudar no diagnóstico de doenças, como por exemplo, artérias obstruídas, fraturas e tumores. Imagens na banda ultravioleta O espectro ultravioleta também tem inúmeras aplicações, como a inspeção industrial, microscopia, imagens biológicas e observações astronômicas [2, p. 8]. Imagens na banda visível e infravermelho Essas duas bandas possuem uma gama extremamente ampla de aplicações, sendo utilizadas juntas ou separadas. Na banda visível, existem diversas aplicações, como em processos industriais, detecção de faces, detecção de placas de carros, etc [2, p. 11]. A banda infravermelho também possui inúmeras aplicações, sendo uma delas imagens a partir de satélites, onde o infravermelho nos permite ver inúmeros detalhes que somente com a banda visível não seria possível [2, p. 9]. Imagens na banda de micro-ondas e rádio Na banda de micro-ondas o melhor exemplo que temos é o radar. Essa banda tem uma peculiaridade de ser extremamente penetrante, podendo gerar imagens através de nuvens, vegetação, etc [2, p. 12]. Já a banda de rádio é muito utilizada na medicina, como exemplo na ressonância magnética e na astronomia [2, p. 12]. Como podemos observar, existem inúmeras maneiras de se conseguir imagens além da clássica imagem no espectro visível, isso nos dá a possibilidade de utilizar o PDI em inúmeras áreas e problemas. Na Figura 1.3 temos uma nebulosa observada a partir de diferentes bandas dos EM, sendo possível observar detalhes que passariam despercebidos se usássemos somente alguma delas. Figura 1.3: Nebulosa CRAB em diferentes frequências [5]. 1.3 Etapas do Processamento e Análise de Imagens Um dos objetivos deste material é servir de referência para o estudo inicial de Visão computacional, assim, para compreender as relações entre as etapas de processamento e análise de imagens apresentamos na Figura 1.4 uma sequência dos principais passos utilizados em uma aplicação de PDI. Neste material nos deteremos nos conteúdos de processamento, desde a aquisição de imagens, pré-processamento até segmentação. Figura 1.4: Etapas de aplicação de PDI [3, p. 4]. Vale ressaltar que essas etapas não são fixas e podem ser modificadas, sendo que uma base de conhecimentos é importante para orientar em uma aplicação específica [3, p. 4]. Aquisição da imagem Captura a imagem por meio de um dispositivo ou sensor e a converte em uma imagem digitalizada [3, p. 3]. Podemos citar como exemplo as câmeras fotográficas, tomógrafos médicos, satélites e scanners. Na Figura 1.5, temos um exemplo de aquisição de imagens do satélite Landsat, neste caso estão identificadas as bandas vermelha, verde e azul visíveis e o infravermelho próximo. Os detalhes sobre a aquisição de imagens serão discutidos no tópico Formação de Imagem. Figura 1.5: Aquisição de imagens de satélite.(a) a (d) mostram quatro imagens espectrais de satélite da cidade de Washington, D.C., banda vermelha, verde e azul visíveis e o infravermelho próximo, respectivamente. (e) Imagem colorida como combinação RGB de (a), (b) e (c). (f) Imagem colorida obtida pela combinação de (b), (c) e (d)[2, p. 279]. Pré processamento Essa etapa busca realizar mudanças e ajustes na imagem visando melhorar seu uso nas etapas futuras [3, p. 3]. Como exemplo temos casos onde não precisamos das cores de uma imagem, podendo então realizar a conversão para grayscale(tons de cinza), ou precisamos gerar imagens coloridas como na Figura 1.5 em que são combinadas as bandas espectrais. Além disso, podemos realizar cortes ou realces, isolando somente a parte de maior interesse na imagem como na Figura 1.6, ou também atenuar o ruído na imagem, além de outras técnicas que serão abordadas em outros tópicos. Figura 1.6: Subtração de imagens para realce de diferenças.(a) Imagem da área de Washington D.C em infravermelho. (b) Resultado ao zerar o bit menos significativo de todos os pixels de (a). (c) Diferença entre as duas imagens ajustada para a faixa [0, 255], sendo que valores em preto (0) indicam pontos nos quais não há nenhuma diferença [2, p. 49]. Segmentação Nessa etapa as informações de interesse são extraídas da imagem, geralmente, pela detecção de descontinuidades (bordas) ou de similaridades na imagem [3, p. 4]. Na Figura 1.7 é mostrado o resultado de um exemplo de segmentação por similaridade, em que o elemento de maior interesse é o rio. Figura 1.7: Extração de características de uma imagem segmentada.(a) Imagem na banda infravermelha da área de Washington, D.C. (b) Segmentação da imagem por limiarização. (c) O maior componente conexo de (b). Técnica de representação por esqueleto de (c) [2, p. 544]. Representação e Descrição Armazenar e manipular objetos de interesse extraídos da imagem. O processo de descrição visa a extração de características para discriminar classes de objetos [3, p. 4]. Na Figura 1.8, o objetivo é determinar o tamanho das ramificações do rio, para isto considerou-se que o tamanho de cada ramificação no esqueleto seria uma boa aproximação [2, p. 545]. O esqueleto é uma representação do rio, e seus elementos são discriminados dentro do maior componente conexo da imagem segmentada. Reconhecimento e Interpretação Essa etapa examina as informações produzidas na etapa anterior e classifica cada objeto como sendo de interesse ou não, atribuindo significado ao conjunto de objetos reconhecidos pelos rótulos [3, p. 3]. Uma aplicação de análise de imagens inclui a classificação de áreas em uma imagem multiespectral como na Figura 1.8. Neste exemplo utilizou-se o método bayesiano, em que cada pixel da imagem foi avaliado em relação a três classes (água, desenvolvimento urbano e vegetação). Nas Figuras 1.8, pontos pretos representam pontos classificados incorretamente, enquanto pontos brancos foram classificados corretamente [2, p. 579]. Figura 1.8: Classificação bayesiana em uma imagem multiespectral. Resultado (em branco) da classificação na classe água, desenvolvimento urbano e vegetação, da esquerda para a direita [2, p. 579]. Refêrencias "],["formação-da-imagem.html", "Capítulo 2 Formação da imagem 2.1 Câmera pinhole e geometria 2.2 Lentes 2.3 Sensor 2.4 Amostragem e Quantização 2.5 Definição de imagem digital 2.6 Resolução espacial e de intensidade 2.7 Pixels", " Capítulo 2 Formação da imagem Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é do espectro eletromagnético, mas podendo ser também, a partir da energia mecânica (ultrassom), feixe de elétrons, etc.. Cada fonte necessita de um método específico de captura, para algumas pode ser uma câmera fotográfica, porém a outras é necessário que o computador sintetize a imagem, como o microscópio eletrônico. Como já mencionado no tópico de introdução, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensitivos a essa faixa de luzes, que vêm da luz solar e nos ajuda a realizar nossas atividades. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas, como a ultravioleta[6, p. 2]. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas[7, p. 8]. A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de 0,43 a 0,79 \\(\\mu m\\). Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar[2, p. 28]. Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir. 2.1 Câmera pinhole e geometria Na figura 2.1 temos um esquema básico de como geralmente ocorre a aquisição de imagens, primeiramente a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida, sendo após isso capturada por um dispositivo, como uma câmera. Figura 2.1: Representação de uma típica captura de imagem [7, p. 8]. Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, este é conhecido como câmara pinhole(do inglês buraco de alfinete) ou câmara escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na figura 2.2, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruim. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores. Figura 2.2: Introdução de barreira para captura de imagem [7, p. 11]. Na figura 2.2 percebemos que a imagem resultante acaba invertida, isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir. Figura 2.3: Geometria de uma câmera pinhole [8, p. 5]. Na figura 2.3, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância \\(Z\\) da abertura e a uma distância \\(Y\\) o eixo óptico, podemos definir a altura \\(y\\) e a largura \\(x\\) da projeção do objeto utilizando a simetria de triângulos: \\[-\\frac{y}{f}=\\frac{Y}{Z}\\Leftrightarrow y=-f\\frac{Y}{Z} \\text{ e } -\\frac{x}{f}=\\frac{x}{f} \\Leftrightarrow x=-f\\frac{X}{Z}\\] A variável \\(f\\) nessa equação se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera. Os sinais negativos das equações significam que a imagem projetada está rotacionada a 180º verticalmente e horizontalmente, como podemos confirmar na imagem acima. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens, como precisar de um longo tempo de exposição para captura da imagem. 2.2 Lentes As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso. Figura 2.4: Ação de uma lente sobre os raios de luz [7, p. 12]. Como podemos ver na figura 2.4, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando-se o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada. O ponto \\(F\\) onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância \\(f\\), que vai do centro óptico \\(O\\) até \\(F\\) é conhecida como Distância Focal. Definindo a distância do objeto real até a lente como g e a distância até a formação da imagem após passa pela lente como b temos que: \\[\\frac{1}{g}+\\frac{1}{b}=\\frac{1}{f}\\] Como \\(f\\) e \\(b\\) estão normalmente entre 1 mm e 100mm isso mostra que \\(\\frac{1}{g}\\) não tem quase nenhum impacto na equação e significa que \\(b = f\\). Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal. Outro ponto importante das lentes é conhecido como zoom óptico. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, \\(B\\), aumenta quando \\(f\\) aumenta. Podemos representar isso na seguinte equação, onde \\(g\\) é o tamanho real do objeto: \\[\\frac{b}{B}=\\frac{g}{G}\\] Na prática \\(f\\) é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos. Se o \\(f\\) for constante, quando alteramos a distância do objeto, no caso \\(g\\), sabemos que \\(b\\) também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos \\(b\\) temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando \\(b\\) para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco. Figura 2.5: Uma imagem focada e desfocada [7, p. 11]. A figura 2.5 ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos. Figura 2.6: Profundidade de campo [7, p. 13]. A figura 2.6 apresenta outro ponto muito importante, chamado Profundidade de Campo(Depth of field), que representa a soma das distâncias \\(g_l\\) e \\(g_r\\), que representam o quando os objetos podem ser movidos e permanecerem em foco. Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão(Field of View ou FOV) que representa a área observável de uma câmera. Na figura 2.7 essa área observável é denotada pelo ângulo \\(V\\). O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as equações seguintes para o FOV vertical e horizontal: \\[FOV_x = 2*\\tan^{-1}\\left(\\frac{\\frac{comprimento\\ do\\ sensor}{2}}{f}\\right) \\text{ e } FOV_y = 2*\\tan^{-1}\\left(\\frac{\\frac{altura\\ do\\ sensor}{2}}{f}\\right)\\] Figura 2.7: Campo de visão [7, p. 14]. Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de 14mm, altura de 10mm e uma distância focal de 5mm temos: \\[FOV_x=2*tan^{-1}\\left(\\frac{7}{5}\\right)=108.0^{\\circ} \\text{ e } FOV_y=2*tan^{-1}(1)=90^{\\circ}\\] Isso significa que essa câmera tem uma área observal de 108.9º horizontalmente e 90º verticalmente. Na figura 2.8 temos o mesmo objeto fotografado com diferentes profundidades de campo: Figura 2.8: Diferentes profundidades de campo [7, p. 15]. Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris no olho humano, ela controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para capturar a imagem. 2.3 Sensor Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD, que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS, usado em casos mais gerais, como câmeras de celulares. Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na figura 2.9, conhecido como PDA(Photodiode Array): Figura 2.9: Sensor(area matricial de celulas), Single Cell(uma única celula sensora) [7, p. 17]. Como podemos ver, o sensor consiste em várias pequenas células, cada uma um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na figura 2.10 podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta(correctly exposed), logo em seguida temos uma que sofreu de superexposição(overexposed) e na terceira temos uma com subexposição(under exposed). Por último temos uma imagem que sobre com o movimento do objeto que estava sendo capturado, oque ocasionou o borramento(motion blur). Figura 2.10: Diferentes níveis de exposição [7, p. 17]. Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas em imagens coloridas, como são capturadas? Imagens coloridas utilizam, especialmente, o formato RGB, que significa Red-Green-Blue, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na figura 2.11 podemos ver uma imagem com seus componentes separados: Figura 2.11: Imagem colorida separada em seus três componentes [7, p. 28]. Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na figura 2.12. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo. Figura 2.12: Captura de imagem com três sensores [9, p. 242]. Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel, isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na figura 2.13: Figura 2.13: Filtro Bayer [7, p. 29]. Podemos perceber que ocorre uma maior ocorrência das cores verdes, isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase a sua captura. Na figura 14 temos uma esquematização de como cada pixel recebe informação de somente uma cor, por meio da filtragem, onde a luz que entra(Incoming light) é filtrada e somente a cor de interesse consegue passar, após isso ela chega a malha de sensores(sensor array): Figura 2.14: Sensores com padrão Bayer [10]. Vemos na figura 2.14 que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos. 2.4 Amostragem e Quantização Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto. 2.4.1 Amostragem Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura 2.15, na qual a figura 2.15 (a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto. A figura 2.15 (b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha AB. Nas extremidades do gráfico na figura 2.15 (b), a intensidade é mais alta devido a parte branca da imagem, já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na figura 2.15 (c). Esse procedimento de amostragem, na prática, é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo teríamos que ter um número infinito de pixels, como isso não é possível recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem [2]. Figura 2.15: Filtro Bayer [2, p. 34]. 2.4.2 Quantização Na figura 2.15 (c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na figura 2.15 (d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na figura 2.15 (c). Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital[8, p. 8]. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo[11, p. 9]. No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos \\(\\{0,\\dots, L-1\\}\\), onde \\(L\\) é uma potência de dois, ou seja, \\(L = 2_k\\) [11, p. 10]. Isso significa que L é o número de tons de cinza que podem ser representados com uma quantidade k de bits. Em muitas situações é utilizado \\(k = 8\\), ou seja, temos 256 níveis de cinza. Ao realizar a quantização e a amostragem linha por linha no objeto da figura 2.16 (a) é produzida uma imagem digital bidimensional como na figura 2.16. Figura 2.16: Filtro Bayer [2, p. 35]. 2.5 Definição de imagem digital Uma imagem pode ser definida como uma função bidimensional, \\(f(x, y)\\), em que \\(x\\) e \\(y\\) são coordenadas espaciais (plano), e a amplitude de \\(f\\) em qualquer par de coordenadas \\((x, y)\\) é chamada de intensidade ou nível de cinza da imagem nesse ponto [2]. Quando \\(x\\), \\(y\\) e os valores de intensidade de f são quantidades finitas e discretas, chamamos de imagem digital. A função \\(f(x, y)\\) pode ser representada na forma de uma matriz (M x N) como na Figura, em que as M linhas são identificadas pelas coordenadas em \\(x\\), e as N colunas em \\(y\\). Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou pel. O formato numérico da matriz, imagem 2.17, é apropriado para o desenvolvimento de algoritmos, particularmente quando se escreve a equação da matriz (M x N): \\[f(x,y) = \\begin{bmatrix} f(0,0) &amp; f(0,1) &amp; \\cdots &amp; f(0,N-1) \\\\ f(1,0) &amp; f(1,1) &amp; \\cdots &amp; f(1, N-1) \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ f(M-1,0) &amp; f(M-1, 1) &amp; \\cdots &amp; f(M-1, N-1) \\end{bmatrix}\\] Figura 2.17: Representações da imagem digital [2, p. 36]. Na figura 2.17 (a) temos a representação da imagem em 3D, onde a intensidade de cada pixel é representada no eixo z, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na figura 2.17 (b), formato que seria visualizado em um monitor ou uma fotografia [2]. Em cada ponto da figura 2.17 (a), o nível de cinza é proporcional ao valor da intensidade \\(f\\), assumindo valores 0, 0,5 ou 1. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco. Note que na Figura, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo x positivo direcionado para baixo e o eixo y positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez [2]. De acordo com o tamanho da matriz (M x N) e dos níveis discretos de tons de cinza (\\(L = 2^k\\)) que os pixels podem assumir é possível determinar o número, \\(b\\), de bits necessários para armazenar uma imagem digitalizada: \\[b = M × N × k\\] Quando uma imagem pode ter \\(2^k\\) níveis de intensidade, geralmente ela é denominada como uma “imagem de k bits”. Por exemplo, uma imagem com 256 níveis discretos de intensidade é chamada de uma imagem de 8 bits. A figura 2.18 mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (N x N) para diferentes valores de N e k. O número de níveis de intensidade (L) correspondente a cada valor de k é mostrado entre parênteses. Observa-se na figura 2.18 que uma imagem de 8 bits com dimensões 1.024 × 1.024 exigiria aproximadamente 1MB para armazenamento. Figura 2.18: Número de bits de armazenamento para vários valores de N e k [2, p. 38]. 2.6 Resolução espacial e de intensidade Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (M x N) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente dots per inch (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2.400 dpi [2]. A figura 2.19 mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A figura 2.19 (a) tem resolução 512 x 512, e a resolução das demais 2.19 (b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução. Figura 2.19: Efeitos da redução da resolução espacial [3, p. 20]. A resolução de intensidade ou profundidade corresponde ao número de bits (k) utilizados para estabelecer os níveis de cinza da imagem (\\(L=2^k\\)). Por exemplo, em uma imagem cuja intensidade é quantizada em L= 256 níveis, a profundidade é de k = 8 bits por pixel. Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura. A imagem (a) apresenta 256 níveis de cinza (k = 8). As imagens (b) e (c) foram geradas pela redução do número de bits k = 4 e k = 2, respectivamente, mas mantendo a mesma dimensão. Figura 2.20: Efeitos da redução de profundidade [7, p. 19]. 2.7 Pixels A topologia de uma imagem digitalizada, desempenha papel importante na especificação, localização e relação entre as coordenadas dos pixels da imagem, tornando mais fácil a manipulação da imagem. Uma imagem digitalizada contém as seguintes propriedades de seus pixels: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas. Para especificar, localizar, e relacionar topologicamente à imagem, consideramos: p,q denotando os pontos e, \\(p(x,y)\\), \\(q(x,y)\\), \\(u(x,y)\\) coordenadas dos pontos denotados, expressaremos \\(V\\) Conjunto de valores em uma imagem binária \\(V=\\{0,1\\}\\). 2.7.1 Vizinhança Vizinhança 4 - \\(\\left[N_4(p)\\right]\\) \\(N_4(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na horizontal outros dois na vertical suas coordenadas, ou seja, é o conjunto de pixels ao redor de p, sem considerar as diagonais [12, p. 15]. Exemplo na figura 2.21 (a). \\[p(x,y): p(x+1, y), p(x-1, y), p(x, y+1), p(x, y-1)\\] Vizinhança D - \\(\\left[N_D(p)\\right]\\) \\(N_D(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na diagonais superiores (direita, esquerda ) outras duas na diagonais inferiores (direita, esquerda) suas coordenadas, ou seja o conjunto de pixels ao redor de \\(p\\), considerando apenas as diagonais [12, p. 15].Exemplo na figura 2.21 (b). \\[p(x,y): p(x+1,y+1), p(x+1, y-1), p(x-1, y+1), p(x-1, y-1)\\] Vizinhança 8 - \\(\\left[N_8(p)\\right]\\) \\(N_8(p)\\) em \\(p(x,y)\\) possui 8 vizinhos, quatro \\(N_4(p)\\) e outros 4 \\(N_D(p)\\)suas coordenadas ou seja o conjunto de pixels ao redor de \\(p\\), considerando união das vizinhanças-4 e vizinhança-8 [12, p. 15]. Exemplo na figura 2.21 (c). \\[p(x,y): N_8(p) = N_4(p) \\cup N_D(p)\\] Figura 2.21: Vizinhanças [12]. 2.7.2 Conectividade Conceito importante, usado no estabelecimento limite das bordas de objetos e, identifica componentes das regiões da imagem (obtenção de propriedades específicas do objeto para o processamento de mais alto nível ). Dois pixels \\(p(x,y)\\), \\(q(x,y)\\) estão conectados se: São de alguma forma vizinhos (\\(N_4\\),\\(N_D\\) ou \\(N_8\\)). Seus níveis de cinza satisfazem algum critério de similaridade (\\(V = \\{ \\dots \\}\\)). Conectividade de 4: Os pixels p e q, assumindo valores em &amp;V&amp; , são conectados de 4 somente se q pertence ao conjunto \\(N_4(p)\\). Exemplo em figura 2.22 (a). \\[C4_{p,q} \\text{ em } V \\Leftrightarrow q \\in N_4(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C4_{p.q} \\text{ verdadeiro}\\] Conectividade de m(conectividade mista): Dois pixels \\(p\\) e \\(q\\), assumindo valores em \\(V\\), são conectados de m somente se: \\(q\\) pertence ao conjunto \\(N_4(p)\\) \\(q\\) pertence ao conjunto \\(N_D(p)\\) e a interseção entre \\(N_4(p)\\) e \\(N_4(q)\\) for vazia. \\[Cm_{p,q} \\text{ em } V\\Leftrightarrow (q \\in N_4(p) \\vee (q \\in N_D(p) \\wedge N_4(p) \\cap N_4(q) = \\{\\})) \\vee f(p) e f(q) \\in V \\] \\[V = \\{0\\} \\to Cm_{p.q}\\text{ falso}\\] Exemplo em figura 2.22 (b). Conectividade de 8: Os pixels \\(p\\) e \\(q\\), assumindo valores em V, são conectados de 8 somente se \\(q\\) pertence ao conjunto \\(N_8(p)\\). Exemplo em figura 2.22 (c). \\[C8_{p,q} \\text{ em } V\\Leftrightarrow q \\in N_8(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C8_{p.q}\\text{ verdadeiro}\\] Figura 2.22: Conectividades [12]. 2.7.3 Adjacência Dois pixels \\(p\\) e \\(q\\), com valores pertencendo a \\(V\\) são: Adjacentes-4 se \\(q\\) estiver no conjunto \\(N_4(p)\\). Adjacentes-8 se \\(q\\) estiver no conjunto \\(N_8(p)\\). Adjacentes-m, \\(p\\) e \\(q\\) subconjuntos de pixels onde \\(\\{(pq) \\vee (pp) \\vee (q q) \\}\\), são ditos adjacentes se pegamos um pixel do primeiro conjunto for adjacente a um pixel do segundo. Na figura 2.23 temos exemplos de 1. e 2. (em 2.23 (a)) e de 3. em 2.23 (b). Figura 2.23: Adjacências [12]. 2.7.4 Componente Conexa Dois pixels \\(p\\) e \\(q\\) de um subconjunto de pixels \\(V\\) da imagem são ditos conexos em \\(V\\) se existir um caminho de pa qinteiramente contido em \\(V\\). Para qualquer pixel \\(p\\) em \\(V\\), o conjunto de pixels em \\(V\\) que são conexos a pé chamado um componente conexo de \\(V\\). Note que em uma componente conexo qualquer dois pixels deste componentes são conexos entre si. Em componentes conexos distintos os pixels são disjuntos (não conectados). 2.7.5 Medidas de Distância Para pixels \\(p\\), \\(q\\) e \\(z\\) com coordenadas \\(p(x,y)\\), \\(q(s,t)\\) e \\((u,v)\\), respectivamente, \\(D\\) é uma função distância ou métrica se: \\(D(p,q) &gt;= 0 (D(p,q) = 0 ) \\Leftrightarrow p = q\\) \\(D(p,q) = D(q,p)\\) \\(D(p,z) &lt;= D(p,q) + D(q,u)\\) A distância entre dois pontos quaisquer pode ser definida por: \\[D_e(p,q) = \\sqrt{(x-s)^2 + (y-t)^2}\\] conhecida como Distância Euclidiana. Distância \\(D_4\\)(City Block ou Quarteirão) entre \\(p(x,y)\\) e \\(q(s,t)\\)é definida por: \\[D4(p,q)=|x-s|+|y-t|\\] Distância D8(Distância Xadrez) entre \\(p\\) e \\(q\\) é definida como: \\[D_8 = max(|x-s|,|y-t|)\\] 2.7.6 Operações Lógico-aritméticas As operações entre pixels são computadas pixel a pixel, considerando p e qpodemos efetuar as seguintes operações aritméticas e lógicas. Operações Aritméticas: Adição: \\(p+q\\). O uso ocorre ao se fazer a média para redução de ruído. Subtração: \\(p-q\\). É usada para remover informação estática de fundo, Detecção de diferenças entre imagens. Multiplicação: \\(p\\cdot q\\). Calibração de brilho. Divisão: \\(p\\div q\\) As operações Aritmética de Multiplicação e Divisão são usadas para corrigir sombras em níveis de cinza, produzidas em não uniformidades da iluminação ou no sensor utilizado para a aquisição da imagem. Operações Lógicas: Conjunção: \\(p\\wedge q\\) Disjunção: \\(p\\vee q\\) Complementar: \\(\\neg q(\\bar{q})\\) As operações lógicas podem ser combinadas para formar qualquer outra operação lógica, são aplicadas apenas em imagens binárias, tem seu uso no mascaramento, detecção de características e análise de forma. Refêrencias "],["transformacões-geométricas.html", "Capítulo 3 Transformacões geométricas 3.1 Definição 3.2 Sistema de coordenadas objetos (2D e 3D) 3.3 Representação Vetorial e Matricial de Imagens digitalizadas 3.4 Matrizes em Computação gráfica 3.5 Transformações em Pontos e Objetos 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação", " Capítulo 3 Transformacões geométricas Nessa seção abordaremos conceitos importantes que detalham processos de transformações geométricas na imagem digitalizada, definiremos a seção proposta, sistema de coordenadas objetos (2D e 3D), representação vetorial e matricial de Imagens digitalizadas, matrizes em computação gráfica, transformações em pontos e objetos; transformações (translação, escala, rotação). 3.1 Definição As transformações geométricas são operações que podem ser utilizadas sobre uma imagem, visando alterar características como: posição , orientação, forma ou tamanho da imagem apresentada. As operações (transformações geométricas) não alteram a topologia (pixels) da imagem operada, apenas possibilitam a projeção da imagem no espaço determinado. 3.2 Sistema de coordenadas objetos (2D e 3D) Sistemas de coordenadas nos objetos (2D e 3D) podem ser usadas para modelar objetos(imagens), servindo de referência em termos de dimensões(tamanhos), rotacionamento e posições dos objetos nas operações geométricas, dentro do ambiente de aplicação. No sistema de coordenadas polares (ao dentro da imagem, figura 3.1), as coordenadas são descritas por raio e ângulo: (r, \\(\\phi\\)). No sistema de coordenadas esférico (à esquerda), as coordenadas são descritas por raio e dois ângulos. Nos sistemas de coordenadas cilíndricos (à direita), as coordenadas são descritas por raio, ângulo e um comprimento. Os dois sistemas de extremidades são 3D. Esse tema é abordado com mais profundidade na matéria de Computação Gráfica (CG), transformações geométricas no plano e no espaço. Fonte: Coordenadas Polares [13, p. 36]. Figura 3.1: Coordenadas Polares [13, p. 36]. 3.3 Representação Vetorial e Matricial de Imagens digitalizadas Um vetor é basicamente um segmento de reta orientada (sentido e direção). Para representar um vetor em dimensão 2D, usaremos \\(V\\), como seta que sai da origem do sistema de coordenadas, para o ponto (x, y), tendo assim a direção um sentido e um comprimento específico. Fonte: [13, p. 14]. A fórmula abaixo é aplicada para calcular comprimento do vetor 2D: \\[|V| = {\\sqrt{x^2 + y^2}}\\] Exemplo 1: Calcule o comprimento de \\(V\\) com \\(x=2\\) e \\(y=3\\). Resolução: \\[|V| = 22+ 32 = 3.60\\] Se pensamos em \\(V\\) no espaço 3D, definindo a origem ao ponto \\(P(x, y, z)\\) seu comprimento seria: \\[|V| = {\\sqrt{x^2+y^2+z^2}}\\] Exemplo 2: Calcule o comprimento de \\(V\\) com \\(x=2\\) , \\(y=3\\) e \\(z=1\\). Resolução: \\[|V| = {\\sqrt{2^2+3^2+1^2} = 3.74}\\] Uma matriz é um arranjo (vetor) de elementos em duas direções (linha e coluna). Para declará-la é necessário definir a quantidade de elementos existentes em cada direção. Representaremos matriz com a letra M, as direções (linha e coluna) pelas letras L e C. Suponhamos que L = 4 e C = 4 então M[4][4] formando matriz quadrática como mostra a figura 3.2, nela pode-se observar a matriz identidade de tamanho 4 x 4. Figura 3.2: Representação matricial [13, p. 14]. 3.4 Matrizes em Computação gráfica As transformações geométricas (translação, escala e rotação) podem ser representadas na forma de equações possibilitando suas manipulações. O problema é que manipulações de objetos gráficos normalmente envolvem muitas operações aritméticas simples. As matrizes são muito usadas nessas manipulações porque são mais fáceis de usar e entender do que as equações algébricas, o que explica por que programadores e engenheiros as usam extensivamente. As matrizes são parecidas com modelo organizacional da memória dos computadores. Suas representações se relacionam diretamente com estas estruturas de armazenamento, facilmente o trabalho dos e permitindo maior velocidade para aplicações críticas como jogos e aplicações em realidade virtual. É devido a esse fato que os computadores com “facilidades vetoriais” têm sido muito usados junto a aplicações de computação gráfica. Devido ao padrão de coordenadas usualmente adotado para representação de pontos no plano (x,y) e no espaço tridimensional (x,y,z), pode ser conveniente manipular esses pontos em matrizes quadradas de 2x2 ou 3x3 elementos. Através de matrizes e de sua multiplicação, podemos representar todas as transformações lineares 2D e 3D. Várias transformações podem ser combinadas resultando em uma única matriz denominada de matriz de transformação. Na imagem digitalizada, são aplicados elementos básicos como pontos, linhas, curvas e as superfícies tridimensionais ou mesmo os sólidos que mostram os elementos que formam as imagens sintaticamente no computador. Em computação gráfica os elementos pontos, linhas, curvas e as superfícies tridimensionais ou mesmo os sólidos são denominados primitivas vetoriais da imagem. As primitivas vetoriais são associadas a um conjunto de atributos que define sua aparência e a um conjunto de dados que define a sua geometria (pontos de controle). Para esclarecer melhor, vamos considerar alguns exemplos, dois elementos facilmente caracterizados como vetoriais, pela noção de vetores já discutida são os pontos e linhas retas. A cada elemento de um conjunto de pontos associa-se uma posição, que pode ser representada por suas coordenadas (geometria), e uma cor, que será como esses pontos aparecerão na tela (tributos). No caso de um conjunto de linhas retas, cada uma pode ser definida pelas coordenadas de seus pontos extremos (geometria) e sua cor, espessura, ou ainda se aparecerá pontilhada ou tracejada (atributos). A descrição matricial é típica das imagens digitalizadas capturadas por scanners ou utilizadas nos vídeos. É a forma de descrição principal na análise e no processamento de imagens. Em computação gráfica sintética, surgem nos processos de finalização (ray tracing, z-buffers). Na representação matricial, a imagem é descrita por um conjunto de células em um arranjo especial bidimensional, uma matriz. Cada célula representa os pixels (ou pontos) da imagem matricial. Os objetos são formados usando adequadamente esses pixels. A figura 3.2 explica melhor as formas de descrição de imagens matriciais. Essa é a representação usualmente empregada para formar a imagem nas memórias e telas dos computadores e na maioria dos dispositivos de saída gráficos (impressoras e vídeos). Fonte: [13, p. 14, 15]. Figura 3.2: Descrição de imagens matriciais por conjunto de pixels [13, p. 15]. 3.5 Transformações em Pontos e Objetos Fonte: Coordenadas Polares [13, p. 38]. A habilidade de representar uma objeto em várias posições no espaço para compreender sua forma. A possibilidade de submetê-lo a diversas transformações é importante em diversas aplicações da computação gráfica [Rogers, 1990]. As operações geométricas de rotação e translação de objetos são chamadas operações de corpos rígidos. 3.6 Transformação de Translação A transformação geométrica translação tem como objetivo movimentar a objeto(imagem) no espaço (ambiente projetado), usando a matriz dedos (pontos) do objeto, aplica-se operações da nova coordenada p(x, y) sobre os pontos da matriz possibilitando transladar ao novo espaço da coordenada definida. Para transladar (mover) um objeto do ponto atual para o novo ponto, p(x) e p(y) pode ser movido por Txunidades em relação ao x, e por Ty, unidades em relação eixo . Logo nova posição do ponto p(x,y) passa a ser p’(x) e p’(y) , que podem ser escrito como: \\[x’ = x + Tx\\] \\[y’ = y + Ty\\] No caso do ponto for representado por vetor, p = (x, y), a translação desse mesmo ponto para o novo pode ser obtida pela adição de vetor de deslocamento à posição atual do ponto: \\[p’ = p+ T = [x’y’] = [xy] + [Tx Ty]\\]. Fonte: [13, p. 38]. A figura 3.3, translação de um triângulo de três unidades na horizontal e-4 na vertical. Repare que se teria o mesmo efeito transladado a origem do sistema de coordenadas para o ponto p(-3, 4)na primeira figura. Fonte: [13, p. 39]. Figura 3.3: Tranformação de translação [13, p. 39]. 3.7 Transformação de Escala A transformação geométrica escala tem como objetivo mudar as dimensões (tamanho) do objeto(imagem) no espaço (ambiente projetado). Figura 3.4. Fonte: Azevedo Eduardo, edição 2, p. 40. Para fazer com que uma imagem definida por conjunto de pontos mude de tamanho, teremos de multiplicar os valores de suas coordenadas por um fator de escala. Transformar um objeto por alguma operação nada mais é do que fazer essa operação com todos os seus pontos. Nesse caso um dos vetores de suas coordenadas são multiplicados por fatores de escala. Estes fatores de escala em 2D podem, por exemplo ser \\(Ss\\) e \\(Sy\\): \\[x’ = x * Sx\\] \\[y’ = y * Sy\\] Essa operação pode ser representada na forma matricial: Figura 3.1: (ref: escala) Figura 3.3: Tranformação de escala [13, p. 40]. Fonte: [13, p. 41]. A mudança de escala de um ponto de um objeto no espaço tridimensional pode ser obtida pela multiplicação de três fatores de escala ao ponto. A operação de mudança de escala pode ser descrita pela multiplicação de coordenadas do ponto por uma matriz diagonal cujos valores dos elementos não-nulos sejam os fatores de escala. Assim, no caso 3D tem-se: Figura 3.4: Tranformação de escala [13, p. 41]. 3.8 Transformação de Rotação A transformação geométrica da rotação tem como objetivo rotacionar objeto(imagem) no espaço (ambiente projetado), é equivalente a gira ao redor da origem do sistema de coordenadas. Figura 3.5. Fonte: [13, p. 41, 42]. Na figura 3.5 rotação de um ponto P em torno da origem, passando para a posição P’. Repare que se chegaria a esse mesmo ponto através de uma rotação de - no sistema de eixos XY. Figura 3.4: Tranformação de rotação [13, p. 42]. Fonte: [13, p. 42]. Se um ponto de coordenada (x,y), distante \\(r=(x2 + y2)^\\frac12\\) da origem do sistema de coordenadas, for rotacionado de um ângulo \\(\\phi\\) em torno da origem, suas coordenadas, que antes eram definidas como: \\(x = r * \\cos(\\phi), y =r * \\sin(\\phi)\\), passam a ser descritas como (x’, y’) dadas por: \\[x&#39; = r * \\cos(\\theta + \\phi) = r * \\cos\\phi * cos\\theta - r * \\sin\\phi * \\sin\\theta\\] \\[x&#39; = r * \\sin(\\theta + \\phi) = r * \\sin\\phi * cos\\theta + r * \\cos\\phi * \\sin\\theta\\] isso equivale às expressões: \\[x&#39; = x\\cos(\\phi) - y\\sin(\\phi)\\] \\[y&#39; = y\\cos(\\phi) + y\\sin(\\phi)\\] Essas expressões podem ser descritas pela multiplicação do vetor de coordenadas do ponto (x,y) pela matriz: Figura 3.5: Tranformação de rotaão [13, p. 42]. Fonte: [13, p. 42]. Para alterar a orientação de um objeto(imagem) em torno de um certo ponto realizando uma combinação da rotação com a translação, é necessário, antes de aplicar a rotação de um ângulo no plano das coordenadas em torno de um ponto, realizar uma translação para localizar esse ponto na origem do sistema, aplicando a rotação desejada e, então, uma translação inversa para localizar o dado ponto na origem. A multiplicação de coordenadas por uma matriz de rotação pode resultar em uma translação. Figura 3.5: Tranformação de rotaão [13, p. 43]. Refêrencias "],["transformações-radiométricas.html", "Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência", " Capítulo 4 Transformações radiométricas As manipulações no domínio espacial ocorrem diretamente sobre os pixels no plano da imagem. As duas principais categorias de transformações de intensidade a nível espacial são transformações radiométricas e filtragem espacial. A filtragem espacial pode ser representada pela expressão: \\(g(x,y) = T[f(x, y)]()\\) O componente \\(f(x, y)\\) é a imagem de entrada, \\(g(x, y)\\) é a imagem de saída, e T é um operador em f definido em uma vizinhança do ponto \\((x, y)\\). Este procedimento pode ser aplicado como na Figura, em que um ponto \\((x, y)\\) está destacado com sua vizinhança. Geralmente a vizinhança é retangular e bem menor que a imagem, e no caso da Figura é um quadrado de tamanho 3 × 3. Figura - Processamento no domínio espacial. Figura 4.1: [2, p. 69]. Fonte: [2, p. 69]. Na filtragem espacial, o valor da intensidade no centro da vizinhança é alterado de um pixel ao outro enquanto se aplica um operador T aos pixels na vizinhança para gerar a saída na posição central. O processo pode começar no canto superior esquerdo da imagem de entrada e avançar pixel por pixel horizontalmente, uma linha por vez. Nas bordas, os vizinhos externos são ignorados nos cálculos ou se preenche a imagem com uma borda de 0s ou outros valores predefinidos. A vizinhança e uma operação predefinida (T) definem o filtro espacial (também denominada máscara espacial, kernel, template ou janela). A menor vizinhança possível, de tamanho 1 x 1, é tratada como uma função de transformação de intensidade (transformação radiométrica). Na transformação radiométrica, a intensidade (s) em cada ponto da imagem g depende apenas do valor (r) em um único ponto na imagem f , como na expressão: s = T(r) Como as transformações de intensidade operam individualmente nos pixels de uma imagem, são chamadas de técnicas de processamento ponto a ponto (Gonzalez; Woods, 2010). Este processo é utilizado, por exemplo, para fins de manipulação de contraste e limiarização de imagem. Já a filtragem espacial, também muito aplicada para realce de imagens, é uma técnica de processamento por vizinhança (Gonzalez; Woods, 2010). Nesta seção serão apresentados alguns exemplos de realce de imagem, que tem o foco em melhorar o aspecto da imagem, tornando-a mais viável para o seu objetivo. O realce pode ser utilizado para minimizar na imagem efeitos de ruídos, perda de contraste, borramento e distorções. Na Figura são mostradas as três funções (T) mais básicas aplicadas na transformação de intensidade, frequentemente utilizadas para o realce de imagens. Para cada uma das três funções - linear (transformações de negativo), logarítmica (transformações de log e log inverso) e de potência (transformações de n-ésima potência e n-ésima raiz) - será apresentado um tópico com mais informações. Figura - Funções de transformação de intensidade. Figura 4.2: [2, p. 71]. 4.1 Transformação Linear O negativo de uma imagem com níveis de intensidade na faixa \\([0, L – 1]\\) é obtido pela transformação: \\[s = L – 1 – r ()\\] Esse tipo de processamento pode ser utilizado para realçar detalhes brancos ou cinza em regiões escuras de uma imagem (Gonzalez; Woods, 2010). Na Figura é exemplificado uma aplicação da transformação de negativo. A imagem original é uma mamografia digital mostrando uma pequena lesão. Após a transformação se torna mais fácil analisar o tecido mamário no negativo da imagem (b). Figura - Transformação Linear (Negativos de Imagens). Mamografia digital original. (b) Negativo da imagem. Fonte: [2, p. 71]. 4.2 Transformação Logarítmica A forma geral da transformação logarítmica é: \\[s = c log (1 + r)\\] em que c é uma constante e considera-se que r ≥ 0. Nas aplicações Log, um dos objetivos é a expansão dos valores de pixels mais escuros em uma imagem, ao mesmo tempo em que se comprime os valores de níveis mais altos (Gonzalez; Woods, 2010). A expansão é quando se mapeia uma faixa estreita de baixos valores de intensidade em uma faixa mais ampla de níveis de saída, como na função Log da Figura. Na compressão ocorre o oposto com os valores mais altos de níveis de intensidade. Na transformação logarítmica inversa se comprime os pixels mais escuros e se expande os mais claros (Gonzalez; Woods, 2010). Uma maneira de avaliar o efeito da transformação logarítmica é utilizar sobre o espectro de Fourier. Na Figura (a) mostra um espectro de Fourier com valores variando de 0 a 1,5 × 106, com baixo nível de detalhamento. Ao aplicar a transformação Log (com c = 1 neste caso) aos valores do espectro, a faixa de valores do resultado passa a ser de 0 a 6,2, o que melhora o detalhamento na exibição da imagem (b). Figura - Transformação Logarítmica. Espectro de Fourier. (b) Resultado da aplicação da transformação logarítmica. Fonte: [2, p. 72]. 4.3 Transformação de Potência As transformações de potência apresentam a forma básica: \\[s = cr^y\\] sendo c e γ constantes positivas. Ao plotar a transformação de potência para diferentes valores (y), e c=1, na Figura se observa um comportamento semelhante ao de expansão/compressão da transformação Logarítmica. Curvas de transformação de potência com valores de y menores que 1 (fração) tem um efeito parecido com a função Log, enquanto que para valores de y maiores que 1 se parecem mais com a logarítmica inversa. Figura - Plotagens da equação \\[s = cr^γ\\] para vários valores de y. Fonte: [2, p. 72]. Uma das aplicações da transformação de potência é a correção gama em dispositivos que funcionam de acordo com uma lei de potência, como em computadores (Gonzalez; Woods, 2010). Por exemplo, dispositivos de tubo de raios catódicos apresentam relação com a função potência de expoentes variando em aproximadamente 1,8 a 2,5. Para valores de gama próximos de 2,5 a imagem de saída no monitor tende a ser mais escura ( Figura b) que a imagem original (Figura a). A imagem corrigida (Figura c) pela correção gama, neste caso com gama menor que 1, gera uma saída (Figura d) mais parecida com a imagem original (Figura a). Figura - Correção Gama. Imagem com variação gradativa de intensidade (gradiente). (b) Imagem vista em um monitor simulado com gama igual a 2,5. (c) Imagem com correção gama. (d) Imagem corrigida vista no mesmo monitor. Fonte: [2, p. 72]. Outra utilidade da transformação de potência pode ser vista na Figura, em que a imagem original está desbotada, indicando que se deve aplicar uma compressão dos níveis mais baixos e expandir valores mais altos (Gonzalez; Woods, 2010). Assim, a transformação foi realizada com gama maior que 1. Os resultados do processamento com \\(y = 3, y = 4,\\) e \\(y = 5\\) podem ser vistos nas imagens (b), (c) e (d), respectivamente. Figura - Transformações de potência. Imagem aérea. (b) a (d) Resultados da aplicação da transformação de potência com \\(c = 1\\) e \\(y = 3, 4\\) e \\(5\\), respectivamente. Fonte: [2, p. 74]. A distribuição dos níveis de intensidade (L) de uma imagem podem ser identificados em um histograma, ou seja, um gráfico com o número de pixels na imagem para cada nível de cinza. Assim, os histogramas podem servir de referência para várias manipulações no domínio espacial, além de fornecer estatísticas das imagens e ser útil em aplicações como compressão e segmentação. O histograma também pode ser interpretado como uma distribuição discreta da probabilidade de ocorrência do nível de intensidade \\(r_k\\) em uma imagem (Gonzalez; Woods, 2010): \\[p(r_k) = \\frac{x_k}{MN} k = 0, 1,2, ..., L - 1 ()\\] sendo \\(M\\) e \\(N\\) as dimensões de linha e coluna da imagem, e \\(n_k\\) é o número de pixels da imagem com intensidade \\(r_k\\). Na Figura estão identificados quatro histogramas referentes a cada uma das imagens dos grãos de pólen do lado esquerdo. O eixo horizontal de cada histograma corresponde a valores de intensidade, \\(r_k\\), e o eixo vertical são os valores de \\(p(r_k)\\). Cada imagem destaca uma característica em relação à intensidade da imagem: escura, clara, baixo contraste e alto contraste. Na imagem do topo, a mais escura, as barras do histograma estão concentradas no lado inferior (escuro) da escala de intensidades, enquanto que na imagem mais clara tendem à região oposta (Gonzalez; Woods, 2010). Uma imagem com baixo contraste, aparência desbotada e sem brilho, tem um histograma estreito normalmente localizado no meio da escala de intensidades. Os componentes do histograma na imagem de alto contraste estão distribuídos quase uniformemente em uma ampla faixa da escala de intensidades, com poucas linhas verticais sendo muito mais altas do que as outras. As imagens de alto contraste tendem a apresentar uma boa correspondência em relação aos detalhes de nível de cinza (Gonzalez; Woods, 2010). Figura - Histogramas de uma imagem com grãos de pólen. De cima para baixo: escura, clara, baixo contraste e alto contraste. Fonte: [2, p. 79]. Os histogramas são utilizados em grande parte para auxiliar em transformações de intensidade com foco em melhorar o contraste, tornando mais fácil a percepção de informações de interesse na imagem (Pedrini; Schwartz, 2008). Nos exemplos de transformações apresentados nesta seção (linear, logarítmica e de potência), a escolha do operador \\((T)\\) de transformação geralmente é empírica, em que se deve considerar a imagem original e o efeito desejado. Para transformar a imagem de forma que se altere o histograma de uma maneira específica e automática utilizam-se métodos em que o formato dos histogramas são pré-definidos ou atendem a um determinado padrão, como na equalização de histograma e especificação de histograma (Gonzalez; Woods, 2010). Na equalização do histograma, o histograma da imagem original \\(f\\) é alterado de maneira que a imagem transformada g tenha uma distribuição aproximadamente uniforme dos níveis de cinza em uma faixa mais ampla de valores, assumindo características próximas de uma imagem de alto contraste (Pedrini; Schwartz, 2008). Neste método se aplica um operador de transformação de intensidade \\((T)\\) na forma contínua, Equação, que gera uma variável aleatória (valores de intensidade da imagem transformada) caracterizada por uma função densidade de probabilidade uniforme (PDF, probability density function) . \\(s = T(r) = (L - 1)0rpr(w) dw ( )\\) sendo w uma variável local da integração, L os níveis de cinza da imagem e pr(r) a PDF de r (valores de intensidade da imagem original). A demonstração da obtenção da PDF da variável \\(s (p_s(s))\\), na equação, pode ser vista com detalhes no livro “Processamento digital de imagens” (Gonzalez; Woods, 2010). O gráfico das PDFs de r e s estão identificados na Figura como \\(pr(r)\\) e \\(p_s(s)\\). \\[p_s(s) = \\frac{1}{L-1} \\] para $s \\(\\leq\\) L - 1 ()$ Figura - Equalização de histograma na forma contínua. (a) Uma PDF arbitrária. (b) Resultado da aplicação da transformação para equalização. Fonte: [2, p. 81]. Para que esses conceitos sejam aplicados no processamento de imagens eles devem ser expressos na forma discreta. A transformação na Equação da forma discreta é: \\(S_k = T(r_k) = (L - 1) \\sum {i=1}^n x i\\) Refêrencias "],["filtros.html", "Capítulo 5 Filtros 5.1 Convolução 5.2 Média 5.3 Mediana 5.4 Gaussiano", " Capítulo 5 Filtros Filtros são uma poderosa ferramenta para se realizar operações em imagens. Diferente das operações de ponto, que operam sobre um único pixel, as operações utilizando filtros levam em consideração os pixels próximos ao pixel atualmente em modificação. Isso nos permite realizar alterações muito mais complexas do que as realizadas anteriormente, como a operação sharpen (aguçamento) e blur (suavizar), que podem ser observados na figura 5.1 (a) e figura 5.1 (b), respectivamente. Figura 5.1: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] 5.1 Convolução Convolução é uma operação muito utilizada no PDI, a qual tem suas origens na matemática, onde ela é definida como uma operação realizada entre duas funções e que resulta numa terceira, ou, em outras palavras, ela recebe dois sinais de entrada e gera um sinal de saída. No caso do PDI, podemos imaginar os sinais de entrada como sendo a nossa imagem e o filtro (kernel), e a nossa saída como sendo a imagem filtrada. Quando dizemos kernel, estamos nos referindo a uma função ou, no caso do processamento de imagens, a uma matriz, que é aplicada em nossa imagem e produz como saída o objeto de entrada com modificações. Na tabela 5.1, temos exemplos de alguns tipos de kernels que podem ser utilizados na convolução e seus respectivos resultados. Além dos efeitos mais comuns, como o de desfoque (blur), podemos utilizar kernels que extraem informações mais complexas das imagens, como os detectores de borda, que serão discutidos mais a fundo nos próximos tópicos. Tabela 5.1: Exemplo de kernels (adaptada de [14]). Operação Kernel Resultado Identidade (Imagem Original) \\[\\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] Detecção de borda \\[\\begin{bmatrix} -1 &amp; -1 &amp; -1\\\\ -1 &amp; 8 &amp; -1\\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}\\] Média (box blur) \\[\\frac{1}{9}\\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\] Gaussian blur \\[\\frac{1}{16}\\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\ 2 &amp; 4 &amp; 2\\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\] Agora, veremos operação de convolução mais detalhadamente. Essa operação geralmente é representada por \\(*\\) e pode ser descrita, em poucas palavras, como uma soma de produtos que é realizada com um deslizamento sobre a função de entrada. A figura 5.2 representa de maneira visual a convolução em um cenário de uma dimensão, através do sinal de entrada (signal) e o kernel. Podemos ainda perceber que o kernel está rotacionado em 180º, isso se deve a definição de convolução. Figura 5.2: Convolução em uma dimensão [15] Algo interessante que podemos observar na imagem 5.2 é que nosso sinal de entrada é quase totalmente 0 e contém um único ponto 1, isso faz com que nosso resultado seja uma cópia do kernel. A partir disso, conseguimos imaginar o por que temos como resultado a própria imagem quando aplicado um kernel identidade, como mostrado na tabela 5.1. Antes de irmos mais adiante no assunto, é importante esclarecermos alguns conceitos para que não se tornem confusos. Existe outra operação matemática extremamente parecida com a convolução chamada correlação, sendo que ela também realiza a soma de multiplicações com a diferença de que ela não rotaciona o kernel. Para entendermos bem essa diferença, podemos observar a figura 5.3, onde temos um exemplo de correlação e convolução sendo executados em um espaço unidimensional. Temos uma função \\(f\\) e um filtro \\(w\\) na figura 5.3 (a) e (b), na sequência, de (b) e (j), temos as funções e os filtros prontos para se realizar a correlação e a convolução. Nas etapas (c) e (k), podemos ver o preenchimento com zeros, isso ocorre porque há partes das funções que não se sobrepõem, dessa forma, permite que \\(w\\) percorra todos os pixels de \\(f\\). Após isso, é realizado o primeiro passo da correlação e convolução, onde podemos observar que o resultado é 0 já que \\(w\\) está sobreposto por somente zeros, logo a soma da multiplicação de cada item de \\(w\\) por \\(f\\) será nulo. Deslocamos então o filtro \\(w\\) em uma unidade a direita, onde o resultado novamente será 0, sendo que o primeiro resultado não nulo se dará no terceiro deslocamento, sendo 8 para a correlação e \\(1\\) para a convolução. Temos o resultado de ambas operações em (g) e (o) e o resultado recortado em (h) e (p), recorte este que remove os zeros até o tamanho ficar igual ao da \\(f\\) inicial. Figura 5.3: Ilustração de correlação e convolução unidimensional [2, p. 96] Vamos extender agora essas duas operações à aplicação em duas dimensões. Uma representação disso pode ser vista na figura 5.4, onde temos novamente o kernel \\(w\\) e a função \\(f\\). Percebe-se outra vez o efeito de se aplicar o kernel em uma imagem com apenas o número 1 no meio, nos dois casos temos como saída a cópia do kernel, com a diferença que na correlação ele sai rotacionado. Assim, nota-se que se pré-rotacionarmos o filtro e realizarmos a correlação teremos no final uma convolução. Já que a correlação e convolução são iguais, quais delas devo utilizar? Segundo Gonzalez, [2, p. 98], isso é uma questão de preferência e qualquer uma das duas operações conseguem realizar a outra com uma simples rotação do kernel. Essa questão se torna ainda menos relevante quando utilizamos filtros que são simétricos, pois, como antes e após a rotação temos o mesmo kernel, tanto correlação ou convolução nos darão o mesmo resultado; já em kernels assimétricos, temos resultados diferentes. Ainda, segundo Moeslund, [7, p. 87], quando trabalhamos com filtros de desfoque, detectores de borda, entre outros, o processo de se aplicar o kernel é comumente chamado de convolução mesmo quando na prática se é implementada a correlação. Figura 5.4: Ilustração de correlação e convolução bidimensional [2, p. 98] 5.1.1 Definção matemática de convolução Vamos explorar um pouco das notações matemáticas utilizadas para representar a convolução e a correlação, assim também poderemos consolidar a idéia de que ambas são muito correlacionadas. Como dito no início desta seção, geralmente a convolução é identificada por \\(*\\), já a correlação costuma ser identificada por ☆. A correlação em duas dimensões segue a seguinte equação (Equação X): \\(g(x,y) = w(x,y)☆f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x+s,y+t)\\) Onde \\(w\\) é nosso \\(kernel\\) e \\(f\\) nossa imagem, podemos perceber que ambas são funções de duas variáveis, \\(x\\) e \\(y\\), pois estamos trabalhando em duas dimensões. Os limites dos somatórios são dados por \\(a=(m-1)/2\\) e \\(b=(n-1)/2\\). E o que essa função faz é andar em cada posição da imagem, ou seja, \\((x,y)\\), e substituir o píxel atual pela soma de produtos da multiplicação dos valores do \\(kernel\\) pelos valores dos píxels da imagem. Já a convolução tem uma equação bem similar, sendo diferente apenas pelos sinais negativos em \\(f\\), o que evidência a rotação do \\(kernel\\). Podemos notar que os sinais inversos estão em \\(f\\) e não em \\(w\\); segundo [2, p. 98], isso é usado para fins de simplicidade de notação e não alteram o resultado. \\(g(x,y) = w(x,y)*f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x-s,y-t)\\) Uma das melhores maneiras de entender bem as equações é ver um exemplo prático. Veremos isso a seguir, onde temos um exemplo passo-a-passo de correlação: \\[ \\text{w}\\text{*f}\\left(0,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(-1,-1\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot0+0\\cdot2+\\left(-2\\right)\\cdot1\\\\ +1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ =0\\,-2-3\\,=\\,-5 \\] \\[ \\text{w}\\text{*f}\\left(0,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,2\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot2+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ =0\\,+4+8=\\,12 \\] \\[ \\text{w}\\text{*f}\\left(0,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,3\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot1+0\\cdot0+\\left(-2\\right)\\cdot0\\\\ +1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ =0\\,+2+3=\\,5 \\] \\[ \\text{w}\\text{*f}\\left(1,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,1\\right)\\\\ =\\,1\\cdot0+0\\cdot2+\\left(-1\\right)\\cdot1\\\\ +2\\cdot0+0\\cdot9+\\left(-2\\right)\\cdot3\\\\ +1\\cdot0+0\\cdot5+\\left(-1\\right)\\cdot4\\\\ =\\left(-1\\right)\\,+\\left(-6\\right)+\\left(-4\\right)=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(1,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,2\\right)\\\\ =\\,1\\cdot2+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot9+0\\cdot3+\\left(-2\\right)\\cdot1\\\\ +1\\cdot5+0\\cdot4+\\left(-1\\right)\\cdot2\\\\ =2\\,+16+3=\\,21 \\] \\[ \\text{w}\\text{*f}\\left(1,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,3\\right)\\\\ =\\,1\\cdot1+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot3+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot4+0\\cdot2+\\left(-1\\right)\\cdot0\\\\ =1\\,+6+4=\\,11 \\] \\[ \\text{w}\\text{*f}\\left(2,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,0+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,1\\right)\\\\ =\\,1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ +2\\cdot0+0\\cdot5+\\left(-2\\right)\\cdot4\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =\\left(-3\\right)+\\left(-8\\right)+0=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(2,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,2\\right)\\\\ =\\,1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ +2\\cdot5+0\\cdot4+\\left(-2\\right)\\cdot2\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =8+6+0=\\,14\\\\ \\] \\[ \\text{w}\\text{f}\\left(2,2\\right)\\text{=}\\sum{s}^{}\\sum{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,2+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,3\\right)\\\\ =,1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot4+0\\cdot2+\\left(-2\\right)\\cdot0\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =3+8+0=11 \\] Fonte: Autoria própria inspirado em http://www.songho.ca/dsp/convolution/convolution2d_example.html e https://arxiv.org/abs/1603.07285 O exemplo utiliza um \\(kernel\\) assimétrico, então, como dito anteriormente, os resultados da correlação e convolução são diferentes. Deixaremos isso como um exercício ao leitor, realizar a convolução para provar a diferença. O caso anterior tem também uma característica que é o complemento das bordas com 0’s para que o \\(kernel\\) pudesse ser aplicado sobre toda a imagem. Essa é uma das maneiras de lidar com o problema das bordas, sendo que temos essa e mais algumas possibilidades [8, p. 125] [16, p. 60]: Preenchimento com constante (constant padding): Método onde as laterais são preenchidas com um valor constante, comumente 0. Preenchimento com vizinho (nearest neighbor): Se realiza o preenchimento das bordas adicionais com os valores dos vizinhos mais próximos. Reflexão (reflect): Os pixels das bordas da imagem são repetidos nas bordas adicionais. Repetição (ou wrap): A imagem é repetida nas bordas. Além dos métodos apresentados existem outras técnicas que podem ser utilizadas, sendo que em cada caso se escolhe a que melhor se ajusta à tarefa realizada. O último ponto a ser explorado é a escolha do tamanho ideal do preenchimento que deve ser aplicado à imagem. De forma geral, se temos uma imagem (\\(f\\)), um kernel (\\(w\\)) e um preenchimento (\\(p\\)), podemos escrever a seguinte relação1: \\[saída = (f_{vertical} - w_{vertical} + p_{vertical} + 1) \\times (f_{horizontal} - w_{horizontal} + p_{horizontal} + 1)\\] Isso nos leva a perceber que o tamanho da imagem de saída aumenta conforme o preenchimento. Na maioria dos casos, queremos que a imagem de entrada e saída tenha o mesmo tamanho, então usamos \\(p_{vertical} = k_{vertical} - 1\\) e \\(p_{horizontal} = k_{horizontal} - 1\\). E como, geralmente, o kernel tem tamanho ímpar, utilizamos \\(\\frac{p_{vertical}}{2}\\) e \\(\\frac{p_{horizontal}}{2}\\). 5.2 Média Um filtro de média é um tipo de filtro que utiliza a média dos valores dos píxels próximos ao píxel central. Como esse tipo de filtro realiza uma operação linear, ele é classificado como um filtro linear de suavização. Essa suavização se dá exatamente pelo tipo da operação utilizada, a média dos píxels vizinhos, que diminui a nitidez pela redução das transições abruptas nos níveis de intensidade. Um dos problemas que podem ocorrer é que as bordas também são mudanças abruptas, então podem ser comprometidas pelo filtro. Na figura 5.5, temos um exemplo do efeito desse filtro: Figura 5.5: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] A máscara (kernel) de um filtro de média pode ser representada por: \\[\\begin{bmatrix} \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9} \\end{bmatrix} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\] A primeira forma é a soma de todos os valores divididos por 9, o qual é o tamanho do filtro. A segunda forma, onde todos os coeficientes do kernel são 1’s, é mais eficiente computacionalmente, pois realizamos todas as somas e multiplicações antes de dividirmos. Esse tipo de filtro é muitas vezes chamado de box filter. Figura 5.6: Imagem com diferentes tamanhos de filtro de média [2, p. 102] Na figura 5.6, temos um exemplo onde foram aplicados filtros de média com diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Podemos notar que com o menor filtro, o de tamanho 3, temos um leve borramento na imagem toda, mas que as partes que tem o mesmo tamanho ou são menores que o filtro tem um borramento maior. Isso exemplifica umas das importantes aplicações dos filtros de suavização, que é a de desfocar os objetos menores e deixar os maiores em maior evidência. Figura 5.7: Exemplo de uso do desfoque [2, p. 103] Na figura 5.7, podemos ver como o desfoque pode ser utilizado para encontrar os detalhes principais da imagem. Nesta imagem obtida a partir do telescópio Hubble foi aplicado o desfoque para diminuir a visibilidade dos objetos menores e dar maior ênfase aos da frente. E, então, foi limiarizado o resultado a fim de destacar esses objetos. 5.3 Mediana A mediana é um filtro não linear que utiliza como princípio a própria técnica estatística, que consiste em ordenar um conjunto de dados em ordem e selecionar o valor central. Esse tipo de filtro é muito eficiente na remoção de ruído, principalmente o tipo de ruído conhecido como ruído sal e pimenta, que tem uma característica aparência de pixels pretos e brancos. Como dito anteriormente que a mediana consiste em ordenar o conjunto de dados, nesse caso os valores da vizinhança do píxel. Se usarmos uma vizinhança de tamanho 3x3, a mediana será o quinto maior valor, pois consideramos também o valor do píxel central. Na figura 5.8, tem-se um exemplo mostrando uma imagem de raios X de uma placa de circuitos com ruído sal e pimenta (a). As respectivas aplicações de um filtro de média (b) e do filtro da mediana (c), ambos com dimensões 3x3. A partir dela, é visto o quanto o filtro da mediana se sai melhor na remoção do ruído. Figura 5.8: Remoção de ruído [2, p. 103] 5.4 Gaussiano Filtro de Gauss é um filtro simétrico (isotrópico) usado para suavizar imagens. Ele funciona análogo a um filtro de média ponderada, mas com um padrão, isto é, dá ênfase ao pixel central da máscara (kernel) e menor ênfase a medida que os demais se distanciam dele, seguindo um gradiente muito similar à distribuição gaussiana. O “similar” se deve somente ao fato que será necessário truncamentos, assim, havendo erros e por ser desconsiderado a constante multiplicadora, o que melhora o perfil da função para aplicação no PDI. Função gaussiana: \\[g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Função gaussiana modificada: \\[g(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Note que a função tem formato de sino, que varia de acordo com os valores atribuídos ao, \\(\\sigma\\), o qual dependerá da ênfase desejada no cálculo. A medida que \\(\\sigma\\) aumenta, maior é o peso dado às “caldas”, no caso, aos pixels mais distantes do pixel central. Característica representada no gráfico 5.9. A fim de que o pixel central da máscara esteja em \\((0,0)\\), considera-se \\(\\mu=0\\). Figura 5.9: Função gaussiana com \\(\\mu=0\\) e diferentes \\(\\sigma\\) O filtro gaussiano 2D é gerado através da convolução de dois vetores da mesma função - a imagem se refere ao subconjunto do contradomínio -, sendo estas, normalmente, oriundas da mesma função gaussiana modificada. De forma alternativa, ele também pode ser construído a partir da i-ésima linha do Triângulo de Pascal que corresponde ao tamanho do kernel desejado. Como o método é o mesmo para as duas fontes, explicaremos ele utilizando como fonte a função gaussiana modificada. O procedimento primário é obter a imagem da função, mas para obter um filtro com intuito gaussiano se deve selecionar a amostragem dos pontos no intervalo de \\(\\pm 2.5\\sigma\\) ou \\(\\pm 3.5\\sigma\\) [8, p. 114]. Assim, optando-se pela primeira opção, procura-se \\(g(-3)\\), \\(g(-2)\\), \\(g(-1)\\), \\(g(0)\\), \\(g(1)\\), \\(g(2)\\) e \\(g(3)\\). Entretanto, como alguns valores têm infinitas casas decimais, devemos truncá-los, no qual foi preferido com cinco casas decimais. \\(g(-3) = g(3) = 0,01110\\) \\(g(-2) = g(2) = 0,13533\\) \\(g(-1) = g(1) = 0,60653\\) \\(g(0) = 1\\) Então, considera-se o menor valor como 1, no caso \\(f(3)\\) e \\(f(-3)\\), e interpolamos os demais, truncando-os com propósito de obter a parte inteira. A divisão por 224 é oriunda da soma dos pesos da média. Pronto, temos a máscara gaussiana 1D necessária: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) Com a máscara em mãos, convoluciona-se os seguintes fatores: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) \\(\\frac{1}{224}\\begin{bmatrix}1 \\\\ 12 \\\\ 54 \\\\ 90 \\\\ 54 \\\\ 12 \\\\ 1\\end{bmatrix}\\) é a transposta da máscara 1D \\(f(x,y)\\) é a imagem de entrada Nessa propriedade há uma determinada ordem que é mais eficiente para o sistema digital, que é a convolução entre \\(f(x,y)\\) e uma das máscaras 1D e, depois, a convolução desse resultado com a transposta desta máscara. Pronto, como saída teremos o filtro gaussiano aplicado a imagem. Figura 5.10: Imagem com diferentes tamanhos de filtro gaussiano. (Adaptado:[2, p. 102]) Nesse exemplo, temos um exemplo similar ao usado no filtro de média, porém, usando-se o filtro gaussiano. A imagem de entrada (a) tem tamanho 500x500 onde foram aplicados filtros de diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Note que, ao contrário do exemplo anterior, há uma maior preservação da nítidez, todavia não seria um filtro eficiente para redução de ruído. Essa característica é devido ao maior peso dado ao pixel central do kernel. Refêrencias "],["segmentação.html", "Capítulo 6 Segmentação 6.1 Detecção de Bordas 6.2 Transformada de Hough 6.3 Detecção de Quinas 6.4 Detecção de Blobs 6.5 Limiarização", " Capítulo 6 Segmentação 6.1 Detecção de Bordas 6.1.1 Método de Canny O algoritmo de Canny recebeu esse nome em alusão a John Canny, que o propôs em seu artigo, “A computational Approach to Edge Detection”[17], publicado em 1986. Sua formulação se baseava em três pontos principais: Uma baixa taxa de erro, ou seja, todas as bordas presentes na imagem devem ser encontradas e não deve haver respostas espúrias. O segundo critério diz que as bordas detectadas devem estar bem localizadas, em outras palavras, elas devem estar o mais próximo possível das bordas verdadeiras. O terceiro e último critério diz que se deve minimizar o número de máximos locais em torno da borda verdadeira, para que não sejam encontrados múltiplos pixels de borda onde deve haver somente um. Em seu trabalho, Canny buscou encontrar soluções ótimas, matematicamente, que obedecessem os três critérios. Apesar disso, é muito difícil, ou impossível, encontrar uma solução que satisfaça completamente os objetivos descritos[2, p. 474]. Todavia é possível utilizar uma aproximação por meio de otimização numérica com as bordas em degrau em um exemplo 1-D que contenham ruído branco gaussiano e mostrar que uma boa aproximação para um ótimo detector de bordas é a primeira derivada de uma gaussiana[2, p. 474]: \\[\\frac{\\mathrm{d} }{\\mathrm{d} x}e^{\\frac{-x^2}{2\\sigma^2}} = \\frac{-x}{\\sigma^2}e^{\\frac{-x^2}{2\\sigma^2}}\\] Canny demonstrou que a utilização dessa aproximação pode ser feita com uma taxa 20% inferior à solução numérica, o que a torna praticamente imperceptível para muitas das aplicações[2, p. 474]. A ideia anterior foi imaginada em um aspecto 1D, precisamos agora, expandir esse conceito para uma generalização 2D. Uma borda de degrau pode ser caracterizada pela sua posição, orientação e possível magnitude. Aplicar um filtro Gaussiano em uma imagem e depois diferenciá-la forma um simples e efetivo operador direcional[18, p. 145]. Digamos então que \\(f(x,y)\\) seja uma imagem e \\(G(x,y)\\) a função gaussiana: \\[G(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}}\\] Temos como saída a imagem suavizada: \\[f_s(x,y)=G(x,y)*f(x,y)\\] E após isso realizamos o cálculo da magnitude e direção do gradiente: \\[M(x,y) = \\sqrt{g_x^2+g_y^2}\\] \\[\\alpha(x,y)= \\tan^{-1}\\left ( \\frac{g_y}{g_x} \\right )\\] onde \\(g_x=\\partial f_s/\\partial x\\) e \\(g_y=\\partial f_s/\\partial y\\). Para o cálculo das derivadas parciais podemos utilizar tanto Prewitt quanto Sobel. Como essa primeira etapa utiliza operadores que calculam as primeiras derivadas, acabamos com bordas grossas, e o terceiro objetivo da proposta de Canny é ter bordas com único ponto, por isso o próximo passo é a de afinar as bordas encontradas. O método que usaremos para isso é chamado supressão dos não máximos. Esse processo tem como base a discretização das direções da normal da borda(vetor gradiente), ou seja, em uma região 3x3 temos 4 direções possíveis, como pode ser visto na figura 6.1(c), sendo que consideramos 4 pois é contando as duas direções, como exemplo, consideramos um borda de 45º se ela se encontra entre +157,5º e +112,5º ou -67,5º e -22,5º. Na figura 6.1(a) temos um exemplo de duas orientações que podem existir em uma borda horizontal, e na figura 6.1(b) podemos ver a normal de uma borda horizontal e o intervalo de valores onde a direção do vetor gradiente pode existir. Figura 6.1: Discretização das direções. (a)Borda horizontal. (b) Intervalo dos possíveis valores do ângulo, normal da borda, para uma borda horizontal. (c) Intervalo de valores do ângulo da normal para os diferentes tipos de borda. [2, p. 475] Se consideramos \\(d1\\), \\(d2\\), \\(d3\\) e \\(d4\\) como as direções possíveis em uma área 3x3, podemos formular o seguinte esquema de supressão de não máximos de uma região 3x3 centrada em todos os pontos \\((x,y)\\) de [2, p. 475]: - Encontrar a direção \\(d_k\\) que está mais perto de \\(\\alpha (x,y)\\). - Se o valor de \\(M(x,y)\\) for inferior a pelo menos um dos seus dois vizinhos ao logo de \\(d_k\\), deixe \\(g_N(x,y)=0\\)(supressão); caso contrário, deixe \\(g_N(x,y)=M(x,y)\\). Onde \\(g_N(x,y)\\) é a imagem suprimida. A última operação a ser realizada é a limiarização, para se remover os pontos de falsas bordas. Aqui usaremos a limiarização por histerese que utiliza dois limiares, um baixo(\\(T_L\\)) e um alto (\\(T_H\\)), sendo que Canny sugeriu, em seu trabalho, que a razão entre o limiar alto para o baixo deva ser de dois ou três para um. Podemos imaginar essa limiarização da seguinte forma, criamos duas imagens adicionais: \\[g_{NH}(x,y) = g_N(x,y)\\geq T_H\\] e \\[g_{NL}(x,y) = g_N(x,y)\\geq T_L\\] Onde \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) são definidas inicialmente como \\(0\\). Temos então que \\(g_{NH}(x,y)\\)conterá os pixels que são maiores que o nosso limiar e \\(g_{NL}(x,y)\\) terá os pixels que estão acima do nosso limiar baixo, o que significa que ele contém os pixels que se encontram no meio dos dois limiares mais o que está acima do limiar alto, temos então que remover esses pixels, o que significa: \\[g_{NL}(x,y)=g_{NL}(x,y)-g_{NH}(x,y)\\] Podemos chamar os pixels de \\(g_{NH}(x,y)\\) de pixels fortes e os de \\(g_{NL}(x,y)\\) de fracos. Ao final dessa limiarização todos os pixels fortes são classificados como borda válida, mas com falhas, que nos leva a outro processo: Localizar o próximo pixel borda a ser revisado em \\(g_{NH}(x,y)\\), chamaremos esse pixel de p. Classificar todos os pixels fracos de \\(g_{NL}(x,y)\\) que tenham conexão, como a conectividade-8, como bordas válidas. Quando todos os pixels de \\(g_{NL}(x,y)\\) Se forem analisados, pulamos para 4, senão voltamos para 1. Zerar todos os pixels de \\(g_{NL}(x,y)\\) que não são bordas válidas. Ao final desses processos teremos a imagem de saída do algoritmo de Canny. Como dito por [2, p. 476], o uso de duas imagens \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) é uma boa maneira para se explicar o algoritmo de uma maneira simples, mas na prática isso pode ser feito diretamente na imagem \\(g_N(x,y)\\). Por fim, sumarizando os passos do algoritmo, com um exemplo: Imagem original Figura 6.2: Imagem original. Aplicação do filtro gaussiano para suavizar a imagem. Figura 6.3: Imagem filtrada com filtro gaussiano. Cálculo da magnitude do gradiente e dos ângulos. Figura 6.4: (a) Sobel na direção vertical. (b) Sobel na direção horizontal. (c) Gradiente. (d) Angulos. [8, p. 98] Aplicação da supressão não máxima para afinar as bordas. Figura 6.5: Resultado da supressão não máxima. Usar limiarização por histerese e análise de conectividade para detectar e conectar as bordas. Figura 6.6: Resultado da histerese e conecção de bordas. Resultado final. Figura 6.7: Resultado final da detecção de bordas de Canny. 6.2 Transformada de Hough A Transformada de Hough é uma técnica utilizada para detectar formas em imagens, sejam elas linhas, círculos ou elipses. Apesar de ela ser muito utilizada e ter sido criada para detecção principalmente de linhas, ela pode ser usada para a detecção de outras formas, como dito anteriormente. 6.2.1 Transformada de Hough para detecção de linhas Para começar a entender essa transformada, imaginemos que temos um ponto \\((x_i, y_i)\\) no plano \\(xy\\) e a equação da reta \\(y_i=ax_i+b\\). Pelo ponto \\((x_i, y_i)\\) passam infinitas retas e todas satisfazem a equação. Podemos escrever a equação anterior em relação a \\(b\\), ou seja, \\(b=-x_ia+y_i\\), o que nos leva ao plano \\(ab\\)(espaço de parâmetros) onde essa nova equação gerará uma única reta. Agora imaginemos um outro ponto \\((x_j, y_j)\\) no plano \\(xy\\), podemos também levá-lo ao plano ab com a equação \\(b=-x_ja+y_j\\). Como podemos ver na figura 6.8(b) as duas retas geradas no plano \\(ab\\) se cruzam nas coordenadas \\((a&#39;, b&#39;)\\), e esse ponto de cruzamento representa a reta que cruza os dois pontos no plano \\(xy\\), como podemos ver na mesma representação 6.8(b). Na realidade, todos os pontos pertencentes a reta definida por esses dois pontos em \\(xy\\) tem sua reta respectiva em \\(ab\\) e todas elas se cruzam no ponto \\((a&#39;, b&#39;)\\), isso nos dá uma maneira de realizar a detecção de bordas, pois podemos imaginar essa reta como nossa borda, assim, para achá-la basta localizar o ponto no espaço de parâmetros onde um grande número de retas se cruzam. Figura 6.8: Plano xy e ab. [2, p. 483] Ocorre um pequeno problema nessa forma, pois quando a reta se aproxima da direção vertical, \\(ab\\) se aproxima do infinito. Para resolver essa dificuldade, em vez de levarmos os pontos a retas no espaço \\(ab\\) cartesiano utilizamos um espaço em coordenadas polares. Para isso utilizamos a seguinte equação: \\[\\rho=x\\cos{\\theta}+y\\ sen{\\ \\theta}\\] Na figura 6.9(a) podemos ver isso de maneira gráfica, temos que p corresponde à distância da origem até a reta. Cada uma das curvas senoidais da figura 6.9(b) representa um conjunto de linhas que cruzam os dois pontos da figura 6.9(a), sendo que na interseção das curvas temos a reta que cruza esses pontos. Figura 6.9: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] A figura 6.9(c) mostra como fazemos a representação do espaço , usamos uma matriz onde esse espaço é subdividido em várias células, chamadas células acumuladoras. Os valores de \\(\\theta_{\\text{min}}\\) e \\(\\theta_{\\text{max}}\\) são geralmente \\(-90^{\\circ}\\leq \\theta\\leq90^{\\circ}\\) e os valores de \\(\\rho_min\\) e \\(\\rho_max\\) são \\(-D\\leq\\rho\\leq D\\), onde \\(D\\) é o comprimento da diagonal da imagem, ou seja, \\(D=\\sqrt{vertical^2+horizontal^2}\\). O que fazemos então é andar por todos os pontos de borda da imagem de entrada e calcular o valor de a partir da equação apresentada anteriormente usando o valor de \\((x,y)\\) e variando o ângulo , com isso a cada valor do ângulo teremos um diferente e somamos mais um na célula correspondente da matriz acumuladora, que inicialmente é toda preenchida com zeros. Ao final de todo o processo termos determinadas células com valores mais altos, essas são conhecidas como picos e correspondem ao cruzamento de duas ou mais curvas senoidais do plano o que corresponde a uma linha ligando pontos no plano \\(xy\\). A seguir temos um exemplo, que nos ajuda a entender e ver na prática o funcionamento da transformada de Hough. A figura 6.10(a) contém uma imagem de tamanho 101x101 com um ponto no centro, ou seja \\((x,y)=(50,50)\\) e a figura 6.10(b) contém a matriz acumuladora da transformada, onde podemos ver a curva senóide formada pelo ponto. Verificando os valores nela vemos que para \\(\\rho=-90^{\\circ}\\) temos: \\[\\rho=50 \\cdot \\cos(-90^{\\circ})+50\\cdot\\text{sen}(-90^{\\circ}) = -50\\] para \\(\\theta=90^{\\circ}\\) temos: \\[\\rho=50\\cdot \\cos(90^{\\circ})+50\\cdot\\text{sen}(90^{\\circ})=50\\] para \\(\\theta=45^{\\circ}\\) temos: \\[\\rho=50\\cdot \\cos(45^{\\circ})+50\\cdot \\text{sen}(45^{\\circ})\\approx70,71\\] e para \\(\\theta=-45^{\\circ}\\) temos: \\[\\rho=50\\cdot\\cos(-45º)+50\\cdot \\text{sen}(-45º)=0\\] Figura 6.10: Transformada de Hough para um ponto. Na figura 6.11(a) temos dois pontos, a e b, onde foi realizada a transformada de Hough que tem como espaço de saída a figura 6.11(b). A reta que passa por esses dois pontos, chamada de reta c é representada por uma reta pontilhada na figura 6.11(a) e na figura 6.11(b) temos o ponto no plano que representa essa reta, ou seja, uma reta a uma distância \\(\\rho\\approx70,71\\) da origem com ângulo de \\(45^{\\circ}\\). Figura 6.11: Transformada de Hough para um ponto em 45º. Na figura 6.12(a) temos mais um exemplo, desta vez com um ponto localizado a sua direita, diferentemente da anterior, esses dois pontos formam uma reta de \\(-45^{\\circ}\\), fato que pode ser visto na figura 6.12(b) onde o ponto de encontro das duas curvas acontece em \\(\\theta=-45^{\\circ}\\) com um valor de \\(\\rho=0\\) já que a reta cruza a origem, ou seja, não possui distância em relação a ela. Figura 6.12: Transformada de Hough para um ponto em -45º. Nosso último exemplo contém uma imagem com três pontos, onde temos três tipos de retas possíveis. Observando a figura 6.13(a) podemos ver os pontos a, b e c e as retas que passam por eles d, e e f, e na figura 6.13(b) temos a transformada de Hough para esse imagem, algo interessante de se notar é o fato de a reta que passa pelos pontos b e c ser detectada duas vezes, isso se deve a uma característica da transformada de Hough chamada relação de adjacência reflexiva, ou seja, isso acontece como resultado pela maneira como \\(\\rho\\) e \\(\\theta\\) mudam de sinal quando chegamos as extremidades de \\(\\pm90^{\\circ}\\). Figura 6.13: Transformada de Hough para três pontos. Na figura 6.14 temos nosso último exemplo na detecção de linhas, dessa vez realizado em uma imagem real, neste caso primeiramente foi realizado a detecção de bordas pelo método de Canny, como pode ser visto na figura 6.14(a). Logo após foi realizada a transformação de Hough, com resultado em figura 6.14(b) e por fim temos a imagem original com as linhas detectadas em figura 6.14(c). Atenção ao fato de que nem todos os picos da transformada podem ser utilizados como linhas, pois teríamos um número enorme delas, para isso utilizamos um threshold, utilizando somente as linhas que tiverem o número de votos(acumulação na matriz) superior a um valor limítrofe. Figura 6.14: Resultado da transformada de Hough usada na detecção de linhas em uma imagem. 6.2.2 Transformada de Hough para detecção de círculos A transformada de Hough pode ser estendida para detecção de círculos, para isso substituímos a equação da reta pela equação do círculo: \\[(x-x_0)^2+(y-y_0)^2=r^2\\] Nesse caso também andamos por cada pixel das bordas da imagem e o levamos ao espaço de parâmetro com as seguintes equações: \\[x_0=x-r\\cos(\\theta)\\] e \\[y_0=y-\\text{sen}(\\theta)\\] A diferença é que neste caso o nosso espaço de parâmetro terá três dimensões, isso decorre do fato de que como desenhamos um círculo para cada pixel do círculo da imagem, a variação do diâmetro desse círculo deve levar a uma variação dos círculos descritos no espaço de parâmetros, então além da variação dos valores de \\(x\\) e \\(y\\) também devemos variar os valores de \\(r\\). Uma representação disso pode ser vista na figura 6.15(a) onde temos três pixels que definem um círculo, na figura 6.15(b) temos os círculos no espaço de parâmetros e na figura 6.15(c) podemos ver um representação de um espaço de parâmetros com diferentes raios. Figura 6.15: Transformada de Hough para círculos.[19, p. 255] A figura 6.16 contém uma imagem com algumas moedas, na figura 6.17(a) temos as bordas da imagem detectada com o método de Canny, logo após, na figura 6.17(b) - (f) temos a representação do espaço de Hough para diferentes valores de raio. E na figura 6.18 temos o resultado da detecção de círculo após encontrados os picos do espaço de parâmetros. Figura 6.16: Imagem original de moedas. [8, p. 98] Figura 6.17: Canny e espaço de parâmetros. [8, p. 98] Figura 6.18: Resultado final da transformada de Hough para círculos. [8, p. 98] 6.3 Detecção de Quinas Quinas são pontos chaves na visão computacional por serem muito úteis na descrição e correspondência de objetos usando poucos dados. E para identificação dos pontos, existem diferentes métodos, dentre eles o mais comum é o de Harris, que é o sucessor do de Moravec [19, p. 178]. 6.3.1 Detector de Quinas de Moravec Moravec obtém sua medida de curvatura através de uma variação média de intensidade em quatro direções principais: \\((0,1), (0,-1), (1,0)\\) e \\((-1,0)\\). Isso é feito através da seguinte equação, considerando a análise sobre o \\(pixel(x,y)\\), o deslocamento \\((u,v)\\) e a janela \\(2w+1\\) [19, p. 185]. \\[E_{u,v}(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}[P_{x+i,\\ y+j} - P_{x+i+u,\\ y+j+v}]^2\\] Essa equação também aproxima a função de autocorrelação na direção \\((u,v)\\) [19, p. 186]. O detector de Moravec apesar de ser intuitivo seu funcionamento, ele considera apenas um pequeno conjunto de mudanças possíveis. Então, Harris propôs ainda avaliar a autocorrelação, mas por uma expressão analítica [19, p. 185]. 6.3.2 Detector de Quinas de Harris O detector de Harris é desenvolvido na ideia de Moravec e sua equação, mas com uma abordagem mais complexa. Harris assume que \\(P_{x+i+u,\\ y+j+v}\\) possa ser estimado pela série de Taylor de primeira ordem [19, p. 193]. Dessa forma, \\[P_{x+i+u,\\ y+j+v} = P_{x+i,\\ y+j} + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial x}u + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial y}v \\] Substituindo na equação de Moravec \\[E_{u,v}(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}[\\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial x}u + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial y}v]^2\\] E expandindo a potência \\[E_{u,v}(x,y) = A(x,y)u^2 + 2C(x,y)uv + B(x,y)v^2\\] Esta última equação pode ser representada forma de matriz. Representação útil para compreensão mais à frente neste tópico [19, p. 187]. \\[ \\begin{split} E_{u,v}(x,y) &amp;= \\begin{bmatrix}u &amp; v\\end{bmatrix} \\begin{bmatrix}A(x,y) &amp; C(x,y)\\\\ C(x,y) &amp; B(x,y)\\end{bmatrix} \\begin{bmatrix}u \\\\ v\\end{bmatrix} \\\\&amp;= D^TMD \\end{split} \\] onde \\[A(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}(\\frac{\\partial P_{x+i, y+j}}{\\partial x})^2\\] \\[B(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w} (\\frac{\\partial P_{x+i, y+j}}{\\partial y})^2\\] \\[C(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w} (\\frac{\\partial P_{x+i, y+j}}{\\partial x})(\\frac{\\partial P_{x+i, y+j}}{\\partial y})\\] Como \\(E_{u.v}(x,y)\\) tem a forma de uma função quadrática, então possui dois eixos principais. Podemos rotacioná-la a fim de alinhar seus eixos com os do sistema de coordenadas, obtendo \\(F_{u,v}(x,y)\\) [19, p. 187]. \\[F_{u,v}(x,y) = \\alpha(x,y)^2u^2 + \\beta(x,y)^2v^2\\] Ou em sua forma matricial. Note que são rotacionados os eixos definidos pelo \\(D\\) \\[F_{u,v}(x,y) = R^TD^TMDR\\] \\[F_{u,v}(x,y) = D^TR^TMRD\\] \\[F_{u,v}(x,y) = D^TQD\\] \\[Q = \\begin{bmatrix} \\alpha &amp; 0\\\\ 0 &amp; \\beta \\end{bmatrix}\\] Os valores de \\(\\alpha\\) e \\(\\beta\\) são proporcionais à função de autocorrelação nos principais eixos. Dessa forma, \\(\\alpha\\) e \\(\\beta\\) serão pequenos se o \\(pixel(x,y)\\) for de uma região com intensidade constante, um será de valor grande e outro pequeno se estiverem em uma borda reta, e ambos terão valores grandes se estiverem em uma borda com curvatura acentuada. Portanto, a medida de curvatura é definida como \\(k_k(x,y)\\) [19, p. 187]. \\[k_k(x,y) = \\alpha \\beta - k(\\alpha + \\beta)^2\\] No qual \\(k\\) controla a sensibilidade do detector. Como \\(Q\\) é uma composição ortogonal de \\(M\\). Os elementos de Q são chamados de autovalores [19, p. 188]. Inferimos que \\[Q = R^TMR\\] Então, a partir da equivalência de determinantes e traços, é possível produzir uma equação equivalente a \\(Y\\) com os valores da matriz \\(M\\) [19, p. 188]. \\[\\alpha \\beta = A(x,y)B(x,y) - C(x,y)^2\\] \\[\\alpha + \\beta = A(x,y) + B(x,y)\\] Assim \\[ \\begin{split} k_k(x,y) &amp;= \\alpha \\beta - k(\\alpha + \\beta)^2 \\\\&amp;= A(x,y)B(x,y) - C(x,y)^2 - k(A(x,y) + B(x,y))^2 \\\\&amp;=det(M) - k(trace(M))^2 \\end{split} \\] A Figura 6.19 (a) é a imagem original. A Figura 6.19 (b) foi gerada usando o detector de Harris com uma vizinhança 5x5 (\\(w=2\\)) para cada deslocamento \\((u,v)\\), com a derivada sendo calculada pelo Operador de Sobel (3x3) e com sensibilizador \\(k=0.01\\). Limiarizou-se a imagem de curvatura, descartando os valores que não fossem maiores que 9% do valor máximo. E nas posições \\((x,y)\\) da imagem que continham as curvaturas, foi destacado em rosa. Observe que a imagem identificou as quinas do tabuleiro de xadrez e do cubo mágico, porém não detectou outras quinas como as das árvores. Além disso, foi encontrado quinas que não são próprias dos objetos, e sim da iluminação. Variando tanto o sensibilizador da função, \\(k\\), como o limiar é provável que consigamos encontrar mais quinas, com o custo de também poder classificar ruídos que foram identificados como bordas também como quinas. Entretanto, já vimos que o filtro gaussiano pode ser que nos ajude neste problema. Figura 6.19: Exemplo de detecção de Quinas pelo método de Harris. 6.4 Detecção de Blobs Blobs, do inglês bolhas, são regiões da imagem em que os pixels têm valores aproximadamente iguais. Uma boa representação - um tanto quanto artificial - disso é a função gaussiana, como pode ser vista na figura 6.20(a) e sua representação 2D na figura 6.20(b), nela temos um conjunto de pixels com valores bem próximos, que caracterizam um blob. Figura 6.20: Função gaussiana em 3D e 2D. Apesar do exemplo, a detecção de blobs não se restringe a elementos circulares, mas a qualquer conjunto de pixels. 6.4.1 LoG Esse método utiliza o do Laplaciano do Gaussiano, que já foi apresentado anteriormente, mas que em resumo é o cálculo de derivadas segunda em uma imagem que foi anteriormente convolucionada com um filtro gaussiano, isso irá gerar fortes respostas positivas em blobs escuros e negativas em blobs escuros nos blobs de tamanho \\(\\sqrt{2\\sigma}\\). Como existe uma relação entre a respostas e o tamanho do desvio padrão, é necessário realizar a operação com uma gama de valores para o sigma, e assim detectar blobs de diferentes tamanhos. Figura 6.21: Imagem de Campo Ultraprofundo do Hubble. [20] Como podemos ver na figura 6.20 com diferentes valores de sigma conseguimos detectar objetos de variados tamanhos, como exemplo na figura 6.22(a) detectamos as estrelas da figura 6.21 que apresentam uma menor resposta ao filtro laplaciano. Figura 6.22: Laplaciano do Gaussiano com diferentes valores de sigma. Na figura 6.23 temos o resultado da detecção dos blobs utilizando LoG. Note que nem todas as estrelas foram detectadas, isso se deve ao fato do uso de um valor de threshold, onde definimos que queremos as detecções acima de determinado limiar. Na figura 6.24 podemos ver o resultado utilizando um valor de limiar menor, onde muito mais objetos foram localizados. Figura 6.23: Resultado da detecção de blobs com LoG. Figura 6.24: Resultado da detecção de blobs com LoG com um threshold menor. 6.4.2 DoG Esse método é basicamente o mesmo do anterior, mas possui uma certa vantagem, que é o fato de ele ser mais eficiente. Como também já foi mencionado no tópico na seção anterior é possível aproximar o Laplaciano do Gaussiano através da Diferença do Gaussiano(DoG), ou seja, primeiramente se realiza a filtragem gaussiano com dois sigmas diferentes e se faz a subtração entre os dois. Realizamos esse processo para diferentes pares de valores, obtendo assim o mesmo espaço de escala construído com o processo do LoG. Na figura 6.25 temos um exemplo de detecção por DoG. Figura 6.25: Resultado da detecção de blobs com DoG. 6.4.3 DoH Uma matriz Hessiana é uma matriz que contém as derivadas de uma função. No nosso caso, utilizamos a Hessiana de ordem 2, pois estamos trabalhando com imagens, que possuem duas dimensões. Ela pode ser representada da seguinte maneira: \\[H[f(x_1, x_2, \\dots,x_n)]= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}\\] A matriz Hessiana tem muita utilidade pois com ela podemos descrever a curvatura em um ponto da função multivariável, o que no nosso caso pode ajudar a detectar os blobs, já que eles são aglomerados de pixels e devem estar separados do restante da imagem, ou seja, um aglomerado claro em um fundo escuro ou o contrário, e isso irá fazer com que sua função tenha uma mudança de sinal que pode ser detectada utilizando-se as informações da matriz. Além disso, como dito por Herbert Bay et al.[21] os detectores baseados na Hessiana são mais estáveis e repetíveis(tem a mesma resposta para a mesma imagem com diferentes ângulos, iluminações etc.). Um dos principais algoritmos que fazem uso dessa matriz se chama Speeded Up Robust Features (SURF)[21], esse método faz uso de várias técnicas que o tornam muito rápido, como seu próprio nome sugere. Uma dessas técnicas é o cálculo da integral da imagem, realizado a partir da soma de todos os pixels de uma área retangular a partir do x atual, sendo que este varia enquanto se é andado pela imagem. Figura 6.26: Integral de uma imagem. [22] Como pode ser visto na figura, a integral de uma imagem contém a soma das regiões, por exemplo, a primeira posição contém a soma de somente uma célula, no caso 1, a segunda tem a soma de duas células, na primeira linha estamos basicamente somando as células de uma só linha, na segunda começamos a formar regiões retangulares, por exemplo, na segunda linha e terceira coluna temos o valor 6, resultante da soma das seis células da primeira linha com a segunda. Com a integral podemos calcular a área de qualquer região com apenas quatro operações, da seguinte forma: \\[soma = D+A-B-C\\] Onde {A,B,C,D} forma uma região. Como exemplo, caso queiramos calcular a área na região quadrada 2x2 na direita inferior utilizados: \\[soma = 9 + 1 - 3 - 3 = 4\\] Isso nos ajuda na aplicação de box filters, já que precisaríamos da soma de determinadas áreas, e com isso aumentamos a velocidade do método. Sendo \\(X=(x,y)\\) um ponto em uma imagem, sua matriz Hessiana em \\(X\\) a uma escala é dada por: \\[H(X,\\sigma)=\\begin{bmatrix} L_{xx}(X, \\sigma) &amp; L_{xy}(X, \\sigma)\\\\ L_{xy}(X, \\sigma) &amp; L_{yy}(X, \\sigma) \\end{bmatrix}\\] Onde \\(L_{xx}(X, \\sigma)\\) é a convolução da imagem no ponto X com a derivada de segunda ordem gaussiana \\(\\frac{\\partial^2g(\\sigma)}{\\partial x^²}\\) e assim por diante[21]. Aqui entra em cena mais um elemento para melhorar a velocidade do algoritmo, Bay, Herbert et al. utilizam box filters para aproximar o filtro gaussiano. Como podemos ver na figura, onde os dois primeiros filtros são os derivativos gaussianos discretizados e os dois últimos são os aproximados a partir de box filters. Figura 6.27: Filtro gaussiano discretizado e aproximado na direção \\(y\\) e \\(xy\\). [21] Chamamos as derivadas realizadas na imagem de \\(D_{xx}\\), \\(D_{yy}\\), \\(D_{xy}\\). Essas derivadas não são realizadas com somente um valor de \\(\\sigma\\), mas como os detectores anteriores usam uma sequência de valores para assim criar um espaço de escalas e conseguir detectar blobs de diferentes tamanhos. Assim, a determinante da Hessiana é dado por: \\[det(H_{\\text{aprox}}) = D_{xx}D_{yy}-(0.9D_{xy})^2\\] Sendo que o valor \\(0.9\\) é um peso introduzido pelos autores Bay, Hebert et al. para corrigir as respostas quando utilizamos várias escalas de sigma e obter uma invariância escalar. Na figura temos o resultado de uma detecção de blobs realizada pela Determinante do Hessiano. Figura 6.28: Resultado da detecção de blobs com DoH. 6.5 Limiarização “a seção anterior, as regiões eram identificadas achando primeiro os segmentos de borda e, em seguida, tentando-se conectá-las com as fronteiras. Nesta seção, discutem-se as técnicas de divisão de imagens diretamente em regiões com base nos valores de intensidade e/ou as propriedades desses valores” [2, p. 486]. “Em virtude de suas propriedades intuitivas, simplicidade de implementação e velocidade computacional, a limiarização de imagens tem uma posição central nas aplicações de segmentação de imagem” [2, p. 486]. É importante salientar que a chance de sucesso da limiarização de intensidade é proporcional à largura e a profundidade do(s) vale(s) que separam os modos (ou classes) do histograma. E os principais fatores que afetam as propriedades do(s) vale(s) são [2, p. 487]: A separação entre picos: quanto mais distantes forem os picos entre si, melhores as possibilidades de separação da imagem Índice de ruído da imagem: os modos ampliam com o aumento do ruído O tamanho relativo dos objetos e do fundo A uniformidade da fonte de iluminação A uniformidade da reflexão da imagem Suponha que os histogramas de intensidade de uma imagem composta por objetos claros sobre um fundo escuro, conforme Figura 6.29 (a), de tal forma que os pixels do objeto e do fundo tenham valores de intensidade agrupados em dois modos ou dois grupos dominantes (Gonzalez; Woods, 2010, p. 486); a idéia é selecionar um limiar \\(T\\) que separa estes modos. E caso tenha três modos, usa-se dois limiares, conforme Figura 6.29 (b). Em outras palavras, a segmentação da imagem da Figura 6.29 (a) é dada por \\(g(x,y)\\)2: \\[g(x,y) = \\begin{cases} 1,\\ se f(x,y) &gt; T \\\\ 0,\\ se f(x,y) \\leq T \\end{cases}\\] Figura 6.29: Histogramas de intensidade que podem ser divididos por um limiar único, (a), e limiares duplos, (b). [2, p. 486] Quando \\(T\\) é uma constante aplicável em uma imagem inteira, o processo é conhecido como limiarização global. Caso \\(T\\) mude ao longo da imagem, usamos o termo limiarização variável. E quando \\(T\\) denotar uma limiarização variável na qual o valor \\(T\\) em qualquer ponto \\((x,y)\\) em uma imagem depende das propriedades de sua vizinhança (por exemplo, a intensidade média dos pixels da vizinhança), o chamamos de limiarização local ou regional3 [2, p. 486]. Os problemas de segmentação que exigem mais do que dois limiares são difíceis (muitas vezes impossíveis) de resolver e os melhores resultados, geralmente, são obtidos por meio de métodos como a limiarização variável ou aumento da região, como discutido [2, p. 486]. O papel do ruído da limiarização O ruído de uma imagem é capaz de fazer com que fique difícil achar um limiar ideal para segmentar a imagem sem processamentos adicionais, pois o(s) vale(s) da imagem podem desaparecer [2]. Observe que conforme os exemplos da Figura 6.30 e seus respectivos histogramas, o aumento no desvio padrão nos níveis de intensidade do ruído gaussiano faz com que o vale que separava os dois modos desapareça, tornando difícil a segmentação do fundo e do objeto. Figura 6.30: (a) Imagem de 8 bits livre de ruído, típica de Computação Gráfica. (b) Imagem com ruído gaussiano aditivo de média 0 e desvio padrão de 10 níveis de intensidade. (c) Imagem com ruído gaussiano aditivo de média 0 e desvio padrão de 50 níveis de intensidade. (d) a (f) Histogramas correspondentes [2, p. 487]. O papel da iluminação e refletância O problema da iluminação é quando não é possível ter uma incidência uniforme da luz, causando um sombreamento. O mesmo efeito acontece quando o problema não é na iluminação, mas nas características da superfície do objeto; pois a iluminação e refletância produzem o mesmo problema. Note que, pela Figura 6.31, o histograma deixou de ser bimodal. Logo para segmentar imagens com problemas de iluminação e refletância não é simples Figura 6.31: (a) Imagem ruidosa. (b) Rampa de intensidade no intervalo [0.2, 0.6]. (c) Produto de (a) e (b). (d) a (f) Histogramas correspondentes [2, p. 488]. E como solução há três abordagens básicas. Corrigir diretamente o padrão de sombreamento através de uma multiplicação com o comportamento inverso do sombreamento. Por exemplo, a iluminação não uniforme, porém fixa, pode ser corrigida multiplicando a imagem pelo inverso do padrão de iluminação, que pode ser obtida na aquisição de uma imagem de uma superfície plana de intensidade constante. Outra maneira é corrigí-lo por meio do processamento, por exemplo, utilizando a transformada top-hat. E a terceira abordagem é a de contornar isso utilizando a limiarização variável [2, p. 488]. 6.5.1 Limiarização global simples A limiarização global simples é um método iterativo básico e que não é o mais eficiente. Ele é um processo iterativo que denomina o limiar ideal como aquele que produz menor diferença entre as médias de intensidade dos modos, o de segmentação e o desprezado [2, p. 488]. Ele consiste em: Selecionar uma estimativa inicial para o limiar global, \\(T\\). Segmentar a imagem usando \\(T\\). Isso dará origem a dois grupos de pixels: \\(G_{1}\\) , composto por todos os pixels com valores de intensidade \\(&gt; T\\), e \\(G_{2}\\), composto pelos pixels com valores \\(\\leq T\\). Calcular os valores de intensidade média dos grupos, \\(m_{1}\\) e \\(m_{2}\\). Calcular um novo valor de limiar: \\(T = \\frac{m_{1}+m_{2}}{2}\\). Repita as etapas 2 a 4 até que a diferença entre os valores de \\(T\\) das iterações sucessivas seja menor que o parâmetro predefinido \\(\\Delta T\\). Exemplo de Limiarização Global: A Figura 6.32 (a) consiste na imagem de uma digital com ruído. A Figura 6.32 (b) mostra que seu histograma possui um vale bem nítido e pela aplicação do algoritmo, usando \\(\\Delta T = 0\\) e iniciando \\(T\\) igual a média de intensidade da imagem, após três iterações, encontramos o limiar \\(T = 125.4\\). A Figura 6.32 (c) mostra o resultado obtido como \\(T = 125\\). Como esperado, a partir da separação clara entre os modos no histograma, segmentação entre o objeto e o fundo foi bastante eficaz. Figura 6.32: (a) Impressão digital ruidosa. (b) Histograma. (c) Segmentação resultante usando um limiar global. [2, p. 489]. 6.5.2 Limiarização pelo Método de Otsu O método de Otsu é uma abordagem que relaciona as informações do histograma com conceitos estatísticos para produzir o chamado limiar ótimo, que é denotado por aquele que maximiza a variância entre classes ou minimiza a variância intraclasse [2, p. 489]. Lembrando que classe é o mesmo que modos do histograma. O primeiro passo é obter o histograma da imagem normalizado, isto é, no qual os pesos de cada intensidade são a probabilidade da ocorrência daquela intensidade na imagem. Segue a equação abaixo que representa um histograma normalizado, no qual \\(L\\) representa a quantidade de níveis de intensidade e \\(p_{i}\\), a probabilidade de ocorrência da intensidade \\(i\\) na imagem [2, p. 490]. \\[\\sum_{i=0}^{L-1}{p_{i} = 1,\\ p_{i} \\geq 0}\\] Para entender a equação cerne de Otsu, é preciso compreender algumas equações que a compõe. A probabilidade de ocorrência do modo 1 é dada pela equação abaixo [2, p. 490] \\[P_{1}(k) = \\sum_{i=0}^{k}{p_{i}}\\] E o valor da intensidade média dos pixels da classe 1, \\(C_{1}\\), para dado limiar \\(k\\) pode ser calculado pela equação abaixo [2, p. 490] \\[\\begin{split} m_{1}(k) &amp; = \\sum_{i=0}^{k}{iP(i/C_{1})}\\\\ &amp; = \\sum_{i=0}^{k}{iP(C_{1}/i)P(i)/P(C_{i})}\\\\ &amp; = \\frac{1}{P_{1}(k)}\\sum_{i=0}^{k}{ip_{i}}\\\\ \\end{split}\\] E a média acumulada (intensidade média) até o nível \\(k\\) ou da classe \\(C_{1}\\) é dada por [2, p. 490] \\[m(k) = \\sum_{i=0}^{k}{ip_{i}}\\] Entendidas as equações anteriores, chegamos a equação cerne, que denota a variância entre classes. \\[\\sigma_B^2(k) = \\frac{[m_GP_{1}(k) - m(k)]^{2}}{P_1(k)[1 - P_1(k)]}\\] Então, o limiar ótimo é o valor que maximiza a variância entre classes, denominado \\(k^*\\) [2, p. 491], representado na equação abaixo. E conforme informado anteriormente, esse resultado é o mesmo que minimiza a variância dentro das classes; isso se deve a uma propriedade estatística que relaciona a variância global de intensidade da imagem, a variância interclasse e a variância intraclasse. \\[\\sigma_B^2(k^*) = \\max_{0 \\leq k \\leq L-1} \\sigma_B^2(k)\\] Se o máximo existir para mais de um valor de \\(k\\), é habitual calcular a média dos valores de \\(k\\) [2, p. 491]. Uma métrica adimensional pode ser usada para obter uma estimativa quantitativa da separabilidade das classes, o que dá uma idéia da facilidade e do resultado da segmentação. \\[\\eta = \\frac{\\sigma_B^2(k^*)}{\\sigma_G^2}\\] Tendo o limiar ótimo, \\(k^*\\), segmentamos a imagem como já visto. Para o cálculo da métrica adimensional, é preciso conhecer a variância global das intensidades da imagem que pode ser obtida pela seguinte equação: \\[\\sigma_G^2 = \\sum_{i=0}^{L-1}{(i - m_G)}^2p_i\\] Resumo do algoritmo de Otsu [2, p. 492]: Calcular o histograma normalizado da imagem de entrada. Designar os componentes do histograma como \\(p_i, i = 0, 1, 2, ..., L-1\\). Calcular as somas acumuladas, \\(P_1(k)\\), para \\(k = 0, 1, 2, ..., L-1\\). Calcular as médias acumuladas \\(m(k)\\), para \\(k = 0, 1, 2, ..., L-1\\). Calcular a intensidade média global, \\(m_G\\). Calcular a variância entre classes, \\(\\sigma_B^2(k)\\), para \\(k = 0, 1, 2, ..., L-1\\). Obter o limiar ideal de Otsu, \\(k^*\\), caso haja mais de um, faz-se a média dos valores. Obter a medida de separabilidade, \\(\\eta^*\\), a fim de estimar a qualidade da segmentação. A Figura 6.33 (a) mostra uma imagem de microscópio ótico de células polimerosomas e a Figura 6.33 (b), seu histograma. O objetivo deste exemplo é segmentar as moléculas do fundo. A Figura 6.33 (c) é o resultado pela limiarização global simples. Como o histograma não tem vales distintos e a diferença de intensidade entre o fundo e os objetos é pequena, o algoritmo não conseguiu alcançar a segmentação desejada. A Figura 6.33 (d) mostra o resultado obtido pelo método de Otsu. Esse resultado, obviamente, é superior ao da Figura 6.33 (c). O valor do limiar calculado pelo algoritmo simples foi o de 169, enquanto o limiar calculado pelo método de Otsu era o de 181, que está mais próximo das áreas mais claras na imagem que define as células. A medida de separabilidade foi 0.467. Figura 6.33: (a) Imagem original. (b) Histograma (os picos elevados foram cortados para realçar os detalhes nos valores mais baixos). (c) Resultado da segmentação pela limiarização global simples. (d) Resultado da segmentação pelo método de Otsu. [2, p. 492]. 6.5.3 Uso de suavização para limiarização O objetivo da suavização é tentar separar os histogramas de imagens ruidosas, que tendem a ser unimodais, em modos com vales mais profundos; pois, quanto mais profundo o vale, melhor será a segmentação da imagem. Atente-se ao tipo de média e ao tamanho do kernel, aconselha-se o filtro gaussiano, pois ele minimiza o borramento de fronteira, e suaviza o ruído ainda que de maneira mais branda do que um filtro de média. A Figura 6.34 (a) mostra uma imagem ruidosa, a 6.34 (b) mostra seu histograma, a Figura 6.34 (c) mostra o resultado do método de Otsu. Já a Figura 6.34 (d) mostra a imagem de (a) suavizada usando uma máscara de média de tamanho 5x5 e a Figura 6.34 (e) é seu histograma e a Figura 6.34 (f) é resultado da limiarização pelo método de Otsu. Figura 6.34: Exemplo de suavização antes da aplicação do método de Otsu [2, p. 493]. Apesar do filtro de média poder nos ajudar, nem sempre será capaz disso. A Figura 6.35 (a) mostra uma imagem ruidosa e a 6.35 (b) mostra o seu histograma, observe que o pontinho branco parece nem estar presente no histograma. E após aplicado o método de Otsu, a 6.35 (c), observe que não foi obtida a segmentação desejada. Então, tentou-se um filtro de média 5x5, que reduz o ruído, Figura 6.35 (d). O resultado no histograma foi a redução do espalhamento do histograma, Figura 6.35 (e), mas a distribuição ainda é unimodal, resultando em falha na segmentação, o que é visto na Figura 6.35 (f). Figura 6.35: Exemplo de insucesso na segmentação por Otsu, mesmo com prévia suavização. Portanto, note que, se a região que deseja segmentar for muito pequena em relação ao background e houver ruído, o que pode surgir na captura da imagem, a chance de não dar certo pelos métodos vistos é grande; pois como os métodos que até agora vimos operam apenas no histograma da imagem, sem uso de maiores recursos. Como visto, imagens com essa característica, um mínimo ruído persiste e nem foi obtido um vale considerável entre as duas regiões. Isso pode ser atribuído ao fato de que a região é tão pequena que sua contribuição para o histograma é insignificante em comparação à intensidade da propagação causada pelo ruído [2, p. 493]. A solução para isso é o uso de máscaras de borda, que será detalhado a seguir. 6.5.4 Uso de bordas para limiarização Em uma imagem com ruído na qual a região a ser segmentada é muito pequena, é como se não houvesse aquela região e houvesse apenas o background. Isso é observado na aparência unimodal do histograma. Portanto, fica difícil estimar um limiar ideal pelos algoritmos supracitados. E como visto anteriormente, é preciso uma aparência bimodal para uma boa segmentação, então, precisamos de um histograma equilibrado; para isso, tomamos o histograma das bordas mais destacadas da imagem. Isso pode ser resumido em gerar uma máscara de gradiente ou laplaciano da imagem, limiarizá-la com um valor alto e usar como máscara para imagem original e prosseguir com o processo de segmentação do objeto a partir dessa amostra, pois, dessa forma, é gerado um histograma simétrico e com um vale destacado, porque, com a máscara de borda, há probabilidade de um píxel estar no background ou foreground tende a ser equilibrada [2, p. 494]. O que se espera com os tipos de máscaras de borda, conforme visto no estudo detecção de bordas, é que a de gradiente produzirá bordas mais grossas e menor detecção aos ruídos da imagem, e a de laplace, bordas mais finas e maior detecção de ruídos, além de apresentar melhor custo computacional. Entretanto, é possível modificar este algoritmo para que tanto a magnitude do gradiente quanto o valor absoluto das imagens laplacianas sejam utilizadas; nesse caso, poderíamos especificar um limiar para cada imagem e formar a lógica OU dos dois resultados para obter a imagem marcadora, essa abordagem é útil quando se deseja ter mais controle sobre os pontos que foram considerados como sendo pontos válidos de borda [2, p. 494]. Resumo das etapas de segmentação pela identificação de bordas do objeto [2, p. 494]: Calcular uma imagem de borda da imagem capturada, \\(f(x,y)\\), ora como a magnitude do gradiente, ora como o valor absoluto do laplaciano, usando qualquer um dos métodos. Especificar um valor de limiar, \\(T\\). Limiarizar a imagem a partir da Etapa 1, utilizando o limiar estabelecido na Etapa 2 para produzir uma imagem binária, \\(g_{T}(x,y)\\). Esta imagem é usada como uma imagem de máscara na etapa seguinte para selecionar os pixels de \\(f(x,y)\\) que correspondem aos pixels “fortes” da borda. Calcular um histograma utilizando apenas os pixels de \\(f(x,y)\\), que correspondem aos endereços de pixel avaliados com o número 1 em \\(g_{T}(x,y)\\). Use o histograma da Etapa 4 para segmentar \\(f(x,y)\\) globalmente, utilizando, por exemplo, o método de Otsu. A Figura 6.36 (a) e (b) mostram as mesmas imagens da Figura 6.35 e seu histograma. Vimos que essa imagem não adianta ser suavizada. Entretanto, usamos a estratégia de máscara de borda que obteve um ótimo resultado. A Figura 6.36 (c) mostra o gradiente já limiarizado, A Figura 6.36 (d) e (e) mostra a máscara multiplicada a imagem original, que tem um histograma mais relevante à segmentação. E a Figura 6.36 (f) mostra o resultado da segmentação pelo novo histograma, 6.36 (e), através do Método de Otsu. O limiar foi de \\(134\\), que fica aproximadamente a meio caminho entre os picos no histograma. Figura 6.36: Exemplo de limiarização por meio de máscara de borda de gradiente. Já a Figura 6.37 (a) e (b) mostra uma imagem de 8 bits de células de levedura e seu histograma. A tentativa em detectar em segmentar os pontos claros pelo método de Otsu sem prévia etapa não foi sucedida, embora o método seja capaz de isolar algumas das regiões das células muitas da regiões segmentadas à direita não estão separadas. O limiar calculado foi de \\(42\\) e a medida de separabilidade foi de \\(0.636\\). A Figura 6.37 (d) mostra a imagem \\(g_T(x,y)\\) obtida pelo cálculo do valor absoluto da imagem laplaciana e a limiarização com \\(T\\) definido a \\(115\\) em uma escala de intensidade no intervalo \\([0, 255]\\). Este valor de \\(T\\) corresponde aproximadamente ao percentil \\(99.5\\) dos valores da imagem laplaciana absoluta; assim, a limiarização a este nível deve resultar em um conjunto de pixels reduzido, como mostra esta Figura. A Figura 6.37 (e) é o histograma dos pixels diferentes a zero no produto de (a) e (d). Finalmente a Figura 6.37 (f) mostra o resultado da segmentação global da imagem original utilizando o método de Otsu baseado no histograma da Figura 6.37 (e). Este resultado está de acordo com as localizações dos pontos claros na imagem. O limiar calculado pelo método de Otsu foi \\(115\\) e a medida de separabilidade foi de \\(0.762\\), sendo que ambos são superiores aos valores obtidos utilizando o histograma original [2, p. 495]. Figura 6.37: Exemplo de limiarização por meio de máscara de borda laplaciana. 6.5.5 Limiares Múltiplos A diferença entre os limiares múltiplos e o que vimos até agora é que se usa mais de um limiar para segmentar a imagem a fim de produzir uma melhor medida de separabilidade entre as classes, por conseguinte, melhor segmentação. Entretanto, como as aplicações que requerem mais de dois limiares geralmente são resolvidas com mais do que apenas valores de intensidade. Ao invés disso, o caminho é usar descritores adicionais (por exemplo, cor) e o problema é moldado para reconhecimento de padrões, como explicado a seguir em Limiarização baseada em diversas variáveis [2, p. 497]. No caso das classes \\(K, C_1, C_2, ..., C_K\\), a variância entre classes se generaliza pela expressão \\[\\sigma_{B}^{2} = \\sum_{k=1}^K{P_k(m_k-m_G)^2}\\] na qual \\[P_k = \\sum_{i\\in C_k}{p_i}\\] \\[m_k = \\frac{1}{P_k} \\sum_{i \\in C_k}{ip_i} \\] As classes \\(K\\) são separadas por \\(K-1\\) limiares cujos valores, \\(k^*_1, k^*_2, ..., k^*_k-1\\) \\[ \\sigma_{B}^{2}(k^*_1, k^*_2, ..., k^*_{K-1}) = \\max_{0&lt;k_1&lt;k_2&lt;...k_{n-1}&lt;L-1}{\\sigma_{B}^{2}(k_1, k_2, ..., k_{K-1})} \\] Como observado na equação anterior, o valor máximo é obtido testando todas as possibilidades de valores para cada limiar, mas lembre-se que não faz sentido assumir limiares para \\(0\\) e \\(L-1\\), pois são os extremos da faixa de intensidade. Também pode ser feito a avaliação de sua medida de separabilidade. Como exemplo, tomemos um histograma com três classes [2, p. 497]. \\[\\eta(k^*_1, k^*_2) = \\frac{\\sigma^2_{B}(k^*_1,k^*_2)}{\\sigma^2_{G}}\\] A Figura 6.38 (a) mostra a imagem de um iceberg. É notório que será possível dividí-la com dois limiares4 a partir das predominâncias de três grupos de intensidade. Olhando no histograma, 6.38 (b), pelos vales bem destacados também é observado isso. Encontra-se pelo método de Otsu dois limiares, \\(80\\) e \\(177\\), com uma excelente medida de separabilidade de \\(0.954\\). Limiarizando a Figura 6.38 (a) o resultado que se obtém é uma segmentação muito boa, Figura 6.38 (c) [2, p. 498]. Figura 6.38: (a) Imagem de um iceberg. (b) Histograma. (c) Imagem segmentada em três regiões usando os limiares duplos de Otsu. 6.5.6 Limiarização variável Vimos perante seções anteriores, que fatores como ruído e iluminação são impecílios para uma boa segmentação. Também foi visto que suavização e informações das bordas podem ser usadas para resolver isto. No entanto, é frequente o caso que essas estratégias são ineficientes ou nem possíveis. Como solução para tal, usamos limiares variáveis. 6.5.7 Particionamento da imagem O particionamento da imagem consiste em fracionar a imagem em retângulos suficientemente pequenos de maneira que eles tenham iluminação e refletância uniformes e aplicar o método de Otsu em cada um deles. O sucesso do método é análogo ao da máscara de bordas, ele produz, para cada fração, histogramas simétricos com vales profundos [2, p. 498]. A Figura 6.39 (a) e (b) mostra uma imagem e seu histograma. Pelo seu histograma é plausível que não resultaria em uma boa segmentação, seja pelo método de Otsu, Figura 6.39 (c) ou pelo método iterativo, Figura 6.39 (d). Após fracionada a imagem, Figura 6.39 (e), a segmentação por Otsu teve sucesso, Figura 6.39 (f). [2, p. 498] Figura 6.39: Exemplo da técnica de particionamento da imagem. Figura 6.40: Representa o histograma das subimagens da Figura 6.39 (e). 6.5.8 Limiarização variável baseada nas propriedades locais da imagem É uma técnica em que se calcula um limiar para cada ponto, \\((x,y)\\), com base em uma ou mais propriedades calculadas em sua vizinhança. Apesar de parecer trabalhoso, os algoritmos e hardwares modernos permitem o processamento rápido da vizinhança, especialmente para as funções comuns, como as operações lógicas e aritméticas [2, p. 499]. Utilizaremos como abordagem básica duas propriedades, \\(\\sigma_{xy}(x,y)\\) e \\(m_{xy}(x,y)\\), já que indicam o grau de contraste e intensidade média na vizinhança. Seguem cálculos da limiarização usando apenas a intensidade do ponto, sendo \\(T_{xy}\\), o limiar local. As equações seguintes são formas comuns de limiares variáveis locais [2, p. 499]: \\[T_{xy} = a\\sigma_{xy} + bm_{xy}\\] em que \\(a\\) e \\(b\\) são constantes não negativas, e \\[T_{xy} = a\\sigma_{xy} + bm_{G}\\] , na qual \\(m_G\\) é a média global da imagem. Também pode ser usado predicados a fim de determinar o limiar, \\(T_{xy}\\), de segmentação. No entanto, o preço dessa limiarização mais rebuscada é um aumento no custo computacional [2, p. 499]. E a imagem \\(g_{xy}\\), ficaria conforme um exemplo a seguir: \\[g(x,y) = \\begin{cases} 1,\\ se\\ f(x,y) &gt; a\\sigma_{xy}\\ \\ E\\ \\ f(x,y) &gt; bm_{xy}\\\\ 0,\\ caso\\ contrário \\end{cases} \\] A Figura 6.41 (a) é a imagem de células de levedura da Figura anterior 6.37 (a). A Figura 6.41 (b) é um exemplo da segmentação da Figura 6.41 (a) com dois limiares. Entretanto, note que as células do canto superior direito foram segmentadas de forma unida. A Figura 6.41 (c) é a imagem dos desvios padrão locais da vizinhança de tamanho 3x3 de cada píxel. E foi escolhida a média global ao invés da local, pois geralmente produz melhores resultados quando o fundo é quase constante e todas as intensidades de objeto estão acima ou abaixo da intensidade do fundo. Os pesos \\(a=30\\) e \\(b=1,5\\) foram assumidos. E, por fim, foi limiarizada pelo predicado exemplificado na última equação e não pela intensidade de um ponto, Figura 6.41 (d). Figura 6.41: Exemplo de limiarização variável baseada nas propriedades locais [2, p. 500]. 6.5.9 Usando média de movimento O método de médias móveis é usado geralmente quando os objetos de interesse são pequenos (ou finos) em relação ao tamanho da imagem, uma condição que as imagens de texto digitado ou manuscrito possuem [2, p. 501]. “Essa aplicação é muito útil no processamento de documentos” [2, p. 500]. O procedimento consiste em um kernel 1D que percorre a imagem linha por linha e calcula média móvel com base em um intervalo de um dado tamanho fixo. A regra inicial é usar um intervalo de tamanho 5 vezes maior que a largura média do objeto que deseja limiarizar [2, p. 501]. Digamos que \\(z_{k+1}\\) denota a intensidade do ponto encontrado na sequência de digitalização na Etapa \\(k+1\\). A média móvel (intensidade média) com este novo ponto é dada por \\[\\begin{split} m(k+1) &amp; = \\frac{1}{n} \\sum_{i = k+2-n}^{k+1}{z_i}\\\\ &amp; = m(k) + \\frac{1}{n}(z_{k+1} - z_{k-n}) \\end{split}\\] na qual \\(n\\) é o tamanho do intervalo ou número de pixels utilizados no cálculo da média e \\(m(1)=\\frac{z1}{n}\\). Este valor inicial não é rigorosamente correto porque a média de um único ponto é o valor do ponto em si. No entanto, o usamos para que cálculos especiais não sejam necessários quando é executada pela primeira vez. Já que a média móvel é calculada para cada ponto da imagem, a segmentação é baseada no limiar \\(T_{xy}=bm_{xy}\\), em que \\(b\\) é constante e \\(m_{xy}\\) é a média móvel no ponto \\((x,y)\\) na imagem de entrada [2, p. 500]. A diferença desse método ao explicado na seção anterior é que neste usasse um kernel 1D que avalia linha por linha a imagem. Na Figura 6.42 (a) mostra uma imagem de texto escrito à mão sombreada por um padrão de intensidade. Esta forma de sombreamento de intensidade é típica de imagens obtidas com um flash fotográfico. A Figura 6.42 (b) é o resultado da segmentação pela limiarização global de Otsu. A Figura 6.42 (b) mostra uma segmentação bem sucedida com limiarização local usando médias móveis, usando \\(n=20\\), já que a largura média do traço era de \\(4\\) pixels, e \\(b=0.5\\). Figura 6.42: Exemplo de aplicação da limiarização por médias móveis em um documento corrompido por um sombreamento típico de flash fotográfico. (a) Imagem original. (b) Aplicado método de Otsu. (c) Aplicado método de médias móveis. [2, p. 501]. Já na Figura 6.43 (a) mostra uma imagem de texto escrito à mão corrompida por um sombreamento senoidal. Esta forma de sombreamento de intensidade é típica de quando o fornecimento de energia em um digitalizador de documentos não é apropriado. A Figura 6.43 (a) é a imagem original, a Figura 6.43 (b) é o resultado da limiarização por Otsu e a Figura 6.43 (c) é o resultado pela média móvel. Os parâmetros utilizados foram o mesmo do anterior, sendo \\(n=20\\) e \\(b=0.5\\), o que mostra relativa robustez do método. Figura 6.43: Exemplo de aplicação da limiarização por médias móveis em um documento corrompido por um sombreamento típico de problemas em scanner em que o fornecimento de energia não é o apropriado. (a) Imagem original. (b) Aplicado método de Otsu. (c) Aplicado método de médias móveis. [2, p. 502]. 6.5.10 Limiarização baseada em diversas variáveis Até agora, falamos apenas da limiarização baseada em uma única variável: intensidade dos tons de cinza. Em alguns casos, um sensor pode disponibilizar mais de uma variável para identificar cada pixel em uma imagem e, assim, permitir uma limiarização multivariada. Um exemplo notável é a imagem em cores, na qual os componentes são vermelho (R), verde (G) e azul (B). Neste caso, cada “pixel” é identificado por três valores e pode ser representado como um vetor 3-D, \\(z = (z_1+z_2+z_3)^T\\), cujos componentes são as cores RGB em um ponto. Estes pontos 3-D são frequentemente chamados de voxels, para denotar elementos volumétricos em oposição aos elementos de imagem [2, p. 501]. Numa limiarização focada na intensidade de cinza, de apenas uma variável, avaliamos apenas a intensidade, um gráfico de duas variáveis (histograma convencional). A sua limiarização é simples. Já no R, G, B, gráfico tridimensional, avaliamos a distância dos píxels da imagem a um píxel de referência, e o limiar é representado pelo contorno de uma Figura(a) simétrica na qual contém os píxels segmentados e tem como seu centro o píxel de referência. Conforme demonstrado na Figura 6.44. Figura 6.44: Segmentação multivariada no RGB. Gráfico-3D [2, p. 295]. Suponha que queiramos extrair de uma imagem colorida todas as regiões com uma faixa de cor específica: por exemplo, tons avermelhados da Figura 6.45 (a). Vamos denotar a cor avermelhada média em que estamos interessados, a amostra é demarcada pelo retângulo de bordas claras em Figura 6.45 (a). Uma forma de segmentar uma imagem colorida com base neste parâmetro é calcular uma medida de distância, \\(D(z, a)\\), entre um ponto de cor arbitrária, \\(z\\), e a cor média, \\(a\\). Então, tem-se a segmentação, Figura 6.45 (b): \\[ g= \\begin{cases} 1,\\ se\\ D(z,a)\\ &lt;\\ T\\\\ 0,\\ caso\\ contrário \\end{cases} \\] Figura 6.45: Exemplo de segmentação multivariada no RGB. [2, p. 295]. Porém, esse cálculo de distância dos pontos ao centro em formato esférico é trabalhoso para o computador. Uma maneira mais eficiente é usar um delimitador cúbico. Nessa metodologia, o cubo é centralizado em \\(a\\) e suas dimensões ao longo de cada um dos eixos de cor são escolhidas em proporção ao desvio padrão das amostras da imagem ao longo de cada um dos eixos (R, G e B). Portanto, o procedimento da Figura 6.45 (a) consistiu em calcular o vetor médio \\(a\\) utilizando os pontos de cor contidos no retângulo. Em seguida, calculou-se o desvio padrão dos componentes vermelho, verde e azul dessas amostras. Um cubo foi centralizado em \\(a\\), e as dimensões ao longo de cada um dos eixos RGB foram escolhidas como 1,25 multiplicado pelo desvio padrão ao longo dos eixos correspondentes. Por exemplo, no eixo \\(R\\), vermelho, a dimensão do cubo é de \\((a_R - 1,25\\sigma_R)\\) até \\((a_R + 1,25\\sigma_R)\\), no qual \\(a_R\\) indica o valor do componente vermelho de \\(a\\). E por fim, realizou-se a limiarização. Refêrencias "],["refêrencias.html", "Refêrencias", " Refêrencias "]]
