[["index.html", "Material introdutório de Processamento Digital de Imagens e Visão Computacional Início", " Material introdutório de Processamento Digital de Imagens e Visão Computacional Início Esse site foi desenvolvido pelo grupo COVAP (Computação Visual Aplicada) que visa oferecer um material introdutório da disciplina de PDI (Processamento Digital de Imagens) e VC (Visão Computacional) para alunos e entusiastas no assunto. Os conteúdos se encontram organizados na seguinte estrutura: Introdução Formação da imagem Transformações geométricas Transformações radiométricas Filtros Digitais Segmentação Separação plano de fundo Deep Learning CNN Lembramos ainda que todo o material está hospedado no Github, agradeçemos então todo apontamento de erros e sugestões, para que assim consigamos sempre disponibilizar um conteúdo revisado e livre de quaisquer erros. "],["intro.html", "Capítulo 1 Introdução 1.1 Relação entre Processamento Digital de Imagem, Visão Computacional e Computação Gráfica 1.2 Etapas do Processamento de Imagens, Visão Computacional e Aprendizado de Máquina 1.3 Áreas de Aplicações", " Capítulo 1 Introdução 1.1 Relação entre Processamento Digital de Imagem, Visão Computacional e Computação Gráfica A visão desempenha um papel importante na vida das pessoas, pois com ela é possível uma percepção incrivelmente rica do mundo ao seu redor. Para tentar reproduzir as capacidades visuais humanas por sistemas autônomos manipulados por computadores foram desenvolvidas pelo menos três grandes áreas [1, p. 2]: Processamento Digital de Imagens (PDI), Visão Computacional (VC) e a Computação Gráfica(CG), apresentados na Figura 1.1. Essas áreas, apesar de serem correlacionadas, têm objetivos e métodos diferentes o que justifica a importância de distingui-las. Figura 1.1: Processos Computacionais com Imagens - O processamento de imagem, a visão computacional e a computação gráfica são áreas que operam com imagens digitais. Estas áreas têm propósitos diferentes, mas podem ser relacionadas, pois a saída do processamento de uma pode ser a entrada de outra. [1, p. 2]. O Processamento Digital de Imagens (PDI) busca realizar o pré-processamento das imagens, utilizando para isso técnicas de tratamento, como a correção da iluminação, eliminação de ruído, e a segmentação. Geralmente, no PDI tanto a entrada quanto a saída do processo são imagens. O foco da Visão Computacional (VC) é a análise das imagens, identificando os seus componentes e obtendo informações de modelos gerados, principalmente do mundo 3D. Diferente da VC, em que as imagens representam o objeto de estudo, na Computação Gráfica (CG), as imagens são o resultado do processo. Na CG são geradas representações visuais seguindo descrições e especificações geométricas de modelos de entrada [1, p. 3]. A Tabela 1.1 apresenta de forma resumida as diferenças entre PDI, VC e CG. Na segunda linha da tabela está uma descrição simples de cada área, e na terceira linha um esquema identificando o objeto e o produto de cada processo. Tabela 1.1: Processos Computacionais com Imagens - Descrição da aplicação das áreas de Computação Gráfica (CG), Visão Computacional (VC) e Processamento Digital. Na última linha está um esquema simplificado da entrada e saída de cada aplicação. Computação Gráfica (CG) Visão Computacional (VC) Processamento Digital de Imagens (PDI) Cria e altera imagens a partir de dados. Análise de imagem para criação de modelos. Transformação de imagem (tratamento). modelo → imagem imagem → modelo imagem → imagem As imagens tratadas em PDI têm como forte potencial servir de material para a Visão Computacional, como pode ser percebido na Figura 1.1. Muitas vezes as áreas de Visão Computacional e PDI são confundidas devido a dificuldade em se definir em que ponto uma termina e a outra começa. Mesmo não existindo uma linha clara entre os limites destas duas áreas é possível utilizar um paradigma que considera três níveis de processamento [2, p. 2]: Baixo nível A nível de pixel, realiza operações de pré-processamento, sendo utilizada, por exemplo, na redução de ruído, aumento de contraste e restauração. Nesta etapa tanto a entrada quanto a saída são imagens. Médio nível Operações mais complexas, como segmentação, partição e reconhecimento de objetos individuais. A entrada é uma imagem mas a saída pode ser um conjunto contendo os atributos extraídos das imagens, como formas, bordas e objetos individuais. Alto nível Interpretação do conteúdo da imagem e análise, muitas vezes processos associados com as funções da visão, como a classificação, o reconhecimento e o rastreamento de objetos. Baseado nesses níveis, iremos considerar que o processamento de imagem atua nos primeiros dois níveis, ou seja, envolve o pré-processamento e processos de extração de elementos de imagens até o reconhecimento de componentes individuais. O campo da Visão Computacional, mesmo associado com os níveis mais baixos, se torna mais evidente a partir das técnicas de alto nível de processamento, que utilizam informações extraídas das imagens para processos de inferências ou aprendizados, resultando em aplicações em diferentes áreas. A VC por ser uma área que tenta emular as funções cognitivas associadas à visão se enquadra como um ramo da inteligência artificial (AI). O processo de aprendizado necessário para as diferentes vertentes da AI - Visão Computacional, Reconhecimento de Voz, Aprendizado Natural de Linguagem, Robótica, entre outros - dependem em grande parte da vertente do Aprendizado de Máquina ou Machine Learning. Assim, muitas técnicas de aprendizado de máquina são associadas com etapas de processamento de imagens dentro da Visão Computacional. Com o avanço do aprendizado de máquina (Machine Learning), principalmente com o desenvolvimento do aprendizado profundo (Deep Learning), surgiu um novo paradigma na Visão Computacional. Anteriormente, as etapas da VC envolviam uma sequência de algoritmos de processamento de imagens construídos em diferentes softwares, específicos para um determinado fim, e que dificilmente se enquadravam em outros problemas. Com a adaptação de modelos de aprendizado profundo foi possível englobar todas as etapas de processamento e aprendizado em um único programa, e que muitas vezes pode ser estendido para diferentes aplicações. 1.2 Etapas do Processamento de Imagens, Visão Computacional e Aprendizado de Máquina Um dos objetivos deste material é servir de referência para o estudo inicial da Visão Computacional, assim, para compreender as relações entre os três temas discutidos no final do tópico anterior - Processamento de Imagens, Visão Computacional e Aprendizado de Máquina - indicamos um guia para o presente material de estudo, em que se descreve como os capítulos foram organizados dentro destes três temas mais amplos. Com base na abordagem adotada no diagrama da Figura 1.2, as principais etapas para o tratamento de problemas envolvem: aquisição da imagem; pré-processamento; e análise da informação visual. Dentro de cada um destes níveis é possível subdividir vários outros subtópicos dependendo da aplicação de interesse, e neste material apresentamos algumas de usos mais gerais, que estão organizadas em capítulos. Figura 1.2: Etapas para o desenvolvimento de aplicações na Visão Computacional - Diagrama com a distribuição dos capítulos com base nas principais etapas dentro do processamento de imagem e do aprendizado de máquina orientados à visão computacional. As primeiras etapas - Aquisição de imagens e Pré-Processamento - são temas que a área de Processamento de Imagem tem apresentado forte contribuição durante muito tempo, em que vários algoritmos, técnicas e conceitos continuam sendo a base para a Visão Computacional. Antigamente, os resultados do processamento de imagens que eram as informações de maior interesse da VC, eram dados tratados que podiam ser avaliados e aplicados em modelos estatísticos, possibilitando a análise e resolução do problema. Com o surgimento de outros paradigmas, todas estas etapas que eram tratadas separadamente, com técnicas e softwares diferentes, passaram a ser tratadas dentro de um único sistema, utilizando modelos de aprendizado de máquina, como as redes neurais. Nesta nova abordagem, as imagens brutas também podem ser vistas como entradas diretas das aplicações. Por esta razão, tanto a Visão Computacional quanto o Aprendizado de Máquina tem as suas fronteiras na Figura 1.2 demarcando do início ao fim todos os processos. Para destacar como os capítulos se relacionam com estas etapas será apresentado uma breve descrição de cada uma e alguns exemplos de conteúdos que serão abordados ao longo do material. Aquisição da imagem Captura a imagem por meio de um dispositivo ou sensor e a converte em uma imagem digitalizada [3, p. 3]. Podemos citar como exemplo as câmeras fotográficas, tomógrafos médicos, satélites e scanners. Na figura 1.3, temos um exemplo de imagem colorida separadas em suas três componentes, e mais detalhes sobre a aquisição e formação deste tipo de imagem e de outras são abordados no Capítulo Formação de Imagem. Como será discutido no Capítulo Espaço 3D, algumas aplicações da Visão computacional dependem de parâmetros determinados pelo equipamento e das condições durante a aquisição das imagens. Figura 1.3: Imagem colorida - Imagem separada em seus três componentes, em que Red é a vermelha, Green é a verde e Blue é a azul [4, p. 28]. Pré processamento Essa etapa busca realizar mudanças e ajustes na imagem visando melhorar seu uso nas etapas futuras [3, p. 3]. Como exemplo temos casos onde não precisamos das cores de uma imagem, podendo então realizar a conversão para grayscale (tons de cinza), ou precisamos gerar imagens coloridas. Além disso, podemos realizar cortes ou realces, isolando somente a parte de maior interesse na imagem utilizando algumas das técnicas apresentadas no Capítulo Transformações Geométricas e no Capítulo Transformações Radiométricas. Na Figura 1.4 se destaca uma das abordagens recorrentes no tratamento de imagem, a atenuação de ruídos e a suavização, utilizando principalmente alguns dos filtros do Capítulo Filtros Digitais. Figura 1.4: Remoção de ruído sal e pimenta por meio do filtro de Mediana - No Capítulo Filtros Digitais serão apresentados alguns filtros como este que tratam as imagens, por exemplo, revomendo ruídos, aumentando o realce ou suavizando[2, p. 103]. Análise À parte de Análise atribuímos todos os processos que fazem uso da informação visual para compor um sistema de aplicação na área de Visão Computacional. Atualmente é mais comum ver estes processos englobados dentro de modelos de aprendizado de máquina mais genéricos como em redes neurais, capazes por exemplo, de classificar, identificar e rastrear objetos em diferentes situações. No Capítulo Deep Learning em Visão Computacional, assim como na Figura 1.5 é possível se ter uma pequena noção de como as redes neurais, em particular as redes multicamadas de perceptrons (MLP), enxergam as imagens e como desagregam suas informações para disponibilizar resultados satisfatórios no campo da VC. Figura 1.5: Aplicação das Redes Neurais na Visão Computacional - Utilização das redes multicamadas de perceptrons para classificar dígitos de números em imagens [5] 1.3 Áreas de Aplicações Visto de forma geral como funcionam as etapas da Visão Computacional e do processamento de imagem é interessante elencar algumas das possíveis áreas em que estes dois campos podem ter atuação. Partindo do que já foi produzido se levanta que as primeiras tarefas de processamento de imagens tiveram aplicações significativas por volta da década de 1960, quando se desenvolveram computadores com potencial suficiente para realizá-las. O programa espacial americano também foi um forte impulso para o contínuo desenvolvimento e aprimoramento das técnicas de PDI, já que imagens, como as obtidas da Lua através de sondas e transmitidas à terra, continham distorções provenientes das câmeras utilizadas. Era necessário então, a utilização de métodos para corrigir essas alterações [2, p. 4]. Outra área que também faz uso extensivo do processamento de imagens e impulsionou seu desenvolvimento é a área médica. Nessa área, o uso de imagens auxiliou no diagnóstico de doenças através de exames visuais como os de raio-x [2, p. 4]. A utilização do processamento de imagens para melhorar informações visuais, ajudando na interpretação humana, expandiu-se para diferentes setores. No sensoriamento remoto, o pré-processamento contribui para uma melhor análise de imagens aéreas e de satélite, aumentando a compreensão da superfície terrestre. Na arqueologia e nas artes, métodos de processamento de imagens podem restaurar fotografias com registros únicos de objetos raros, pinturas, documentos antigos e conteúdos em vídeos [3, p. 2]. Na física e em áreas da biologia, técnicas computacionais realçam imagens de experimentos em áreas como plasmas de alta energia e microscopia eletrônica [2, p. 5]. Com o aumento da automatização de tarefas, o processamento de imagens tem se destacado na aquisição de dados de imagens visando a percepção automática por máquinas [3, p. 3]. Técnicas de identificação de padrões podem ser aplicados no reconhecimento automático de caracteres, de impressões digitais, de faces, e de placas de veículos, contribuindo com setores de segurança. Na automação industrial tem sido utilizado no sistema de visão computacional para inspeção e montagem de produtos. Na área militar, pode ser aplicado na identificação e rastreamento de alvos em imagens de satélites, e na navegação de veículos autônomos. Nas áreas de medicina e biologia, rastreamentos automáticos em imagens radiográficas e amostras de sangue têm contribuído para os exames e testes [3, p. 3]. O processamento computacional de imagens aéreas e de satélites também é utilizado na previsão do tempo e em avaliações ambientais [2, p. 5]. Este variado campo de aplicações pode ser justificado pela capacidade dos aparelhos de processamento de imagens trabalharem com imagens de diversas fontes. Diferentemente dos seres humanos, que são limitados à banda visual do espectro eletromagnético (EM), o processamento computacional cobre todo o EM, variando de ondas gama a ondas de rádio [2, p. 1]. No processamento digital ainda é possível trabalhar com imagens geradas por fontes que os humanos não estão acostumados a associar com imagens. Essas fontes incluem acústica, ultrassom, microscopia eletrônica e imagens geradas por computador [2, p. 13]. Uma das formas mais fáceis de desenvolver uma compreensão básica da extensão das aplicações de PDI é categorizar as imagens de acordo com sua fonte. Na Figura 1.6 ttemos uma representação do EM e iremos a seguir explorar cada uma dessas faixas, apresentando algumas das áreas onde podem ser utilizados: Figura 1.6: Espectro eletromagnético - As imagens podem ser geradas a partir de diferentes fontes eletromagnéticas, incluindo raios gama, raio x, ultravioleta, visível, infravermelho, microondas e rádio [6]. Imagens formadas por raios gama As imagens formadas a partir de raios gama têm diferentes utilidades, sendo muito utilizadas na medicina e astronomia [2, p. 6]. Na medicina, existem procedimentos onde se injetam isótopos radioativos no paciente e por meio dos detectores de raio gama é formada uma imagem, como exemplo, escaneamento ósseo e tomografia por emissão de pósitrons (PET-scan). Na astronomia, ela pode ser utilizada para se conseguir ver detalhes astronômicos que estão presentes na faixa eletromagnética dos raios gama. Imagens formadas por raios X Imagens formadas a partir de raio X têm uma ampla gama de aplicações, desde seu uso na medicina até seu uso no meio industrial [2, p. 6]. Na indústria, pode ser utilizado para se encontrar defeitos de fabricação em produtos, e na medicina, vêm se utilizando muito o processamento de imagem e a visão computacional para ajudar no diagnóstico de doenças, como por exemplo, artérias obstruídas, fraturas e tumores. Imagens na banda ultravioleta O espectro ultravioleta também tem inúmeras aplicações, como a inspeção industrial, microscopia, imagens biológicas e observações astronômicas [2, p. 8]. Imagens na banda visível e infravermelho Essas duas bandas possuem uma gama extremamente ampla de aplicações, sendo utilizadas juntas ou separadas. Na banda visível, existem diversas aplicações, como em processos industriais, detecção de faces, detecção de placas de carros, etc [2, p. 11]. A banda infravermelho também possui inúmeras aplicações, sendo uma delas imagens a partir de satélites, onde o infravermelho nos permite ver inúmeros detalhes que somente com a banda visível não seria possível [2, p. 9]. Imagens na banda de micro-ondas e rádio Na banda de micro-ondas o melhor exemplo que temos é o radar. Essa banda tem uma peculiaridade de ser extremamente penetrante, podendo gerar imagens através de nuvens, vegetação, etc [2, p. 12]. Já a banda de rádio é muito utilizada na medicina, como exemplo na ressonância magnética e na astronomia [2, p. 12]. Como podemos observar, existem inúmeras maneiras de se conseguir imagens além da clássica imagem no espectro visível, isso nos dá a possibilidade de utilizar o PDI em inúmeras áreas e problemas. Na Figura 1.7 temos uma nebulosa observada a partir de diferentes bandas do EM, sendo possível observar detalhes que passariam despercebidos se usássemos somente alguma delas. Figura 1.7: Nebulosa CRAB em diferentes frequências - Ao observar um mesmo objeto a partir de várias bandas do EM é possível avaliar diferentes aspectos [7]. Refêrencias "],["formacaoImagem.html", "Capítulo 2 Formação da imagem 2.1 Câmera pinhole e geometria 2.2 Lentes Finas 2.3 Sensor 2.4 Amostragem e Quantização 2.5 Definição de imagem digital 2.6 Resolução espacial e de intensidade 2.7 Pixels", " Capítulo 2 Formação da imagem Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é o espectro eletromagnético na faixa de ondas visíveis. Outras fontes de energia também podem ser utilizadas como energia mecânica na forma de ultrassom, feixe de elétrons em microscópio eletrônicos, ondas de rádio no radar, etc. Cada fonte necessita de um método específico de captura. Para ondas eletromagnéticas pode ser usada uma câmera fotográfica equipada com sensores adequados ao comprimento de onda. Porém, para outras fontes, é necessário que o computador sintetize a imagem, como o microscópio eletrônico. Como já mencionado no tópico de Introdução, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensíveis a essa faixa de luz, que vem da luz solar e nos ajuda a realizar nossas atividades cotidianas. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas de onda, como a ultravioleta [8, p. 2]. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas [4, p. 8]. A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza ou escala de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de \\(0.43\\) a \\(0.79 \\mu m\\). Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar [2, p. 28]. Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir. 2.1 Câmera pinhole e geometria Na Figura 2.1, temos um esquema básico de como geralmente ocorre a aquisição de imagens. Primeiramente, a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida. A parte refletida é capturada por um dispositivo, como uma câmera fotográfica. Figura 2.1: Representação de uma típica captura de imagem [4, p. 8]. Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, conhecido como câmera pinhole (do inglês buraco de alfinete) ou câmera escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício, tão pequeno quanto possível, por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na Figura 2.2, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruidosa. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores. Figura 2.2: Introdução de barreira para captura de imagem [4, p. 11]. Na Figura 2.2 percebemos que a imagem resultante acaba invertida. Isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir. Figura 2.3: Geometria de uma câmera pinhole [9, p. 5]. Na Figura 2.3, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância horizontal \\(Z\\) da abertura e a uma distância vertical \\(Y\\) do eixo óptico, podemos definir a altura \\(y\\) e a largura \\(x\\) da projeção do objeto utilizando a simetria de triângulos: \\[ -\\frac{y}{f}=\\frac{Y}{Z}\\Leftrightarrow y=-f\\frac{Y}{Z} \\ \\ \\ \\ \\text{e}\\ \\ \\ \\ -\\frac{x}{f}=\\frac{X}{Z} \\Leftrightarrow x=-f\\frac{X}{Z} \\tag{2.1} \\] A variável \\(f\\) nessa Equação (2.1) se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera, pois a imagem é formada em seu fundo [9, p. 4]. Os sinais negativos das equações significam que a imagem projetada está rotacionada a \\(180^\\circ\\) verticalmente e horizontalmente devido a semelhança de triângulos [9, p. 5], como podemos confirmar na Figura 2.3. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens como precisar de um longo tempo de exposição para captura da imagem. As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso. 2.2 Lentes Finas Figura 2.4: Ação de uma lente sobre os raios de luz [4, p. 12]. Como podemos ver na Figura 2.4, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada. O ponto \\(F\\) onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância \\(f\\), que vai do centro óptico \\(O\\) até \\(F\\) é conhecida como Distância Focal. Definindo a distância do objeto real até a lente como \\(g\\) e a distância até a formação da imagem, após passar pela lente, como \\(b\\) temos que: \\[ \\frac{1}{g}+\\frac{1}{b}=\\frac{1}{f} \\tag{2.2} \\] Como \\(f\\) e \\(b\\) estão normalmente entre 1mm e 100mm isso mostra que \\(\\frac{1}{g}\\) não tem quase nenhum impacto na Equação (2.2) e significa que \\(b = f\\). Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal. Outro ponto importante das lentes é conhecido como zoom óptico, ilustrado na Figura 2.5. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, \\(B\\), aumenta quando \\(f\\) aumenta. Podemos representar isso na seguinte Equação (2.3), onde \\(g\\) é o tamanho real do objeto: \\[ \\frac{b}{B}=\\frac{g}{G} \\tag{2.3} \\] Figura 2.5: Zoom óptico através de lentes com diferentes distâncias focais [4, p. 13]. Na prática \\(f\\) é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos. Se o \\(f\\) for constante, quando alteramos a distância do objeto, no caso \\(g\\), sabemos que \\(b\\) também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos \\(b\\) temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando \\(b\\) para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco. Figura 2.6: Uma imagem focada em (a) e desfocada em (b) [4, p. 11]. A Figura 2.6 ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos. Figura 2.7: Profundidade de campo [4, p. 13]. A Figura 2.7 apresenta outro ponto muito importante, chamado Profundidade de Campo (Depth of field), que representa a soma das distâncias \\(g_l\\) e \\(g_r\\), que representam o quanto os objetos podem ser movidos e permanecerem em foco. Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão (Field of View ou FOV) que representa a área observável de uma câmera. Na Figura 2.8, essa área observável é denotada pelo ângulo \\(V\\). O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as Equações (2.4) e (2.5), respectivamente, do campo de visão horizontal e vertical: \\[ FOV_x = 2*\\tan^{-1}\\left(\\frac{\\frac{comprimento\\ do\\ sensor}{2}}{f}\\right) \\tag{2.4} \\] \\[ FOV_y = 2*\\tan^{-1}\\left(\\frac{\\frac{altura\\ do\\ sensor}{2}}{f}\\right) \\tag{2.5} \\] Figura 2.8: Dois diferentes Campos de visão [4, p. 14]. Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de \\(14mm\\), altura de \\(10mm\\) e uma distância focal de \\(5mm\\) temos: \\[ FOV_x=2*tan^{-1}\\left(\\frac{7}{5}\\right)=108.9^{\\circ} \\ \\ \\ \\ \\text{e}\\ \\ \\ \\ FOV_y=2*tan^{-1}(1)=90^{\\circ} \\tag{2.6} \\] Isso significa que essa câmera tem uma área observável de \\(108.9^\\circ\\) horizontalmente e \\(90^\\circ\\) verticalmente. Na Figura 2.9, temos o mesmo objeto fotografado com diferentes profundidades de campo: Figura 2.9: Objeto fotografado com diferentes profundidades de campo [4, p. 15]. Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris do olho humano. É ele que controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para a captura da imagem. 2.3 Sensor Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD (Charge-coupled device), que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS (Complementary metal–oxide semiconductor), usado em casos mais gerais, como câmeras de celulares. Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na Figura 2.10, conhecido como PDA (Photodiode Array): Figura 2.10: Sensor (área matricial de células), Single Cell (uma única célula sensor) [4, p. 17]. Como podemos ver, o sensor consiste em várias pequenas células, cada uma com um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na Figura 2.11, podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta (correctly exposed), logo em seguida temos uma que sofreu de superexposição (overexposed) e na terceira temos uma com subexposição (underexposed). Por último temos uma imagem que sofre com o movimento do objeto cuja imagem estava sendo capturada, o que ocasionou o borramento (motion blur). Figura 2.11: Diferentes níveis de exposição [4, p. 17]. Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas como são capturadas imagens coloridas? Imagens coloridas utilizam, especialmente, o formato RGB, que significa Red-Green-Blue, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na Figura 2.12, podemos ver uma imagem com seus componentes separados: Figura 2.12: Imagem colorida separada em seus três componentes, em que Red é a vermelha, Green é a verde e Blue é a azul [4, p. 28]. Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na Figura 2.13. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo. Figura 2.13: Captura de imagem com três sensores [10, p. 242]. Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel. Isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na Figura 2.14: Figura 2.14: Filtro Bayer [4, p. 29]. Podemos perceber que ocorre uma maior ocorrência das cores verdes. Isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase à sua captura. Na Figura 2.15, temos um esquema de como cada pixel recebe informação de somente uma cor, por meio da filtragem. Nesse esquema a luz que entra (Incoming light) é filtrada e somente a cor de interesse consegue passar. Após isso, ela chega a malha de sensores (sensor array): Figura 2.15: Sensores com padrão Bayer [11]. Vemos na Figura 2.15 que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos. 2.4 Amostragem e Quantização Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens, serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto. 2.4.1 Amostragem Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura 2.16, na qual a Figura 2.16(a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto. A Figura 2.16(b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha \\(\\overline{AB}\\). Nas extremidades do gráfico na Figura 2.16(b), a intensidade é mais alta devido a parte branca da imagem. Já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na Figura 2.16(c). Na prática, esse procedimento de amostragem é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo, teríamos que ter um número infinito de pixels. Como isso não é possível, recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem [2, p. 34]. Figura 2.16: Produzindo uma imagem digital. (a) Imagem contínua. (b) Linha de varredura de \\(A\\) a \\(B\\) na imagem contínua utilizada para ilustrar os conceitos de amostragem e quantização. (c) Amostragem e quantização. (d) Linha de varredura digital. [2, p. 34] 2.4.2 Quantização Na Figura 2.16(c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na Figura 2.16(d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na Figura 2.16(c). Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital [9, p. 8]. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo [12, p. 9]. No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos \\(\\{0,\\dots, L-1\\}\\), onde \\(L\\) é uma potência de dois, ou seja, \\(L = 2^k\\) [12, p. 10]. Isso significa que \\(L\\) é o número de tons de cinza que podem ser representados com uma quantidade \\(k\\) de bits. Em muitas situações é utilizado \\(k = 8\\), ou seja, temos \\(256\\) níveis de cinza. Ao realizar a quantização e a amostragem linha por linha no objeto da Figura 2.17(a) é produzida uma imagem digital bidimensional como na Figura 2.17. Figura 2.17: (a) Imagem contínua projetada em uma matriz de sensores. (b) Resultado da amostragem e quantização da imagem. [2, p. 35] 2.5 Definição de imagem digital Uma imagem pode ser definida como uma função bidimensional, \\(f(x, y)\\), em que \\(x\\) e \\(y\\) são coordenadas espaciais (plano), e a amplitude de \\(f\\) em qualquer par de coordenadas \\((x, y)\\) é chamada de intensidade ou nível de cinza da imagem nesse ponto [2, p. 36]. Quando \\(x\\), \\(y\\) e os valores de intensidade de \\(f\\) são quantidades finitas e discretas, chamamos de imagem digital. A função \\(f(x, y)\\) pode ser representada na forma de uma matriz MxN como na equação (2.7), em que as \\(M\\) linhas são identificadas pelas coordenadas em \\(x\\), e as \\(N\\) colunas em \\(y\\). Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou pel. O formato numérico da matriz, imagem 2.18, é apropriado para o desenvolvimento de algoritmos, representado pela Equação (2.7). \\[f(x,y) = \\begin{bmatrix} f(0,0) &amp; f(0,1) &amp; \\cdots &amp; f(0,N-1) \\\\ f(1,0) &amp; f(1,1) &amp; \\cdots &amp; f(1, N-1) \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ f(M-1,0) &amp; f(M-1, 1) &amp; \\cdots &amp; f(M-1, N-1) \\end{bmatrix} \\tag{2.7} \\] Figura 2.18: Representações da imagem digital [2, p. 36]. Na Figura 2.18(a) temos a representação da imagem em 3-D, onde a intensidade de cada pixel é representada no eixo \\(z\\), ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na Figura 2.18(b), formato que seria visualizado em um monitor ou uma fotografia [2, p. 36]. Em cada ponto da Figura 2.18(a), o nível de cinza é proporcional ao valor da intensidade \\(f\\), assumindo valores \\(0\\), \\(0.5\\) ou \\(1\\). Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco. Note que na Figura 2.18, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo \\(x\\) positivo direcionado para baixo e o eixo \\(y\\) positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez [2, p. 36]. De acordo com o tamanho da matriz (MxN) e dos níveis discretos de tons de cinza (\\(L = 2^k\\)) que os pixels podem assumir é possível determinar o número, \\(b\\), de bits necessários para armazenar uma imagem digitalizada: \\[ b = M × N × k \\tag{2.8} \\] Quando uma imagem pode ter \\(2^k\\) níveis de intensidade, geralmente ela é denominada como uma “imagem de \\(k\\) bits”. Por exemplo, uma imagem com \\(256\\) níveis discretos de intensidade é chamada de uma imagem de \\(8\\) bits. A Figura 2.19 mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (NxN) para diferentes valores de \\(N\\) e \\(k\\). O número de níveis de intensidade (\\(L\\)) correspondente a cada valor de \\(k\\) é mostrado entre parênteses. Observa-se na Figura 2.19 que uma imagem de \\(8\\) bits com dimensões 1024x1024 exigiria aproximadamente 1MB para armazenamento. Figura 2.19: Número de bits de armazenamento para vários valores de \\(N\\) e \\(k\\) [2, p. 38]. 2.6 Resolução espacial e de intensidade Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (MxN) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente dots per inch (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2400 dpi [2, p. 38]. A Figura 2.20 mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A Figura 2.20(a) tem resolução 512x512, e a resolução das demais 2.20(b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução. Figura 2.20: Efeitos da redução da resolução espacial [3, p. 21]. A resolução de intensidade ou profundidade corresponde ao número de bits (\\(k\\)) utilizados para estabelecer os níveis de cinza da imagem (\\(L=2^k\\)). Por exemplo, em uma imagem cuja intensidade é quantizada em \\(L= 256\\) níveis, a profundidade é de \\(k = 8\\) bits por pixel. Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura 2.21. A Figura 2.21(a) apresenta \\(256\\) níveis de cinza (\\(k = 8\\)). As Figuras 2.21(b) e (c) foram geradas pela redução do número de bits \\(k = 4\\) e \\(k = 2\\), respectivamente, mas mantendo a mesma dimensão. Figura 2.21: Efeitos da redução de profundidade [4, p. 19]. 2.7 Pixels Os pixels são elementos principais na formação da imagem, sua importância determina topologicamente as características da imagem, nesta seção mostraremos conceitos básicos da topologia da imagem em relação seus elementos (pixels). Em memória computacional os pixels da imagem são representados nos formatos de matrizes (bidimensional ou tridimensional) já abordado na seção Seção 2.5, através delas é permitido aplicar operações sobre seus elementos (pixels), efetuar análises, identificação de padrões, acessar regiões, alterar cores, posições e tamanho tendo como referência espaço projetado usando as coordenadas do plano. Propriedades topológicas dos pixels da imagem: Vizinhança (4, D, 6, 8 e 26) Conectividade Adjacência Caminho Componente Conexa Borda e Interior Medidas de Distância Operações Lógico-aritméticas 2.7.1 Vizinhança As vizinhanças de um elemento (pixels) \\(f\\) pertencente ao conjunto \\(S\\) (matriz de pixels) com coordenadas \\((x,y)\\) possui vizinhos dos tipos 4, D e 8. Para definir uma vizinhança-4 (denotado \\([N_4(f)])\\) de \\(f(x,y)\\) ilustrado na Figura 2.22, \\(f_{(1,1)}\\) possui quatro vizinhos, dois na horizontal e outros dois na vertical cuja as coordenadas da vizinhança são \\((x + 1, y), (x - 1, y), (x, y + 1) \\ e \\ (x, y -1)\\). Os quatros vizinhos diagonais de \\(f_{(5, 1)}\\) são de coordenadas \\((x - 1 , y - 1), (x - 1, y + 1), (x + 1, y + 1)\\), que constituem o conjunto \\([N_d(f)]\\). A vizinhança-8 (denotado \\([N_8(f)]\\) e ilustrada na Figura 2.22 \\(f_{(4, 5)}\\), é definida como \\(N_8(f)= N_4(f) \\ U \\ Nd(f)\\). Figura 2.22: Vizinhança de 4-D e 8 de um pixel, matriz de pixel da imagem. 2.7.2 Conectividade A conectividade entre os pixels é outro conceito importante na topologia da imagem, usada para determinar limites de objetos e componentes de regiões na imagem. Dois pixels são conexos entre si primeiro verificar-se seus tipos de (vizinhos Seção 2.7.1), de seguida se existe similaridade na intensidade de cinza, cor ou textura. Por exemplo, em uma imagem binária, em que os pixels podem assumir os valores \\(0\\) ou \\(1\\), dois pixels podem ter vizinhança-4, mas somente serão considerados conexos se possuírem o mesmo valor. [3, p. 31]. 2.7.3 Adjacência Um conjunto \\(V\\) com valores de intensidade para definir adjacência, em uma imagem binária, com \\(V={1}\\) e outra imagem com mesmo conjunto contendo escala de nível de cinza no limite \\(0\\) a \\(255\\) com vários elementos (pixels) o conjunto \\(V\\) poderia ser qualquer subconjunto desses \\(256\\) valores se eles estiverem conectados de acordo com o tipo de (vizinhança Seção 2.7.1) especificado. [2, p. 31]. 2.7.4 Caminho Uma imagem com coordenada de pixels \\((x_1 , y_1), (x_2 , y_2) … (x_n , y_n)\\) que determina a sequência do caminho dos variados pixels onde né o comprimento do caminho, e \\((x_i , y_i)e (x_i + 1 , y_i + 1)\\) são adjacentes, tal que \\(i = 1,2,...,n - 1\\).Se na relação de conectividade considerar vizinhança-4, então existe um caminho-4; para vizinhança-8, tem-se um caminho-8. Exemplo de caminhos são mostrados na Figura 2.23 sendo que o caminho-4 possui comprimento \\(10\\) e o caminho-8 possui comprimento \\(7\\). O conceito de caminho também pode ser estendido para imagens tridimensionais. [3, p. 31]. Figura 2.23: (a) Caminho-4, (b) Caminho-8. [3, p. 31] 2.7.5 Componente Conexa Componentes conexos de pixels de uma imagem é um conjunto de elementos (pixels) que de alguma forma estão conectados entre si, dois ou mais pixels são componentes conexos se existir um caminho seção 2.7.4 Caminho pertencente ao conjunto. A Figura 2.24 mostra uma imagem bidimensional contendo três componentes conexos caso seja considerada a vizinhança-4 ou, então, dois componentes conexos se considerada a vizinhança-8. [3, p. 32]. Figura 2.24: Componentes Conexos de uma imagem bidimensional [3, p. 32]. 2.7.6 Borda e Interior A borda de um componente conexo seção 2.7.5 \\(S\\) em uma imagem bidimensional é o conjunto de pixels pertencentes ao componente e que possuem vizinhança-4 com um ou mais pixels externos a \\(S\\) [3, p. 32]. Intuitivamente, a borda corresponde ao conjunto de pontos no contorno do componente conexo. O interior é o conjunto de pixels de \\(S\\) que não estão em sua borda. A Figura 2.25 mostra um exemplo de uma imagem binária com sua borda interior. Figura 2.25: Borda e interior de um componente, Figura do lado esquerdo, mostra a imagem original, a Figura do lado direito mostra os pixels da borda e interior. [3, p. 32]. 2.7.7 Medidas de Distância As medidas de distância são aplicadas sobre os pixels e componentes da imagem, considerando \\(D\\) uma função com os pixels \\(f_1\\), \\(f_2\\) e \\(f_3\\) de coordenadas \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\((x_3, y_3)\\), para verificar se \\(D\\) é uma função distância ou medida de distância, deve satisfazer as seguintes propriedades: \\[ D(f_1,f_2) \\geq 0 (D(f_1,f_2) = 0 \\ se \\ f_1 = f_2 \\tag{2.9} \\] \\[ (f_1,f_2) = D(f_2, f_1) \\tag{2.10} \\] \\[ D(f_1,f_3) \\leq D(f_1,f_2) + D(f_2,f_3) \\tag{2.11} \\] Aplicabilidade da fórmula Euclidiana uma das formas de satisfazer as propriedades acima, desse modo podemos analisar a métrica de dois ou mais pixels de uma imagem, supondo que os pixels \\(f_1 \\ e \\ f_2\\) de coordenadas \\((x_1, x_2), (y_1, y_2)\\) para medir suas distância aplicamos: \\[D_4(f_1 , f_2) = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} \\tag{2.12} \\] Na medida de distância Euclidiana, todos os pixels menores ou igual de qualquer valor \\(d\\) formam um disco de raio \\(d\\) centro em \\(f_1\\). Como exemplo, os pontos com distância \\(D_E \\leq 3\\) de um ponto central \\((x, y)\\) formam o conjunto mostra na Figura: 2.26 Figura 2.26: Conjunto de pontos com distância Euclidiana menor ou igual de ponto central. [3, p. 33]. A distância \\(D_4\\) entre \\(f_1\\) e \\(f_2\\), também denotada de city-block, é definida como: \\[ D_4(f_1 , f_2) = |x_1 - x_2 | + |y_1 - y_2 | \\tag{2.13} \\] Os pixels com uma distância \\(D_4\\) de \\(f_4\\) menor ou igual a algum valor \\(d\\) formam um losango centrado em \\(f_1\\). Em particular, os pontos com distância \\(1\\) são os pixels com vizinhança-4 de ponto central. Por exemplo, os pontos com distância \\(D_4 \\leq 3\\) de um ponto central \\((x, y)\\) formam o conjunto mostrado na Figura 2.27 Figura 2.27: Conjunto de pontos com distância \\(D_4\\) menor ou igual a \\(3\\) de um ponto central [3, p. 33]. A distância \\(D_8\\) entre \\(f_1\\) e \\(f_2\\), também denotado de chessboard, é definida como \\(D_8(f_1 , f_2) = max(| x_1 = x_2 | , | y_1 =y_2 |)\\) Os pixels com uma distância \\(D_8\\) de \\(f_1\\) menor ou igual a algum valor \\(d\\) formam um quadrado central em \\(f_1\\). Os pontos com distância \\(1\\) são os pixels com a vizinhança-8 do ponto central. Por exemplo, os pontos com distância \\(D_8 \\leq 3\\) de um ponto central \\((x, y)\\) formam o conjunto mostrado na Figura 2.28 Figura 2.28: Conjunto de pontos com distância \\(D_4\\) menor ou igual a \\(3\\) de um ponto central [3, p. 33]. 2.7.8 Operações Lógico-aritméticas Operações lógicas e aritméticas podem ser utilizadas para modificar imagens. Embora essas operações permitam uma forma simples de processamento, há uma grande variedade de aplicações em que tais operações podem produzir resultados de interesse prático [3, p. 34]. Dadas duas imagens, \\(f_1\\) e \\(f_2\\), as operações aritméticas mais comuns entre dois pixels \\(f_1(x, y)\\) e \\(f_2(x, y)\\) são a adição, subtração. multiplicação e divisão, definida de acordo com: Operações Aritméticas: Adição: \\(f_1(x_1, y_1) + f_2(x_2, y_2)\\). Subtração: \\(f_1(x_1, y_1) - f_2(x_2, y_2)\\). Multiplicação: \\(f_1(x_1, y_1) \\cdot f_2(x_2, y_2)\\) Divisão: \\(f_1(x_1, y_1) \\div f_2(x_2, y_2)\\) Durante o processo de operações aritméticas sobre uma imagem alguns cuidados devem ser tomados para contornar a produção de valores fora do intervalo de níveis de cinza sobre a imagem original no processo. Neste caso, a operação de adição de duas imagens com \\(256\\) níveis de cinza, pode resultar em um número maior que valor \\(255\\) para determinados pixels, por outro lado, a operação de subtração de duas imagens podem resultar em valores negativos para alguns pixels durante o processo. Para solucionar esse problema depois do processo aritmético (aplicação aritmética), realizar uma transformação de escala de cinza na imagem resultante para manter seus valores do intervalo adequado [3, p. 34]. Refêrencias "],["transformacoesGeometricas.html", "Capítulo 3 Transformacões geométricas 3.1 Definição 3.2 Sistema de coordenadas objetos (2D e 3D) 3.3 Representação Vetorial e Matricial de Imagens digitalizadas 3.4 Matrizes em Computação gráfica 3.5 Transformações em Pontos e Objetos 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação", " Capítulo 3 Transformacões geométricas Nessa seção abordaremos conceitos importantes que detalham processos de transformações geométricas na imagem digitalizada, definiremos a seção proposta, sistema de coordenadas objetos (2D e 3D), representação vetorial e matricial de Imagens digitalizadas, matrizes em computação gráfica, transformações em pontos e objetos; transformações (translação, escala e rotação). 3.1 Definição As transformações geométricas são operações que podem ser utilizadas sobre uma imagem, visando alterar características como: posição , orientação, forma ou tamanho da imagem apresentada. As operações (transformações geométricas) não alteram a topologia (pixels) da imagem operada, apenas possibilitam a projeção da imagem no espaço determinado. 3.2 Sistema de coordenadas objetos (2D e 3D) Sistemas de coordenadas nos objetos (2D e 3D) podem ser usadas para modelar objetos(imagens), servindo de referência em termos de dimensões(tamanhos), rotacionamento e posições dos objetos nas operações geométricas, dentro do ambiente de aplicação. No sistema de coordenadas polares (ao dentro da imagem, Figura 3.1), as coordenadas são descritas por raio e ângulo: (r, \\(\\phi\\)). No sistema de coordenadas esférico (à esquerda), as coordenadas são descritas por raio e dois ângulos. Nos sistemas de coordenadas cilíndricos (à direita), as coordenadas são descritas por raio, ângulo e um comprimento. Os dois sistemas de extremidades são 3D. Esse tema é abordado com mais profundidade na matéria de Computação Gráfica (CG), transformações geométricas no plano e no espaço [13, p. 36]. Figura 3.1: Coordenadas Polares [13, p. 36]. 3.3 Representação Vetorial e Matricial de Imagens digitalizadas Um vetor é basicamente um segmento de reta orientada (sentido e direção). Para representar um vetor em dimensão 2D, usaremos \\(V\\), como seta que sai da origem do sistema de coordenadas, para o ponto (x, y), tendo assim a direção um sentido e um comprimento específico. [13, p. 14]. A fórmula abaixo é aplicada para calcular comprimento do vetor 2D: \\[ |V| = {\\sqrt{x^2 + y^2}} \\tag{3.1} \\] Exemplo 1: Calcule o comprimento de \\(V\\) com \\(x=2\\) e \\(y=3\\). Resolução: \\[|V| = {\\sqrt{22+ 32}} = 3.60\\] Se pensamos em \\(V\\) no espaço 3D, definindo a origem ao ponto \\(P(x, y, z)\\) seu comprimento seria: \\[|V| = {\\sqrt{x^2+y^2+z^2}}\\] Exemplo 2: Calcule o comprimento de \\(V\\) com \\(x=2\\) , \\(y=3\\) e \\(z=1\\). Resolução: \\[ |V| = {\\sqrt{2^2+3^2+1^2} = 3.74} \\tag{3.2} \\] Uma matriz é um arranjo (vetor) de elementos em duas direções (linha e coluna). Para declará-la é necessário definir a quantidade de elementos existentes em cada direção. Representaremos matriz com a letra M, as direções (linha e coluna) pelas letras L e C. Suponhamos que L = 4 e C = 4 então M[4][4] formando matriz quadrática como mostra a Figura 3.2, nela pode-se observar a matriz identidade de tamanho 4 x 4. (ref:repreMatricial) Representação matricial [13, p. 14]. Figura 3.2: (ref:repreMatricial) 3.4 Matrizes em Computação gráfica As transformações geométricas (translação, escala e rotação) podem ser representadas na forma de equações possibilitando suas manipulações. O problema é que manipulações de objetos gráficos normalmente envolvem muitas operações aritméticas simples. As matrizes são muito usadas nessas manipulações porque são mais fáceis de usar e entender do que as equações algébricas, o que explica por que programadores e engenheiros as usam extensivamente. As matrizes são parecidas com modelo organizacional da memória dos computadores. Suas representações se relacionam diretamente com estas estruturas de armazenamento, facilmente o trabalho dos e permitindo maior velocidade para aplicações críticas como jogos e aplicações em realidade virtual. É devido a esse fato que os computadores com “facilidades vetoriais” têm sido muito usados junto a aplicações de computação gráfica. Devido ao padrão de coordenadas usualmente adotado para representação de pontos no plano \\((x,y)\\) e no espaço tridimensional \\((x,y,z)\\), pode ser conveniente manipular esses pontos em matrizes quadradas de 2x2 ou 3x3 elementos. Através de matrizes e de sua multiplicação, podemos representar todas as transformações lineares 2D e 3D. Várias transformações podem ser combinadas resultando em uma única matriz denominada de matriz de transformação. Na imagem digitalizada, são aplicados elementos básicos como pontos, linhas, curvas e as superfícies tridimensionais ou mesmo os sólidos que mostram os elementos que formam as imagens sintaticamente no computador. Em computação gráfica os elementos pontos, linhas, curvas e as superfícies tridimensionais ou mesmo os sólidos são denominados primitivas vetoriais da imagem. As primitivas vetoriais são associadas a um conjunto de atributos que define sua aparência e a um conjunto de dados que define a sua geometria (pontos de controle). Para esclarecer melhor, vamos considerar alguns exemplos, dois elementos facilmente caracterizados como vetoriais, pela noção de vetores já discutida são os pontos e linhas retas. A cada elemento de um conjunto de pontos associa-se uma posição, que pode ser representada por suas coordenadas (geometria), e uma cor, que será como esses pontos aparecerão na tela (tributos). No caso de um conjunto de linhas retas, cada uma pode ser definida pelas coordenadas de seus pontos extremos (geometria) e sua cor, espessura, ou ainda se aparecerá pontilhada ou tracejada (atributos). A descrição matricial é típica das imagens digitalizadas capturadas por scanners ou utilizadas nos vídeos. É a forma de descrição principal na análise e no processamento de imagens. Em computação gráfica sintética, surgem nos processos de finalização (ray tracing, z-buffers). Na representação matricial, a imagem é descrita por um conjunto de células em um arranjo especial bidimensional, uma matriz. Cada célula representa os pixels (ou pontos) da imagem matricial. Os objetos são formados usando adequadamente esses pixels. A figura 3.3 mostra a representação matricial da imagem digitalizada e sua tela. Essa é a representação usualmente empregada para formar a imagem nas memórias e telas dos computadores e na maioria dos dispositivos de saída gráficos (impressoras e vídeos) [13, p. 14]. Figura 3.3: Descrição de imagens matriciais por conjunto de pixels [13, p. 15]. 3.5 Transformações em Pontos e Objetos A habilidade de representar uma objeto em várias posições no espaço para compreender sua forma. A possibilidade de submetê-lo a diversas transformações é importante em diversas aplicações da computação gráfica [Rogers, 1990]. As operações geométricas de rotação e translação de objetos são chamadas operações de corpos rígidos[13, p. 38]. 3.6 Transformação de Translação A transformação geométrica translação tem como objetivo movimentar a objeto(imagem) no espaço (ambiente projetado), usando a matriz dedos (pontos) do objeto, aplica-se operações da nova coordenada p(x, y) sobre os pontos da matriz possibilitando transladar ao novo espaço da coordenada definida. Para transladar (mover) um objeto do ponto atual para o novo ponto, p(x) e p(y) pode ser movido por Txunidades em relação ao x, e por Ty, unidades em relação eixo . Logo nova posição do ponto p(x,y) passa a ser p’(x) e p’(y) , que podem ser escrito como: \\[ x’ = x + Tx \\tag{3.3} \\] \\[ y’ = y + Ty \\tag{3.4} \\] No caso do ponto for representado por vetor, p = (x, y), a translação desse mesmo ponto para o novo pode ser obtida pela adição de vetor de deslocamento à posição atual do ponto [13, p. 38]: \\[ p’ = p+ T = [x’y’] = [xy] + [Tx Ty] \\tag{3.5} \\]. A figura 3.4, translação de um triângulo de três unidades na horizontal e-4 na vertical. Repare que se teria o mesmo efeito transladado a origem do sistema de coordenadas para o ponto p(-3, 4)na primeira figura [13, p. 39]. Figura 3.4: Tranformação de translação [13, p. 39]. 3.7 Transformação de Escala A transformação geométrica escala tem como objetivo mudar as dimensões (tamanho) do objeto(imagem) no espaço (ambiente projetado)[13, p. 40]. Para fazer com que uma imagem definida por conjunto de pontos mude de tamanho, teremos de multiplicar os valores de suas coordenadas por um fator de escala. Transformar um objeto por alguma operação nada mais é do que fazer essa operação com todos os seus pontos. Nesse caso um dos vetores de suas coordenadas são multiplicados por fatores de escala. Estes fatores de escala em 2D podem, por exemplo ser \\(Ss\\) e \\(Sy\\): \\[ x’ = x * Sx \\tag{3.6} \\] \\[ y’ = y * Sy \\tag{3.7} \\] Essa operação pode ser representada na forma matricial: Figura 3.5: Tranformação de escala [13, p. 40]. A mudança de escala de um ponto de um objeto no espaço tridimensional pode ser obtida pela multiplicação de três fatores de escala ao ponto. A operação de mudança de escala pode ser descrita pela multiplicação de coordenadas do ponto por uma matriz diagonal cujos valores dos elementos não-nulos sejam os fatores de escala [13, p. 41]. Assim, no caso 3D tem-se: Figura 3.6: Escala espaço tridimencional [13, p. 41]. 3.8 Transformação de Rotação A transformação geométrica da rotação tem como objetivo rotacionar objeto(imagem) no espaço (ambiente projetado), é equivalente a girar ao redor da origem do sistema de coordenadas como na Figura 3.7. Na Figura 3.7 rotação de um ponto P em torno da origem, passando para a posição P’. Repare que se chegaria a esse mesmo ponto através de uma rotação de - no sistema de eixos XY [13, p. 41]. Figura 3.7: Transformação de Rotação [13, p. 42]. Se um ponto de coordenada (x,y), distante \\(r=(x2 + y2)^\\frac12\\) da origem do sistema de coordenadas, for rotacionado de um ângulo \\(\\phi\\) em torno da origem, suas coordenadas, que antes eram definidas como: \\(x = r * \\cos(\\phi), y =r * \\sin(\\phi)\\), passam a ser descritas como (x’, y’) dadas por [13, p. 42]: \\[ x&#39; = r * \\cos(\\theta + \\phi) = r * \\cos\\phi * cos\\theta - r * \\sin\\phi * \\sin\\theta \\tag{3.8} \\] \\[ x&#39; = r * \\sin(\\theta + \\phi) = r * \\sin\\phi * cos\\theta + r * \\cos\\phi * \\sin\\theta \\tag{3.9} \\] isso equivale às expressões: \\[ x&#39; = x\\cos(\\phi) - y\\sin(\\phi) \\tag{3.10} \\] \\[ y&#39; = y\\cos(\\phi) + y\\sin(\\phi) \\tag{3.11} \\] Essas expressões podem ser descritas pela multiplicação do vetor de coordenadas do ponto (x,y) pela matriz: Figura 3.8: Matriz de rotação [13, p. 42]. Para alterar a orientação de um objeto(imagem) em torno de um certo ponto realizando uma combinação da rotação com a translação, é necessário, antes de aplicar a rotação de um ângulo no plano das coordenadas em torno de um ponto, realizar uma translação para localizar esse ponto na origem do sistema, aplicando a rotação desejada e, então, uma translação inversa para localizar o dado ponto na origem [13, p. 42]. A multiplicação de coordenadas por uma matriz de rotação pode resultar em uma translação. Figura 3.9: Proceso de aplicação rotação [13, p. 43]. Refêrencias "],["transformacoesRadiometricas.html", "Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência 4.4 Processamento de histograma 4.5 Equalização do histograma 4.6 Especificação de histograma", " Capítulo 4 Transformações radiométricas As manipulações no domínio espacial ocorrem diretamente sobre os pixels no plano da imagem. As duas principais categorias de transformações de intensidade a nível espacial são transformações radiométricas e filtragem espacial. A filtragem espacial pode ser representada pela expressão (4.1). \\[ g(x,y) = T[f(x, y)] \\tag{4.1} \\] O componente \\(f(x, y)\\) é a imagem de entrada, \\(g(x, y)\\) é a imagem de saída, e \\(T\\) é um operador em \\(f\\) definido em uma vizinhança do ponto \\((x, y)\\). Este procedimento pode ser aplicado como na Figura 4.1, em que um ponto \\((x, y)\\) está destacado com sua vizinhança. Geralmente a vizinhança é retangular e bem menor que a imagem, e no caso da Figura é um quadrado de tamanho 3 x 3. Figura 4.1: Processamento no domínio espacial [2, p. 69]. Na filtragem espacial, o valor da intensidade no centro da vizinhança é alterado de um pixel ao outro enquanto se aplica um operador \\(T\\) aos pixels na vizinhança para gerar a saída na posição central. O processo pode começar no canto superior esquerdo da imagem de entrada e avançar pixel por pixel horizontalmente, uma linha por vez. Nas bordas, os vizinhos externos são ignorados nos cálculos ou se preenche a imagem com uma borda de 0’s ou outros valores predefinidos. A vizinhança e uma operação predefinida \\(T\\) definem o filtro espacial (também denominada máscara espacial, kernel, template ou janela). A menor vizinhança possível, de tamanho 1 x 1, é tratada como uma função de transformação de intensidade (transformação radiométrica). Na transformação radiométrica, a intensidade \\(s\\) em cada ponto da imagem \\(g\\) depende apenas do valor \\(r\\) em um único ponto na imagem \\(f\\) , como na expressão (4.2). \\[s = T(r) \\tag{4.2}\\] Como as transformações de intensidade operam individualmente nos pixels de uma imagem, são chamadas de técnicas de processamento ponto-a-ponto [2, p. 69]. Este processo é utilizado, por exemplo, para fins de manipulação de contraste e limiarização de imagem. Já a filtragem espacial, também muito aplicada para realce de imagens, é uma técnica de processamento por vizinhança [2, p. 69]. Nesta seção serão apresentados alguns exemplos de realce de imagem, que tem o foco em melhorar o aspecto da imagem, tornando-a mais viável para o seu objetivo. O realce pode ser utilizado para minimizar na imagem efeitos de ruídos, perda de contraste, borramento e distorções. Na Figura 4.2 são mostradas as três funções radiométricas \\(T\\) mais básicas aplicadas na transformação de intensidade, frequentemente utilizadas para o realce de imagens. Para cada uma das três funções - linear (transformações de negativo), logarítmica (transformações de log e log inverso) e de potência (transformações de n-ésima potência e n-ésima raiz). Figura 4.2: Funções de transformação de intensidade [2, p. 71]. 4.1 Transformação Linear O negativo de uma imagem com níveis de intensidade na faixa \\([0, L – 1]\\) é obtido pela transformação (4.3): \\[s = L – 1 – r \\tag{4.3}\\] Esse tipo de processamento pode ser utilizado para realçar detalhes brancos ou cinza em regiões escuras de uma imagem [2, p. 70]. Na Figura 4.3 é exemplificado uma aplicação da transformação de negativo. A imagem original é uma mamografia digital, 4.3(a), mostrando uma pequena lesão. Após a transformação se torna mais fácil analisar o tecido mamário no negativo da imagem 4.3(b). Figura 4.3: Transformação Linear (Negativos de Imagens). (a) Mamografia digital original. (b) Negativo da imagem. [2, p. 71]. 4.2 Transformação Logarítmica A forma geral da transformação logarítmica é (4.4): \\[s = c log (1 + r) \\tag{4.4}\\] em que \\(c\\) é uma constante e considera-se que \\(r \\geq 0\\). Nas aplicações Log, um dos objetivos é a expansão dos valores de pixels mais escuros em uma imagem, ao mesmo tempo em que se comprime os valores de níveis mais altos [2]. A expansão é quando se mapeia uma faixa estreita de baixos valores de intensidade em uma faixa mais ampla de níveis de saída, como na função Log da Figura 4.4. Na compressão, ocorre o oposto com os valores mais altos de níveis de intensidade. Na transformação logarítmica inversa, comprime-se os pixels mais escuros e se expande os mais claros [2, p. 71]. Uma maneira de avaliar o efeito da transformação logarítmica é utilizar sobre o espectro de Fourier. Na Figura 4.4(a) mostra um espectro de Fourier com valores variando de 0 a 1,5 × 106, com baixo nível de detalhamento. Ao aplicar a transformação Log (com \\(c = 1\\) neste caso) aos valores do espectro, a faixa de valores do resultado passa a ser de 0 a 6,2, o que melhora o detalhamento na exibição da imagem 4.4(b). Figura 4.4: Transformação Logarítmica. (a) Espectro de Fourier. (b) Resultado da aplicação da transformação logarítmica.[2, p. 72]. 4.3 Transformação de Potência As transformações de potência apresentam a forma básica (4.5): \\[s = cr^\\gamma \\tag{4.5}\\] sendo \\(c\\) e \\(\\gamma\\) constantes positivas. Ao plotar a transformação de potência para diferentes valores \\(\\gamma\\), e \\(c=1\\), na Figura 4.5, se observa um comportamento semelhante ao de expansão/compressão da transformação Logarítmica. Curvas de transformação de potência com valores de \\(\\gamma\\) menores que 1 tem um efeito parecido com a função logarítmica, enquanto que para valores de \\(\\gamma\\) maiores que 1 se parecem mais com a logarítmica inversa. Figura 4.5: Plotagens da equação \\(s = cr^\\gamma\\) para vários valores de \\(\\gamma\\) [2, p. 72]. Uma das aplicações da transformação de potência é a correção gama em dispositivos que funcionam de acordo com uma lei de potência, como em computadores [2, p. 72]. Por exemplo, dispositivos de tubo de raios catódicos apresentam relação com a função potência de expoentes variando em aproximadamente 1,8 a 2,5. Para valores de gama próximos de 2,5 a imagem de saída no monitor tende a ser mais escura (Figura 4.6(b)) que a imagem original (Figura 4.6(a)). A imagem corrigida (Figura 4.6(c)) pela correção gama, neste caso com gama menor que 1, gera uma saída (Figura 4.6(d) mais parecida com a imagem original (Figura 4.6(a)). Figura 4.6: Correcão Gama. (a) Imagem com variação gradativa de intensidade (gradiente). (b) Imagem vista em um monitor simulado com gama igual a 2,5. (c) Imagem com correção gama. (d) Imagem corrigida vista no mesmo monitor. [2, p. 72]. Outra utilidade da transformação de potência pode ser vista na Figura 4.7, em que a imagem original está desbotada, indicando que se deve aplicar uma compressão dos níveis mais baixos e expandir valores mais altos [2, p. 72]. Assim, a transformação foi realizada com gama maior que 1. Os resultados do processamento com \\(\\gamma = 3, \\gamma = 4,\\) e \\(\\gamma = 5\\) podem ser vistos nas imagens 4.7(b), 4.7(c) e 4.7(d), respectivamente. Figura 4.7: Transformações de potência. (a) Imagem aérea. (b) a (d) Resultados da aplicação da transformação de potência com \\(c = 1 \\text{ e } \\gamma = 3, 4 \\text{ e } 5\\), respectivamente. [2, p. 74]. 4.4 Processamento de histograma A distribuição dos níveis de intensidade \\(L\\) de uma imagem podem ser identificados em um histograma, ou seja, um gráfico com o número de pixels na imagem para cada nível de cinza. Assim, os histogramas podem servir de referência para várias manipulações no domínio espacial, além de fornecer estatísticas das imagens e ser útil em aplicações como compressão e segmentação. O histograma também pode ser interpretado como uma distribuição discreta da probabilidade de ocorrência do nível de intensidade \\(r_k\\) em uma imagem (4.6) [2, p. 78]: \\[p(r_k) = \\frac{n_k}{MN} \\text{ para k = 0, 1, 2, ..., L - 1} \\tag{4.6}\\] sendo \\(M\\) e \\(N\\) as dimensões de linha e coluna da imagem, e \\(n_k\\) é o número de pixels da imagem com intensidade \\(r_k\\). Na Figura 4.8, estão identificados quatro histogramas referentes a cada uma das imagens dos grãos de pólen do lado esquerdo. O eixo horizontal de cada histograma corresponde a valores de intensidade, \\(r_k\\), e o eixo vertical são os valores de \\(p(r_k)\\). Cada imagem destaca uma característica em relação à intensidade da imagem: escura, clara, baixo contraste e alto contraste. Na imagem do topo, a mais escura, as barras do histograma estão concentradas no lado inferior esquerdo (escuro) da escala de intensidades, enquanto que na imagem mais clara tendem à região oposta [2, p. 79]. Uma imagem com baixo contraste, aparência desbotada e sem brilho, tem um histograma estreito normalmente localizado no meio da escala de intensidades. Os componentes do histograma na imagem de alto contraste estão distribuídos quase uniformemente em uma ampla faixa da escala de intensidades, com poucas linhas verticais sendo muito mais altas do que as outras. As imagens de alto contraste tendem a apresentar uma boa correspondência em relação aos detalhes de nível de cinza [2, p. 78]. Figura 4.8: Histogramas de uma imagem com grãos de pólen. De cima para baixo: escura, clara, baixo contraste e alto contraste. [2, p. 79]. Os histogramas são utilizados em grande parte para auxiliar em transformações de intensidade com foco em melhorar o contraste, tornando mais fácil a percepção de informações de interesse na imagem [3]. Nos exemplos de transformações apresentados nesta seção (linear, logarítmica e de potência), a escolha do operador \\(T\\) de transformação geralmente é empírica, em que se deve considerar a imagem original e o efeito desejado. Para transformar a imagem de forma que se altere o histograma de uma maneira específica e automática utilizam-se métodos em que o formato dos histogramas são pré-definidos ou atendem a um determinado padrão, como na equalização de histograma e especificação de histograma [2]. 4.5 Equalização do histograma Na equalização do histograma, o histograma da imagem original é alterado de maneira que a imagem transformada tenha uma distribuição aproximadamente uniforme dos níveis de cinza em uma faixa mais ampla de valores, assumindo características próximas de uma imagem de alto contraste [3]. Neste método se aplica um operador de transformação de intensidade \\(T\\) na forma contínua, Equação (4.7), que gera uma variável aleatória (valores de intensidade da imagem transformada) caracterizada por uma função densidade de probabilidade uniforme (PDF, probability density function) . \\[s = T(r) = (L - 1)\\int_{0}^{r} p_r(w)dw \\tag{4.7}\\] sendo \\(w\\) uma variável local de integração, \\(L\\) os níveis de cinza da imagem e \\(p_r(r)\\) a PDF de \\(r\\) (valores de intensidade da imagem original). A demonstração da obtenção da PDF da variável \\(s (p_s(s))\\), na Equação (4.8), pode ser vista com detalhes no livro “Processamento digital de imagens” [2]. O gráfico das PDF’s de \\(r\\) e \\(s\\) estão identificados na Figura 4.9 como \\(p_r(r)\\) e \\(p_s(s)\\). \\[p_s(s) = \\frac{1}{L-1} \\text{ para } 0 \\leq s \\leq L - 1 \\tag{4.8}\\] Figura 4.9: Equalização de histograma na forma contínua. (a) Uma PDF arbitrária. (b) Resultado da aplicação da transformação para equalização [2, p. 81]. Para que esses conceitos sejam aplicados no processamento de imagens, eles devem ser expressos na forma discreta, Equação (4.9). \\[s_k = T(r_k) = (L - 1) \\sum_{j=0}^{k}p_r(r_j) = \\frac{(L-1)}{MN}\\sum_{j=0}^{k}n_j \\text{ para } k = 0, 1, 2,\\dots, L - 1 \\tag{4.9}\\] Esta expressão deve ser aplicada para cada pixel da imagem de entrada com intensidade \\(r_k\\), obtendo-se um pixel correspondente com nível \\(s_k\\) na imagem de saída, estes resultados são arredondados para o nível de cinza mais próximo, que é um valor inteiro. Diferente do equivalente contínuo, não pode ser provado (em geral) que a equalização de um histograma discreto resulta em um histograma uniforme [2, p. 81]. No entanto, como identificado na Figura 4.10, a equalização discreta tende a espalhar o histograma da imagem de entrada, aumentando o intervalo da escala de intensidade. Figura 4.10: Equalização de histograma[3, p. 113]. 4.6 Especificação de histograma Algumas vezes é útil poder especificar o formato do histograma da imagem processada. O método conhecido como especificação de histograma modifica uma imagem para que seu histograma tenha uma distribuição particular[3]. Sejam, \\(p_r(r)\\) e \\(p_z(z)\\), as funções densidade de probabilidade (PDF) original e especificada, respectivamente. Pela Equação (4.9), o histograma da imagem original pode ser equalizado, representado pela transformação \\(T\\). Na Figura 4.11(a), \\(T_2(z)\\) é a função de transformação que realiza a equalização do histograma especificado. O método para obter os níveis de cinza \\(z\\) da imagem processada envolve o cálculo da função de transformação inversa \\(z = T_2^{-1}(s)\\), como identificado na Figura 4.11(b). O termo \\(s\\) corresponde às intensidades da imagem equalizada para os dois caminhos de transformação e \\(p_s(s)\\) a sua PDF. Figura 4.11: Etapas de especificação de histograma. (a) equalização do histograma especificado pela função \\(T_2(z)\\). (b) cálculo da função de transformação inversa \\(T_2^{-1}(s)\\)[3, p. 116]. Assim, a especificação de histograma envolve a aplicação de duas transformações, \\(T_1(r)\\) seguida de \\(T_2^{-1}(s)\\). Na forma discreta não é necessário calcular a função inversa \\(T_2^{-1}(s)\\), é possível determinar os valores de \\(z\\) de forma mais direta por mapeamento em tabela[2]. Após o cálculo das duas transformações, \\(T_1(r)\\) e \\(T_2(z)\\), os valores são arredondados para os valores inteiros mais próximos no intervalo \\([0, L – 1]\\). Com os valores organizados em uma tabela, se faz o mapeamento de cada valor \\(s_k\\), procurando a melhor correspondência nos valores obtidos em \\(T_2(z_q)\\), determinando a solução mais próxima \\(z_q\\). Estas etapas ficam mais compreensíveis ao se aplicar a um exemplo. Suponha que uma imagem de 3 bits \\((L = 8)\\) de dimensões 64 × 64 pixels \\((MN = 4096)\\) tenha a distribuição de intensidade da Figura 4.12, na qual os níveis de cinza são números inteiros no intervalo \\([0, L – 1] = [0, 7]\\). Figura 4.12: Distribuição de intensidades para uma imagem digital de 3 bits, 64 × 64 pixels[2, p. 83]. O histograma da imagem hipotética é apresentado na Figura 4.13(a). Os valores ajustados do histograma equalizado \\(s\\), obtidos pela função \\(s =T_1(r)\\), Equação (4.9), são: \\(s_0 = 1, s_1 = 3, s_2 = 5, s_3 = 6, s_4 = 6, s_5 = 7, s_6 = 7, s_7 = 7\\) Figura 4.13: Exemplo de especificação de histograma. (a) Histograma de uma imagem de 3 bits. (b) Histograma especificado desejado. (c) Função de transformação obtida a partir do histograma especificado desejado. (d) Resultado da especificação do histograma.[2, p. 88]. No próximo passo, todos os valores de \\(T_2(z_q)\\) são calculados da mesma forma que \\(T_1(r)\\), Equação (4.9) de equalização, em que os valores de \\(p_z(z_q)\\) estão na Figura 4.14. Os resultados desta segunda etapa estão na Figura 4.15 e a função de transformação é esboçada na Figura 4.13(c). Figura 4.14: Histograma especificado[2, p. 88]. No terceiro passo, sendo \\(G(z_q) = T_2(z_q)\\), verifica-se o menor valor de \\(z_q\\) na Figura 4.15 de modo que \\(G(z_q)\\) seja o mais próximo possível de \\(s_k\\) . Isso é feito para todos os valores de \\(s_k\\) para criar os mapeamentos necessários de \\(s\\) em \\(z\\), Figura 4.16. Figura 4.15: Valores da função de transformação \\(G(z_q) = T_2(z_q)\\) arredondados[2, p. 88]. Na última etapa, o mapeamento na Figura 4.16 juntamente com a distribuição dos pixels na Figura 4.12 é utilizado para mapear cada pixel na imagem do histograma equalizado \\(p_s(s)\\) em um pixel correspondente na imagem do histograma especificado recém-criado \\(p_z(z)\\). O histograma resultante da especificação, calculado com base na equação (4.6) é esboçado na Figura 4.13(d). Figura 4.16: Mapeamentos dos valores de \\(s_k\\) nos valores correspondentes de \\(z_q\\).[2, p. 89]. Refêrencias "],["filtros.html", "Capítulo 5 Filtros Digitais 5.1 Convolução 5.2 Filtro da Média 5.3 Filtro da Mediana 5.4 Filtro Gaussiano", " Capítulo 5 Filtros Digitais Filtros digitais representam um conjunto de ferramentas poderosas para realizar operações em imagens. Diferente das operações de ponto que operam sobre um único pixel, as operações utilizando filtros levam em consideração os pixels próximos ao pixel que atualmente está em modificação. Isso nos permite realizar alterações muito mais complexas do que as realizadas anteriormente. Como exemplos disso podemos considerar a operação sharpen (aguçamento) e blur (suavização), que podem ser observados na Figura 5.1(a) e (b), respectivamente. Figura 5.1: Imagem de ônibus com filtro de aguçamento e de suavização[9, p. 98] 5.1 Convolução A convolução é uma operação muito utilizada em PDI tendo suas origens na matemática, onde é definida como uma operação realizada entre duas funções que resulta numa terceira. Em outras palavras, a convolução recebe dois sinais como entrada e gera um sinal como saída. No caso de PDI, podemos imaginar os sinais de entrada como sendo a nossa imagem original e o kernel do filtro, e a nossa saída como sendo a imagem filtrada. Quando dizemos kernel, estamos nos referindo a uma função ou, no caso de PDI, a uma matriz que é aplicada a nossa imagem usando a convolução que irá produzir outra imagem como saída. Na Tabela 5.1, temos exemplos de alguns tipos de kernels que podem ser utilizados na convolução e seus respectivos resultados. Além dos efeitos mais comuns, como o de desfoque (blur), podemos utilizar kernels que extraiam informações mais complexas das imagens, como os detectores de borda, que serão explicados nos próximos tópicos. Tabela 5.1: Exemplo de kernels (adaptada de [14]). Operação Kernel Resultado Identidade (Imagem Original) \\[\\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] Detecção de borda \\[\\begin{bmatrix} -1 &amp; -1 &amp; -1\\\\ -1 &amp; 8 &amp; -1\\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}\\] Média (box blur) \\[\\frac{1}{9}\\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\] Gaussian blur \\[\\frac{1}{16}\\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\ 2 &amp; 4 &amp; 2\\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\] Em porcessamento de sinais, a operação de convolução, geralmente, é representada pelo símbolo \\(*\\) e pode ser descrita, em poucas palavras, como uma soma de produtos entre um ponto do kernel e um ponto correspondente da imagem. Essa correspondência é encontrada fazendo um deslizamento sobre a imagem de entrada. A Figura 5.2 representa de maneira visual a convolução em um cenário de uma dimensão, através do sinal de entrada (signal) e o kernel. Pode-se ainda perceber que o kernel está rotacionado em 180º, pelo fato da definição de convolução. Figura 5.2: Convolução unidimensional[15]. Algo interessante que podemos observar na Figura 5.2 é que nosso sinal de entrada é quase totalmente 0 e contém um único ponto com o valor 1, assim, nosso resultado será uma cópia do kernel. A partir disso, conseguimos imaginar o porquê de termos como resultado a própria imagem quando aplicado um kernel identidade, como mostrado na primeira linha da Tabela 5.1. Antes de irmos mais adiante no assunto, é importante serem esclarecidos alguns conceitos para que não se tornem confusos. Existe outra operação matemática extremamente parecida com a convolução, que é chamada de correlação, com a diferença de que não se rotaciona o kernel. Para entendermos bem essa diferença, podemos observar a Figura 5.3, onde temos um exemplo de correlação e convolução sendo executados em um espaço unidimensional. Temos uma função \\(f\\) e um filtro \\(w\\) na Figura 5.3(a) e (b), respectivamente. Na sequência, em (b) e (j), temos as funções e os filtros prontos para iniciar a correlação e a convolução. Nas etapas (c) e (k), podemos ver o preenchimento com zeros, porque há partes das funções que não se sobrepõem, dessa forma, permite que \\(w\\) percorra todos os pixels de \\(f\\). Após isso, é realizado o primeiro passo da correlação e convolução, onde podemos observar que o resultado é 0, já que \\(w\\) está sobreposto por somente zeros; logo, a soma da multiplicação de cada item de \\(w\\) por \\(f\\) será zero. Desloca-se, então, o filtro \\(w\\) em uma unidade à direita, onde o resultado novamente será 0. O primeiro resultado não nulo se dará no terceiro deslocamento, sendo 8 para a correlação e \\(1\\) para a convolução. Temos o resultado de ambas operações em (g) e (o), e o resultado recortado em (h) e (p) que remove os zeros até o tamanho ficar igual ao da \\(f\\) inicial. Figura 5.3: Ilustração de correlação e convolução unidimensional[2, p. 96]. Vamos extender agora essas duas operações à aplicação em duas dimensões. Uma representação disso pode ser vista na Figura 5.4, onde temos novamente o kernel \\(w\\) e a função \\(f\\). Percebe-se, novamente, o efeito de se aplicar o kernel em uma imagem com apenas o número 1 no meio, nos dois casos temos como saída a cópia do kernel aplicado, com a diferença que na correlação o kernel aparece rotacionado no resultado. Assim, nota-se que, se pré-rotacionarmos, a correlação produzirá o mesmo que uma convolução. Já que a correlação e convolução são iguais, é preciso saber qual delas utilizar. Segundo Gonzalez, [2, p. 98], isso é uma questão de preferência e qualquer uma das duas operações conseguem realizar a outra com uma simples rotação do kernel. Essa questão se torna ainda menos relevante quando utilizamos filtros que são simétricos, pois temos o mesmo kernel antes e após a rotação, logo, tanto na correlação quanto na convolução, nos darão o mesmo resultado. Entretanto, usando kernels assimétricos, temos resultados diferentes. Conforme Moeslund, [4, p. 87], quando trabalhamos com filtros de desfoque, detectores de borda, entre outros, o processo de se aplicar o kernel é comumente chamado de convolução mesmo quando na prática se é feita a correlação. Figura 5.4: Ilustração de correlação e convolução bidimensional[2, p. 98]. 5.1.1 Definção matemática da convolução Vamos explorar um pouco das notações matemáticas utilizadas para representar a convolução e a correlação, pois, assim, também poderemos consolidar a idéia de como são tão similares. Como dito no início desta seção, geralmente, a convolução é identificada por \\(*\\) e a correlação por \\(\\star\\). A correlação em duas dimensões segue a Equação (5.1) \\[g(x,y) = w(x,y)\\star f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x+s,y+t) \\tag{5.1}\\] em que \\(w\\) é nosso \\(kernel\\) e \\(f\\) nossa imagem. Percebe-se que ambas são funções de duas variáveis, \\(x\\) e \\(y\\), pois estamos trabalhando em duas dimensões. Os limites dos somatórios são dados por \\(a=(m-1)/2\\) e \\(b=(n-1)/2\\) e o que essa função faz é andar em cada posição da imagem, ou seja, \\((x,y)\\), e substituir o pixel atual pela soma de produtos entre os valores do \\(kernel\\) e os valores dos pixels da imagem. Já a convolução tem uma equação bem similar, sendo diferente apenas pelos sinais negativos em \\(f\\), o que evidência a rotação do \\(kernel\\). Pode-se notar que os sinais inversos estão em \\(f\\) e não em \\(w\\). Segundo Gonzalez[2, p. 98], isso é usado para fins de simplicidade de notação e não alteram o resultado. \\[g(x,y) = w(x,y)*f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x-s,y-t)\\]. Uma das melhores maneiras de entender bem as equações é através de um exemplo prático. A seguir, um exemplo passo-a-passo de correlação: Etapa 1 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(0,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(-1,-1\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot0+0\\cdot2+\\left(-2\\right)\\cdot1\\\\ +1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ =0\\,-2-3\\,=\\,-5 \\end{gathered} \\] Etapa 2 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(0,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,1+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,2\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,2\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,2\\right) =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 +2\\cdot2+0\\cdot1+\\left(-2\\right)\\cdot0 +1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1 =0\\,+4+8=\\,12 \\end{gathered} \\] Etapa 3 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(0,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,2+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,3\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,3\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,3\\right) =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 +2\\cdot1+0\\cdot0+\\left(-2\\right)\\cdot0 +1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0 =0\\,+2+3=\\,5 \\end{gathered} \\] Etapa 4 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(1,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,0+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,1\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,1\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,1\\right) =\\,1\\cdot0+0\\cdot2+\\left(-1\\right)\\cdot1 +2\\cdot0+0\\cdot9+\\left(-2\\right)\\cdot3 +1\\cdot0+0\\cdot5+\\left(-1\\right)\\cdot4 =\\left(-1\\right)\\,+\\left(-6\\right)+\\left(-4\\right)=\\,-11 \\end{gathered} \\] Etapa 5 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(1,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,1+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,2\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,2\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,2\\right) =\\,1\\cdot2+0\\cdot1+\\left(-1\\right)\\cdot0 +2\\cdot9+0\\cdot3+\\left(-2\\right)\\cdot1 +1\\cdot5+0\\cdot4+\\left(-1\\right)\\cdot2 =2\\,+16+3=\\,21 \\end{gathered} \\] Etapa 6 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(1,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,2+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,3\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,3\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,3\\right) =\\,1\\cdot1+0\\cdot1+\\left(-1\\right)\\cdot0 +2\\cdot3+0\\cdot1+\\left(-2\\right)\\cdot0 +1\\cdot4+0\\cdot2+\\left(-1\\right)\\cdot0 =1\\,+6+4=\\,11 \\end{gathered} \\] Etapa 7 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(2,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,0+t\\right)\\text{=} \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,1\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,1\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,1\\right) =\\,1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3 +2\\cdot0+0\\cdot5+\\left(-2\\right)\\cdot4 +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 =\\left(-3\\right)+\\left(-8\\right)+0=\\,-11 \\end{gathered} \\] Etapa 8 de 9 \\[ \\begin{gathered} \\text{w}\\text{*f}\\left(2,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,1+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,2\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,2\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,2\\right) =\\,1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1 +2\\cdot5+0\\cdot4+\\left(-2\\right)\\cdot2 +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 =8+6+0=\\,14 \\end{gathered} \\] Etapa 9 de 9 \\[ \\begin{gathered} \\text{w}\\text{f}\\left(2,2\\right)\\text{=}\\sum{s}^{}\\sum{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,2+t\\right)\\text{=} \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,3\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,3\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,3\\right) =,1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0 +2\\cdot4+0\\cdot2+\\left(-2\\right)\\cdot0 +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 =3+8+0=11 \\end{gathered} \\] Figura 5.5: Exemplo da aplicação da correlação sobre uma imagem. A matriz verde é a imagem, a azul claro é o kernel e a azul escuro é a imagem resultante. Fonte: autoria própria inspirado em http://www.songho.ca/dsp/convolution/convolution2d_example.html e https://arxiv.org/abs/1603.07285. O exemplo utiliza um \\(kernel\\) assimétrico. Então, os resultados da correlação e convolução são diferentes. Deixaremos isso como um exercício ao leitor, realizar a convolução para provar essa diferença. O caso anterior também tem uma característica que é o complemento das bordas com 0’s, constant padding, para que o \\(kernel\\) pudesse ser aplicado sobre toda a imagem. A seguir, tem-se métodos alternativos para lidar com essa característica[9, p. 125], [16, p. 60]: Preenchimento com constante (constant padding): Método onde as laterais são preenchidas com um valor constante, comumente 0. Preenchimento com vizinho (nearest neighbor): Se realiza o preenchimento das bordas adicionais com os valores dos vizinhos mais próximos. Reflexão (reflect): Os pixels das bordas da imagem são repetidos nas bordas adicionais. Repetição (ou wrap): A imagem é repetida nas bordas. Além dos métodos apresentados, existem outras técnicas que podem ser utilizadas, sendo que em cada caso se escolhe a que melhor se ajusta à tarefa realizada. O último ponto a ser explorado é a escolha do tamanho ideal do preenchimento que deve ser aplicado à imagem. De forma geral, se temos uma imagem (\\(f\\)), um kernel (\\(w\\)) e um preenchimento (\\(p\\)), podemos escrever a seguinte relação1(5.2) \\[saída = (f_{vertical} - w_{vertical} + p_{vertical} + 1) \\times (f_{horizontal} - w_{horizontal} + p_{horizontal} + 1) \\tag{5.2}\\] Isso nos leva a perceber que o tamanho da imagem de saída aumenta conforme o preenchimento. Na maioria dos casos, queremos que a imagem de entrada e saída tenha o mesmo tamanho, então, usamos \\(p_{vertical} = k_{vertical} - 1\\) e \\(p_{horizontal} = k_{horizontal} - 1\\). E, como o kernel tem tamanho ímpar geralmente, utilizamos \\(\\frac{p_{vertical}}{2}\\) e \\(\\frac{p_{horizontal}}{2}\\). 5.2 Filtro da Média O filtro da média é um tipo de filtro que utiliza a média dos valores dos pixels próximos ao pixel central. Como esse tipo de filtro realiza uma operação linear, ele é classificado como um filtro linear de suavização [2, p. 101]. Essa suavização se dá exatamente pelo tipo da operação utilizada, a média dos pixels vizinhos, que diminui a nitidez pela redução das transições abruptas nos níveis de intensidade. Um dos problemas que podem ocorrer é que as bordas também são mudanças abruptas, então podem ser comprometidas pelo filtro[2, p. 101]. Na Figura 5.6, temos um exemplo do efeito desse filtro. Figura 5.6: Imagem de ônibus com filtro de aguçamento e de suavização[9, p. 98]. A máscara (kernel) de um filtro de média 3x3 pode ser representada por (5.3): \\[\\begin{bmatrix} \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9} \\end{bmatrix} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\tag{5.3}\\] A primeira forma é a soma de todos os valores divididos por 9, o qual é o tamanho do filtro. A segunda forma, onde todos os coeficientes do kernel são 1’s, é mais eficiente computacionalmente, pois realizamos todas as somas e multiplicações antes de dividirmos. Esse tipo de filtro é muitas vezes chamado de box filter. Figura 5.7: Imagem com diferentes tamanhos de filtro de média. (b) a (f) resultados da suavização com filtros quadrados de tamanhos 3, 5, 9, 15 e 35, respectivamente.[2, p. 102]. Na Figura 5.7, temos um exemplo onde foram aplicados filtros de média com diferentes tamanhos, 3x3(b), 5x5(c), 9x9(d), 15x15(e) e 35x35(f). Podemos notar que com o menor filtro, o de tamanho 3, temos um leve borramento na imagem toda, mas que as partes que tem o mesmo tamanho ou são menores que o filtro tem um borramento maior. Isso exemplifica umas das importantes aplicações dos filtros de suavização, que é a de desfocar os objetos menores e deixar os maiores em maior evidência[2, p. 101]. Figura 5.8: Exemplo de uso do desfoque através de um kernel 15x15 para uma melhor segmentação[2, p. 103]. Na Figura 5.8, podemos ver como o desfoque pode ser utilizado para encontrar os detalhes principais da imagem. Nesta imagem obtida a partir do telescópio Hubble, foi aplicado o desfoque para diminuir a visibilidade dos objetos menores e dar maior ênfase aos da frente. E, então, foi limiarizado o resultado a fim de destacar esses objetos. A limiarização é um método de segmentação, que será explicado em sua devida seção. 5.3 Filtro da Mediana A mediana é um filtro não linear que utiliza como princípio a própria técnica estatística, que consiste em colocar um conjunto de dados em ordem e selecionar o valor central. Esse tipo de filtro é muito eficiente na remoção de ruído, principalmente, o tipo de ruído conhecido como ruído sal e pimenta, que são os ruídos impulsivos[2, p. 102]. Como dito anteriormente, a mediana consiste em ordenar o conjunto de dados e ter como saída o valor do 50º percentil, nesse caso o conjunto de dados são os valores dos pixels da imagem. Assim, para um kernel de tamanho 3x3, o valor desse pixel na imagem de saída será o quinto maior valor de intensidade. Na Figura 5.9, temos um exemplo mostrando uma imagem de raios-X de uma placa de circuitos eletrônicos com a presença de ruído tipo sal e pimenta, 5.9(a). A figura também apresenta as respectivas aplicações de um filtro da média, 5.9(b), e do filtro da mediana, 5.9(c), ambos com kernel de tamanho 3x3. É possível perceber o quanto o filtro da mediana se sai melhor na remoção desse tipo de ruído. Portanto, pela característica da mediana, em ter como saída 50º percentil, pode-se afirmar que agrupamentos de ruídos menores que metade da área do filtro serão removidos[2, p. 102]. Figura 5.9: Remoção de ruído sal e pimenta por meio do filtro de Mediana[2, p. 103]. 5.4 Filtro Gaussiano O Filtro gaussiano é um filtro simétrico (isotrópico) usado para suavizar imagens. Ele é um filtro de média ponderada, mas com seus pesos seguindo aproximadamente uma distribuição gaussiana. O “aproximadamente” é pelo fato que será necessário truncamentos, havendo erros, e caso opte pela função modificada, (5.5), ao invés da original, (5.4). Ao usar a função modificada, sem a constante multiplicadora, o perfil da função fica melhor, variando os pesos do kernel de (0, 1], conforme Figura 5.10. Função gaussiana: \\[G(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\tag{5.4}\\] Função gaussiana modificada: \\[G&#39;(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\tag{5.5}\\] Note que, na Figura 5.10, a função tem formato de sino, variando de acordo com os valores atribuídos ao \\(\\sigma\\), que dependerá da ênfase desejada no cálculo. A medida que \\(\\sigma\\) aumenta, maior é o peso dado às “caldas” da função, isto é, aos pixels mais distantes do centro do kernel. Essa característica é representada no gráfico da Figura 5.10. A fim de que o pixel central da máscara esteja em \\((0,0)\\), considera-se \\(\\mu=0\\). Figura 5.10: Função gaussiana com \\(\\mu=0\\) e diferentes \\(\\sigma\\) O Filtro Gaussiano 2D pode ser gerado através da convolução de dois vetores resultantes de \\(G&#39;(x)\\) sendo \\(x\\) ora as linhas, representando um vetor da matriz 2D, ora as colunas, representando o outro vetor da matriz 2D. De forma alternativa, ele também pode ser construído a partir da n-ésima linha do Triângulo de Pascal que corresponde ao tamanho do kernel desejado [3, p. 126]. Como o método é o mesmo para as duas fontes, optamos explicá-lo pela função gaussiana modificada, (5.5). O procedimento primário é obter a imagem da função, mas, para obter um filtro com intuito gaussiano, deve-se selecionar a amostragem dos pontos no intervalo de \\(\\pm 2.5\\sigma\\) ou \\(\\pm 3.5\\sigma\\) [9, p. 114]. Assim, escolhendo a primeira opção, para modelagem de um kernel 7x7, procura-se \\(G&#39;(0)\\), \\(G&#39;(1)\\), \\(G&#39;(2)\\) e \\(G&#39;(3)\\), pela simetria da função, esses valores já bastam, conforme demonstrado em (5.6). Entretanto, como os valores têm infinitas casas decimais, devem ser truncados. Aqui escolhemos o uso de cinco casas decimais. \\[\\begin{split} &amp;G&#39;(-3) = G&#39;(3) = 0,01110\\\\ &amp;G&#39;(-2) = G&#39;(2) = 0,13533\\\\ &amp;G&#39;(-1) = G&#39;(1) = 0,60653\\\\ &amp;G&#39;(0) = 1 \\end{split} \\tag{5.6}\\] Então, considera-se o menor valor como 1, no caso \\(G&#39;(3)\\) e \\(G&#39;(-3)\\), e interpolamos os demais, truncando-os com propósito de obter a parte inteira. A divisão por 224 é oriunda da soma dos coeficientes encontrados no vetor. Pronto, temos a máscara gaussiana 1D (5.7). \\[\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix} \\tag{5.7}\\] Com a máscara em mãos, convoluciona-se os seguintes fatores: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) \\(\\frac{1}{224}\\begin{bmatrix}1 \\\\ 12 \\\\ 54 \\\\ 90 \\\\ 54 \\\\ 12 \\\\ 1\\end{bmatrix}\\) \\(f(x,y)\\) é a imagem de entrada Há uma determinada ordem do cálculo que é mais eficiente para o sistema digital, que é a convolução entre \\(f(x,y)\\) e uma das máscaras 1D e a convolução desse resultado com a transposta desse kernel. Pronto, tem-se o filtro gaussiano 2D aplicado a imagem como saída. Figura 5.11: Imagem com diferentes tamanhos de filtro gaussiano. Adaptado de [2, p. 102] Na Figura 5.11, tem-se um exemplo similar ao usado no filtro de média, porém, usando-se o filtro gaussiano. A imagem de entrada (a) tem tamanho 500x500 onde foram aplicados filtros de diferentes tamanhos, 3x3(b), 5x5(c), 9x9(d), 15x15(e) e 35x35(f). Note que, ao contrário do exemplo anterior, Figura 5.7, há uma maior preservação da nitidez, todavia não seria um filtro eficiente para redução de ruído. Essa característica é devido ao maior peso dado aos pixels quão mais próximo do centro do kernel estão. Refêrencias "],["segmentacao.html", "Capítulo 6 Segmentação 6.1 Detecção por descontinuidade 6.2 Detecção de Bordas 6.3 Transformada de Hough 6.4 Detecção de Quinas 6.5 Detecção de Blobs 6.6 Limiarização", " Capítulo 6 Segmentação A segmentação subdivide uma imagem para detecção de regiões ou objetos para uma determinada aplicação. Os principais métodos de segmentação utilizam a distribuição dos valores de intensidade, seja pelos padrões de similaridade, ou de descontinuidade [2, p. 454]. Nas técnicas de similaridade, as imagens são divididas em regiões semelhantes com base em um conjunto de características. Na segmentação por descontinuidade se consideram as mudanças abruptas de intensidades, caracterizadas por pontos isolados, linhas ou bordas na imagem. 6.1 Detecção por descontinuidade Bordas podem ser descritas como o limite ou a fronteira entre duas regiões onde ocorre uma variação brusca de intensidade em uma determinada direção. Uma linha pode ser vista como um segmento de borda em que a intensidade do fundo de cada lado da linha ou é muito superior, ou muito inferior à intensidade dos pixels da linha. Linhas com o comprimento e largura iguais a um pixel são tratados como pontos isolados [2, p. 456]. Os métodos de detecção de bordas se baseiam no comportamento das derivadas, que podem detectar mudanças locais abruptas. Em uma função digital, as derivadas são aproximadas como termos de diferenças. Aproximações podem ser obtidas por meio de expansões em séries de Taylor, como demonstrado no livro “Processamento digital de imagens” [2, p. 457], em que os resultados para a primeira e a segunda ordem no ponto \\(x\\) pode ser obtido pelas Equações (6.1) e (6.2) respectivamente: \\[\\frac{\\delta f}{\\delta x} = f&#39;(x) = f(x+1) - f(x) \\tag{6.1}\\] \\[\\frac{\\delta^2 f}{\\delta x^2} = f&#39;&#39;(x) = f(x+1) + f(x-1) - 2f(x) \\tag{6.2}\\] Para avaliar o comportamento das derivadas na transição de intensidades apresentamos na Figura 6.1 um perfil de intensidade horizontal (linha de digitalização) de uma imagem, juntamente com os resultados das duas últimas equações, (6.1) e (6.2), para alguns pontos. A imagem contém vários objetos sólidos, uma linha e um ponto interno de ruído, e a linha de digitalização está próxima ao centro, incluindo o ponto isolado. A Figura 6.1(c) mostra uma simplificação do perfil e como as derivadas de primeira e segunda ordem se comportam quando encontram um ponto isolado, uma linha e as bordas dos objetos. Com base nos resultados das Equações (6.1) e (6.2), na Figura 6.1, observa-se que atendem as propriedades das derivadas. No início e ao longo da rampa de intensidade, a derivada de primeira ordem é diferente de zero, enquanto a segunda derivada é diferente de zero apenas no início e no final da rampa. Estes comportamentos indicam que as derivadas de primeira ordem produzem bordas grossas, e as de segunda ordem produzem bordas mais finas [2, p. 458]. Figura 6.1: (a) Imagem. (b) Perfil de intensidade horizontal no centro da imagem. (c) Perfil simplificado e resultados das derivadas. [2, p. 457]. No ponto de ruído, a resposta para a derivada de segunda ordem é mais forte do que para a primeira derivada. Derivadas de segunda ordem acentuam as respostas em mudanças bruscas, podendo melhorar pequenos detalhes [2, p. 458]. Na linha com detalhes bem finos, a derivada de segunda ordem também tem maior magnitude. Nota-se nas bordas em rampa e nas bordas em degrau, que a segunda derivada tem sinais opostos (negativo para positivo ou vice-versa) conforme entra e sai da borda, o que caracteriza uma “borda dupla”. O sinal da segunda derivada também pode ser utilizado para determinar se uma transição em uma borda é de claro para escuro (segunda derivada negativa) ou de escuro para claro (segunda derivada positiva) [2, p. 458]. 6.1.1 Detecção de pontos isolados Considerando que as derivadas de segunda ordem têm uma resposta mais forte aos detalhes finos, o operador diferencial laplaciano (6.3) \\[\\nabla^2f(x,y) = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2} \\tag{6.3}\\] é ideal para a detecção de pontos [2, p. 459]. As aproximações das derivadas de segunda ordem por termos de diferenças vistas podem ser estendidas para os dois termos do laplaciano (6.4) e (6.5) \\[\\frac{\\partial^2f}{\\partial x^2} = f(x+1, y) + f(x-1, y) -2f(x,y) \\tag{6.4}\\] \\[\\frac{\\partial^2f}{\\partial y^2} = f(x, y+1) + f(x, y-1) -2f(x,y) \\tag{6.5}\\] Ao se relacionar essas três últimas equações, (6.3), (6.4) e (6.5), obtém-se o laplaciano discreto (6.6): \\[\\nabla^2f(x,y) = f(x+1, y) + f(x-1, y) + f(x, y+1) + f(x, y-1) -4f(x,y) \\tag{6.6}\\] Essa equação pode ser implementada utilizando a máscara de filtragem identificada na Figura 6.2(a). Para incluir as direções diagonais na definição do laplaciano digital, acrescenta-se mais dois termos a esta equação, (6.6), um para cada direção diagonal. A Figura 6.2(b) mostra a máscara de filtragem desta atualização que inclui as diagonais. Figura 6.2: (a) Máscara referente a equação laplaciana discreta. (b) Máscara que inclui as diagonais. [2, p. 106] A intensidade de um ponto isolado será muito diferente do seu entorno, portanto, ao aplicar a máscara, as únicas diferenças de intensidade relevantes serão as mais altas, maiores que um limite determinado (\\(T\\)) para serem consideradas pontos isolados [2, p. 459]. Para detectar o ponto se utiliza a seguinte expressão (6.7): \\[ g(x,y) = \\begin{cases} 1,\\ se\\ R(x,y) \\geq T \\\\0,\\ caso\\ contrário \\end{cases} \\tag{6.7} \\] em que \\(g\\) é a imagem de saída, \\(T\\) é um limiar não negativo e \\(R\\) é o operador laplaciano. O ponto isolado será detectado no local \\((x, y)\\) em que a máscara está centrada se o módulo da resposta nesse ponto exceder o limiar estabelecido. Os pontos detectados são rotulados como 1 na imagem de saída, e os demais como 0, o que produz uma imagem binária. 6.1.2 Detecção de linhas Semelhante à detecção de pontos isolados, as derivadas de segunda ordem geram respostas mais fortes na detecção de linhas, e produzem linhas mais finas do que as derivadas de primeira ordem [2, p. 460]. Ao utilizar a máscara laplaciana da Figura 6.2 sobre a imagem de um componente em um circuito eletrônico (parte de uma conexão wire-bond), representado na Figura 6.3(a), obtém-se como resultado a Figura 6.3(b). Na seção ampliada da Figura 6.3(b), as linhas mais claras identificam os pontos positivos, o cinza escuro são os valores negativos, e o cinza médio representa zero. O efeito de linha dupla é evidente na região ampliada ao se utilizar a segunda derivada. Figura 6.3: (a) Imagem do circuito. (b) Resultado do filtro laplaciano. (c) Valor absoluto do filtro laplaciano. (d) Valores positivos do filtro laplaciano. [2, p. 460] Para destacar as linhas após a aplicação do filtro e remover os valores negativos pode ser utilizado o módulo dos valores calculados ou utilizar apenas os valores positivos do laplaciano [2, p. 460], os resultados destas metodologias estão identificados na Figura 6.3(c) e (d), respectivamente. Nota-se que as linhas geradas pela abordagem do módulo são mais grossas, pois a espessura é o dobro se comparada com a outra metodologia. Nas duas situações, as linhas mais largas que o tamanho do filtro laplaciano são separadas por um “vale” de zeros. A detecção de linhas é ideal quando a espessura é menor que o detector, ao contrário se recomenda tratar o elemento como uma região e utilizar outros métodos [2, p. 460]. O filtro laplaciano, na Figura 6.2, é isotrópico, assim, a sua resposta independe da direção [2, p. 460]. No caso das linhas, pode ser interessante a detecção em apenas uma determinada direção, utilizando, por exemplo, uma das máscaras apresentadas na Figura 6.4. Para a primeira máscara da Figura 6.4, os sinais mais fortes ocorrem em linhas horizontais da imagem que passam pela linha do meio da máscara. O segundo filtro detecta melhor as linhas com \\(45°\\) de inclinação. O terceiro filtro para linhas verticais. O quarto filtro para linhas com \\(-45º\\) de inclinação. Vale destacar que estamos utilizando a convenção de que os eixos da imagem têm sua origem no canto superior esquerdo, com o eixo \\(x\\) positivo apontando para baixo e o eixo \\(y\\) positivo se estendendo à direita. Os ângulos das linhas são medidos em relação ao eixo \\(x\\) positivo. Figura 6.4: Máscaras para detecção de linhas em uma determinada direção [2, p. 461]. Quando o objetivo é detectar todas as linhas da imagem em uma direção específica, aplica-se a máscara associada a essa direção e se define um limiar (\\(T\\)) para selecionar os sinais mais fortes. Para os quatro últimos filtros apresentados, as linhas de 1 pixel de espessura apresentam os maiores resultados [2, p. 462]. 6.2 Detecção de Bordas 6.2.1 Modelos de Bordas Na Figura 6.5, estão os principais modelos de borda, que são classificados de acordo com os perfis de intensidade. A primeira borda, borda em degrau, (a), apresenta uma transição entre dois níveis de intensidade e são consideradas ideais com uma distância de 1 pixel. Na prática, as bordas nas imagens digitais não apresentam uma transição tão bem definida, assim, modelos mais apropriados consideram um perfil de rampa como da Figura 6.5(b) [2, p. 462]. Quanto mais indefinida é a transição da borda, menor é a inclinação da rampa. Em vez de uma borda com 1 pixel de espessura, todos os pontos na rampa fazem parte da borda. Além disso, na Figura 6.5, mostra os modelos de bordas com seus respectivos perfis de intensidade. Figura 6.5: (a) Borda degrau. (b) Borda rampa. (c) Borda telhado. [2, p. 462] No terceiro modelo de borda, Figura 6.5(c), em forma de telhado ou roof edge, a base (largura) de uma borda é definida pela espessura e a nitidez da linha. Quando a base é igual 1 pixel de espessura, uma borda em forma de telhado é definida como uma linha [2, p. 462]. As imagens da Figura 6.6 não apresentam ruídos, entretanto os modelos de detecção devem considerar que as bordas estejam desfocadas e com ruídos. Na primeira coluna da Figura 6.5 estão quatro bordas de perfil rampa com diferentes níveis de ruídos, e abaixo de cada imagem, está o perfil horizontal de intensidade que passa pelo centro da imagem. A primeira imagem no canto esquerdo não apresenta ruído, e as outras três imagens da primeira coluna foram alteradas com ruído gaussiano aditivo com média zero e desvio padrão de \\(0.1\\), \\(1.0\\) e \\(10.0\\) níveis de intensidade, respectivamente. Figura 6.6: Primeira coluna: imagens e perfis de intensidade de uma borda em declive corrompida pelo ruído gaussiano aleatório de desvio padrão \\(0.0\\), \\(0.1\\), \\(1.0\\) e \\(10.0\\) níveis de intensidade, respectivamente. Segunda coluna: imagens da primeira derivada. Terceira coluna: imagens da segunda derivada. [2, p. 465] Na segunda e terceira coluna estão identificados, respectivamente, a primeira e a segunda derivada dos perfis de intensidade da primeira coluna. A primeira derivada é utilizada para detectar bordas, pois identifica pontos de transição abrupta de intensidade na imagem. Para o resultado da primeira derivada na primeira linha, os valores na rampa são positivos e, nas regiões de intensidade constante é igual a zero. As duas faixas pretas na imagem da parte superior da coluna central são os resultados iguais a zero da primeira derivada, e os valores constantes estão identificados como cinza claro. Como as bordas são uma transição de uma região escura para uma região branca, a segunda derivada na terceira coluna é positiva no início da rampa e negativa no final. A linha que passa por estes dois resultados da derivada se intercepta com o eixo de intensidade zero, gerando o ponto de cruzamento por zero que é utilizado para localizar o centro de bordas espessas [2, p. 464]. Nas regiões de intensidade constante, as derivadas de segunda ordem também são zero, e se apresentam na cor cinza claro nas imagens da terceira coluna. As linhas finas verticais brancas e pretas são os resultados positivos e negativos da segunda derivada. Mesmo não sendo perceptível os ruídos nas imagens da segunda e terceira linha, eles apresentaram um impacto significativo nas derivadas, principalmente na derivada de segunda ordem que é mais sensível a eles. Quanto maior o ruído, mais difícil de se associar com os perfis das derivadas, dificultando a detecção de bordas. Este comportamento pode ser tratado com a suavização da imagem em uma etapa anterior a segmentação [2, p. 464]. 6.2.2 Método do gradiente (Roberts, Prewitt, Sobel) Na detecção de bordas, as derivadas de primeira ordem são calculadas utilizando a magnitude do gradiente, que é um vetor cuja a direção indica os pontos de maior variação de intensidade [3, p. 155]. A Figura 6.7 indica que a direção do gradiente sempre será perpendicular à direção tangente da borda. Para uma função \\(f(x, y)\\), o gradiente de \\(f\\) nas coordenadas \\((x, y)\\) na forma matricial é expresso como a Equação (6.8) \\[\\nabla f = \\begin{bmatrix} G_x \\\\ G_y \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} \\tag{6.8}\\] Figura 6.7: Vetor gradiente \\(\\nabla f\\) em uma borda. [3, p. 155] O módulo ou magnitude (tamanho) do vetor \\(\\nabla f\\) é dado como \\(M(x,y)\\), Equação (6.9). \\[M(x,y) = \\sqrt{G^2_x + G^2_y} \\tag{6.9}\\] O módulo do gradiente é a maior taxa de variação de \\(f(x,y)\\) na direção do vetor gradiente. Devido ao custo computacional, a magnitude do gradiente é aproximada pelo uso dos valores absolutos, Equação (6.10) [3, p. 155]. \\[M(x,y) \\simeq |G_x| + |G_y| \\tag{6.10}\\] A direção do vetor gradiente é dada pela Equação (6.11), que retorna um ângulo \\[ \\alpha(x,y) = tg^{-1} \\begin{bmatrix} \\frac{G_y}{G_x} \\end{bmatrix} \\tag{6.11}\\] medido em relação ao eixo \\(x\\). Da mesma forma que a magnitude \\(M(x, y)\\), ou imagem gradiente, o ângulo \\(\\alpha(x, y)\\) também é uma imagem do mesmo tamanho que a original formado pela divisão da imagem \\(G_y\\) por \\(G_x\\) [2, p. 466]. Para calcular as derivadas parciais \\(\\partial f/ \\partial x\\) e \\(\\partial f/ \\partial y\\) no caso de quantidades digitais, é necessário utilizar aproximações discretas e, em seguida, determinar as máscaras de filtragem correspondentes. Uma forma de aproximação é considerar a diferença entre os elementos da vizinhança para calcular a magnitude do gradiente [3, p. 157]. Ao se utilizar as diferenças cruzadas, como na seguinte Equação (6.12) \\[M(x,y) \\simeq |f(x,y) - f(x+1, y+1)| + |f(x,y+1) - f(x+1, y)| \\tag{6.12}\\] é o mesmo que aplicar os filtros de tamanho 2x2 da Figura 6.8 e somar os resultados absolutos. Figura 6.8: Operador de Roberts [3, p. 157]. Estes operadores, conhecidos como operadores de Roberts, foram uma das primeiras tentativas de usar máscaras 2-D, entretanto não são tão úteis quanto as máscaras simétricas ao redor do ponto central, em que as menores são de tamanho 3×3 [2, p. 467]. As aproximações mais simples para as derivadas parciais usando máscaras de tamanho 3×3 são dadas pela Equação (6.13) \\[ \\begin{split} M(x,y) &amp;\\simeq |[f(x+1,y-1) + f(x+1,y) + f(x+1,y+1)]-\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y-1) + f(x-1,y) + f(x-1,y+1)]+\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y+1) + f(x,y+1) + f(x+1,y+1)]-\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y-1) + f(x,y-1) + f(x+1,y-1)]| \\end{split} \\tag{6.13}\\] Figura 6.9: Operador de Prewitt [3, p. 158]. A Equação anterior, (6.8), pode ser implementada aplicando as máscaras da Figura 6.9, que recebem o nome de operadores de Prewitt. Uma variação deste último método utiliza o valor \\(2\\) como peso no centro do coeficiente [3, p. 158] \\[ \\begin{split} M(x,y) &amp;\\simeq |[f(x+1,y-1) + 2f(x+1,y) + 2f(x+1,y+1)]-\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y-1) + 2f(x-1,y) + f(x-1,y+1)]+\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y+1) + 2f(x,y+1) + f(x+1,y+1)]-\\\\ &amp;\\ \\ \\ \\ \\ [f(x-1,y-1) + 2f(x,y-1) + f(x+1,y-1)]| \\end{split} \\tag{6.14}\\] Figura 6.10: Operador de Sobel [3, p. 158]. O resultado das equação pode ser obtido com a soma dos valores absolutos dos resultados das máscaras na Figura 6.10, chamadas de operadores de Sobel. Mesmo que as máscaras de Prewitt sejam mais simples de implementar do que as máscaras de Sobel, as de Sobel são mais utilizadas por melhor suavização, diminuindo os ruídos [2, p. 468]. Ao calcular a magnitude como uma aproximação da soma dos valores absolutos dos componentes de gradiente \\(G_x\\) e \\(G_y\\), Equação (6.10), pode se perder a propriedade isotrópica dos filtros. No caso das máscaras de Sobel e de Prewitt, este problema não ocorre, pois dão resultados isotrópicos apenas para bordas verticais e horizontais, e que são independentes da Equação (6.9) ou Equação (6.10) [2, p. 468]. 6.2.3 Método de Marr-Hildreth Nos métodos de detecção de bordas utilizando a derivada de segunda ordem se observa maior sensibilidade aos ruídos, assim, recomenda-se um pré-processamento de suavização. A técnica de detecção proposta por Marr e Hildreth (1980), por exemplo, combina a filtragem Gaussiana com o operador Laplaciano (Figura 6.2) [3, p. 163]. Após a suavização da imagem com o filtro gaussiano, as bordas são identificadas pelos pontos de cruzamento por zero da segunda derivada. Um aspecto que torna a técnica interessante para imagens de diferentes escalas é que considera as características da borda e dos ruídos, empregando-se operadores de tamanho mais adequado para cada imagem [2, p. 470]. Operadores de maior tamanho são recomendados para detectar bordas borradas, enquanto operadores menores detectam melhor detalhes finos com foco nítido. Como o laplaciano é isotrópico, respondendo de forma igual às variações nas diferentes direções, evita-se a utilização de várias máscaras [2, p. 472]. O operador proposto por Marr e Hildreth é obtido pela convolução, Equação (6.15) \\[g(x,y) = \\nabla^2[G(x,y)*f(x,y)] \\tag{6.15}\\] em que \\(f(x,y)\\) é a imagem, \\(\\nabla^2\\) é o operador laplaciano, (\\(\\partial^2 f / \\partial x^2 + \\partial^2 f / \\partial y^2\\)), e \\(G\\) é a função gaussiana 2-D, (6.16): \\[G(x,y) = e^{-\\frac{x^2 + y^2}{2\\sigma^2}} \\tag{6.16}\\] com desvio padrão \\(\\sigma\\). Em razão da linearidade das operações, a ordem da diferenciação e da convolução podem ser alteradas [3, p. 163], assim, após a diferenciação da expressão gaussiana, obtém-se o Laplaciano da Gaussiana (LoG), Equação (6.17): \\[\\nabla^2G(x,y) = \\begin{bmatrix} \\frac{x^2 + y^2 - \\sigma^2}{\\sigma^4} e^{-\\frac{x^2 + y^2}{2\\sigma^2}} \\end{bmatrix} \\tag{6.17}\\] As máscaras podem ser geradas pela amostragem da Equação (6.17) com ajustes nos coeficientes para que a soma seja zero. Um exemplo de máscara 5x5 que atende ao Laplaciano da Gaussiana está na Figura 6.11(d). O comportamento dessa máscara é próximo do efeito da função LoG na Figura 6.11(a), em que o termo positivo e central é rodeado por uma região negativa cujo os valores aumentam ao se distanciar da origem, e uma região externa com zeros [2, p. 472]. Figura 6.11: (a) Gráfico 3-D do negativo de LoG. (b) Imagem do negativo de LoG. (c) Seção transversal de (a). (d) Aproximação de máscara 5x5 para LoG. [2, p. 471] Devido ao formato na Figura 6.11, a função LoG é conhecida como operador de chapéu mexicano. Seu gráfico 3-D, sua imagem e sua seção transversal se referem ao negativo da função LoG. Na seção transversal (Figura 6.11(c)), o cruzamento por zero do LoG ocorre em \\(x^2 + y^2 = 2 \\sigma^2\\) , definindo um círculo centrado na origem e de raio \\(2\\sigma\\). Na prática, o filtro LoG é gerado pelas seguintes etapas [2, p. 472]: Define-se um filtro \\(n\\)×\\(n\\) gaussiano a partir de amostragem com a Equação (6.16). Lembrando da sugestão que o tamanho do filtro gaussiano, \\(n\\), precisa ser o menor inteiro ímpar maior ou igual a \\(6\\sigma\\). Após suavização da imagem com o filtro gaussiano, o resultado é processado pelo laplaciano, por exemplo, com uma máscara 3×3 da Figura 6.2. Na imagem resultante da etapa anterior são encontrados os pontos de cruzamento por zero. Estes pontos podem ser identificados em um pixel, \\(p\\), com base na sua vizinhança 3x3. No caso de \\(p\\) ser um cruzamento por zero, pelo menos dois de seus vizinhos opostos devem apresentar sinais diferentes. Neste caso são realizados quatro testes: esquerda/direita, acima/abaixo e com as duas diagonais. Quando se utiliza um limiar para identificar o cruzamento por zero, tanto os sinais dos vizinhos opostos devem ser diferentes quanto o valor absoluto da diferença numérica deve ultrapassar o limiar para que o ponto \\(p\\) seja um cruzamento por zero. O filtro LoG também pode ser aproximado com a convolução de uma máscara gerada a partir da diferença de duas funções gaussianas (DoG), Equação (6.18) [2, p. 473]: \\[DoG(x,y) = \\frac{1}{2\\pi \\sigma^2_1} e^ {-\\frac{x^2+y^2}{2\\sigma^2_1}} - \\frac{1}{2\\pi \\sigma^2_2} e^ {-\\frac{x^2+y^2}{2\\sigma^2_2}} \\tag{6.18}\\] com \\(\\sigma_1 &gt; \\sigma_2\\). Marr e Hildreth mostraram que para \\(\\sigma_2/\\sigma_1 = 1.6\\) o operador tem maior aproximação com a função LoG [2, p. 473]. Para que LoG e DoG tenham os mesmos cruzamentos por zero se sugere que se mantenha a seguinte relação para o valor de \\(\\sigma\\) em LoG, Equação (6.19): \\[\\sigma^2 = \\frac{\\sigma^2_1\\sigma^2_2}{\\sigma^2_1 - \\sigma^2_2} \\ln \\begin{bmatrix} \\frac{\\sigma^2_1}{\\sigma^2_2} \\end{bmatrix} \\tag{6.19}\\] Para estabelecer uma mesma escala de amplitude no resultado dos dois operadores, LoG e DoG, ocorre um ajuste para o mesmo valor na origem em ambas as funções [2, p. 474]. 6.2.4 Método de Canny O algoritmo de Canny recebeu esse nome em alusão a John Canny, que o propôs em seu artigo, “A computational Approach to Edge Detection” [17], publicado em 1986. Sua formulação se baseava em três pontos principais: Uma baixa taxa de erro, ou seja, todas as bordas presentes na imagem devem ser encontradas e não deve haver respostas espúrias. O segundo critério diz que as bordas detectadas devem estar bem localizadas, em outras palavras, elas devem estar o mais próximo possível das bordas verdadeiras. O terceiro, e último, critério diz que se deve minimizar o número de máximos locais em torno da borda verdadeira para que não sejam encontrados múltiplos pixels de borda onde deve haver somente um. Em seu trabalho, Canny buscou encontrar soluções ótimas, matematicamente, que obedecessem os três critérios. Apesar disso, é muito difícil, ou impossível, encontrar uma solução que satisfaça completamente os objetivos descritos [2, p. 474]. Todavia é possível utilizar uma aproximação por meio de otimização numérica com as bordas em degrau em um exemplo 1-D que contenham ruído branco gaussiano para mostrar que uma boa aproximação de um ótimo detector de bordas é a primeira derivada de uma gaussiana, Equação (6.20) [2, p. 474]: \\[\\frac{\\mathrm{d} }{\\mathrm{d} x}e^{\\frac{-x^2}{2\\sigma^2}} = \\frac{-x}{\\sigma^2}e^{\\frac{-x^2}{2\\sigma^2}} \\tag{6.20}\\] Canny demonstrou que a utilização dessa aproximação pode ser feita com uma taxa 20% inferior à solução numérica, o que a torna, praticamente, imperceptível para muitas das aplicações [2, p. 474]. A ideia anterior foi imaginada em um aspecto 1-D, precisamos expandir esse conceito para uma generalização 2-D. Uma borda de degrau pode ser caracterizada pela sua posição, orientação e possível magnitude. Aplicar um filtro Gaussiano em uma imagem e depois diferenciá-la forma um simples e efetivo operador direcional [18, p. 145]. Digamos, então, que \\(f(x,y)\\) seja uma imagem e \\(G(x,y)\\), a função gaussiana, Equação (6.21): \\[G(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}} \\tag{6.21}\\] Temos como saída a imagem suavizada, \\(f_s\\), Equação (6.22): \\[f_s(x,y)=G(x,y)*f(x,y) \\tag{6.22}\\] E, após isso, realizamos o cálculo da magnitude, Equação (6.23), e a direção do gradiente, Equação (6.24): \\[M(x,y) = \\sqrt{g_x^2+g_y^2} \\tag{6.23}\\] \\[\\alpha(x,y)= \\tan^{-1}\\left ( \\frac{g_y}{g_x} \\right ) \\tag{6.24}\\] onde \\(g_x=\\partial f_s/\\partial x\\) e \\(g_y=\\partial f_s/\\partial y\\). Para o cálculo das derivadas parciais, podemos utilizar tanto Prewitt quanto Sobel. Como essa primeira etapa utiliza operadores que calculam as primeiras derivadas, isso produz bordas grossas, e o terceiro objetivo da proposta de Canny é ter bordas com único ponto, por isso o próximo passo é o de afinar as bordas encontradas. O método que usaremos para isso é chamado supressão dos não máximos. A etapa de não máximos tem como base a discretização das direções da normal da borda (vetor gradiente), ou seja, em uma região 3x3, temos 4 direções possíveis, como pode ser visto na Figura 6.12(c); por exemplo, uma borda de \\(45º\\), se ela estiver entre \\(+157.5º\\) e \\(+112.5º\\) ou \\(-67.5º\\) e \\(-22.5º\\). Na Figura 6.12(a), temos um exemplo de duas orientações que podem existir em uma borda horizontal. Na Figura 6.12(b), podemos ver a normal de uma borda horizontal e o intervalo de valores onde a direção do vetor gradiente pode existir. Figura 6.12: Discretização das direções. (a) Borda horizontal. (b) Intervalo dos possíveis valor do ângulo normal da borda para uma borda horizontal. (c) Intervalo de valores do ângulo da normal para os diferentes tipos de borda. [2, p. 475] Se considerarmos \\(d_1\\), \\(d_2\\), \\(d_3\\) e \\(d_4\\) como as direções possíveis em uma área 3x3, podemos formular o seguinte esquema de supressão de não máximos em todos os pontos \\((x,y)\\) [2, p. 475]: Encontrar a direção \\(d_k\\) que está mais perto de \\(\\alpha (x,y)\\). Se o valor de \\(M(x,y)\\) for inferior a pelo menos um dos seus dois vizinhos ao longo de \\(d_k\\), deixe \\(g_N(x,y)=0\\) (supressão), senão deixe \\(g_N(x,y)=M(x,y)\\), em que \\(g_N(x,y)\\) é a imagem suprimida. A última operação a ser realizada é a limiarização para se remover os pontos de falsas bordas. Aqui, usaremos a limiarização por histerese, que utiliza dois limiares, um baixo (\\(T_L\\)) e um alto (\\(T_H\\)), sendo que Canny sugeriu, em seu trabalho, que a razão entre o limiar alto e o baixo deve ser de 2:1 ou 3:1. Podemos imaginar essa limiarização da seguinte forma, em que se cria duas imagens adicionais, (6.25) e (6.26) \\[g_{NH}(x,y) = g_N(x,y)\\geq T_H \\tag{6.25}\\] \\[g_{NL}(x,y) = g_N(x,y)\\geq T_L \\tag{6.26}\\] Onde \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) são definidas inicialmente como \\(0\\). Dessa forma, \\(g_{NH}(x,y)\\) conterá os pixels que são maiores que o nosso limiar alto e \\(g_{NL}(x,y)\\), os que estão acima do nosso limiar baixo, o que significa que \\(g_{NL}(x,y)\\) contém os pixels que estão entre os dois limiares e os que estão acima do limiar alto. A próxima etapa é remover esses pixels redundantes entre \\(g_{NL}\\) e \\(g_{NH}\\), (6.27): \\[g_{NL}(x,y)=g_{NL}(x,y)-g_{NH}(x,y) \\tag{6.27}\\] Sem redundância, chama-se os pixels de \\(g_{NH}(x,y)\\) de pixels fortes e, os de \\(g_{NL}(x,y)\\), de fracos. Ao final dessa limiarização, todos os pixels fortes são classificados como borda válida, mas com falhas, que nos leva a outro processo: Localizar o próximo pixel de borda, \\(p\\), a ser revisado em \\(g_{NH}(x,y)\\). Classificar todos os pixels fracos de \\(g_{NL}(x,y)\\) que tenham conexão como bordas válidas, por exemplo, os que tiverem conectividade-8. Enquanto todos os pixels de \\(g_{NL}(x,y)\\) não forem analisados, retorna-se ao passo 1, senão continuamos ao passo 4. Zerar todos os pixels de \\(g_{NL}(x,y)\\) que não são bordas válidas. Ao final desses processos, teremos a imagem de saída do algoritmo de Canny. Como dito por [2, p. 476], o uso de duas imagens para \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) é uma boa maneira para se explicar o algoritmo de uma maneira simples, mas, na prática, isso pode ser feito diretamente na imagem \\(g_N(x,y)\\). Finalmente, sumarizando os passos do algoritmo, com um exemplo passo-a-passo: Imagem original Figura 6.13: Imagem original. Aplicação do filtro gaussiano para suavizar a imagem. Figura 6.14: Imagem filtrada com filtro gaussiano. Cálculo da magnitude do gradiente e dos ângulos. Figura 6.15: (a) Sobel na direção vertical. (b) Sobel na direção horizontal. (c) Gradiente. (d) Ângulos. [9, p. 98] Aplicação da supressão não máximos para afinar as bordas. Figura 6.16: Resultado da supressão não máxima. Usar limiarização por histerese. Figura 6.17: Resultado da histerese. Resultado final, após a análise de conectividade para detectar e conectar as bordas. Figura 6.18: Resultado final da detecção de bordas de Canny. 6.3 Transformada de Hough A Transformada de Hough é uma técnica utilizada para detectar formas em imagens, sejam elas linhas, círculos ou elipses, apesar de ela ser muito utilizada e ter sido criada, principalmente, para detecção de linhas. 6.3.1 Transformada de Hough para detecção de linhas Para começar a entender essa transformada, imaginemos que temos um ponto \\((x_i, y_i)\\) no plano \\(xy\\) e a equação da reta \\(y_i=ax_i+b\\). É fato que pelo ponto \\((x_i, y_i)\\), passam infinitas retas. Podemos escrever a equação anterior em relação a \\(b\\), ou seja, \\(b=-x_ia+y_i\\), o que nos leva ao plano \\(ab\\) (espaço de parâmetros) onde essa nova equação gerará uma única reta. Imaginemos um outro ponto \\((x_j, y_j)\\) no plano \\(xy\\), podemos também levá-lo ao plano \\(ab\\) através da equação \\(b=-x_ja+y_j\\). Como podemos ver na Figura 6.19(b), as duas retas geradas no plano \\(ab\\) se cruzam nas coordenadas \\((a&#39;, b&#39;)\\). O ponto de cruzamento representa a reta que cruza os dois pontos no plano \\(xy\\), como podemos ver na mesma representação 6.19(a). Na realidade, todos os pontos pertencentes a reta definida por esses dois pontos em \\(xy\\), tem sua reta respectiva em \\(ab\\) e todas elas se cruzam no ponto \\((a&#39;, b&#39;)\\), isso nos dá uma maneira de realizar a detecção de bordas, pois podemos imaginar essa reta no plano \\(xy\\) como nossa borda, assim, para achá-la, basta localizar o ponto no espaço de parâmetros onde um grande número de retas se cruzam. Figura 6.19: Planos \\(xy\\) e \\(ab\\) [2, p. 483]. Ocorre um pequeno problema nessa forma, pois quando a reta se aproxima da direção vertical, \\(a\\), o coeficiente angular da reta, aproxima-se do infinito. Por essa razão, em vez de levarmos os pontos a retas no espaço \\(ab\\) cartesiano, utilizamos um espaço em coordenadas polares. Portanto, utilizamos a seguinte Equação (6.28): \\[\\rho=x\\cos{\\theta}+y\\ sen{\\ \\theta} \\tag{6.28}\\] A Figura 6.20 mostra a aplicação de coordenadas polares nesse contexto por exemplos gráficos. Na Figura 6.20(a), \\(\\rho\\) é a distância da origem até a reta. Na Figura 6.20(b), mostra-se que cada uma das curvas senoidais representa as infinitas retas que passam por cada um dos dois pontos, \\((x_i,y_i)\\) e \\((x_j,y_j)\\) e a interseção das curvas, \\((\\rho&#39;,\\theta&#39;)\\), é a reta que passa por esses dois pontos. Figura 6.20: Exemplo de conversão de uma reta no plano cartesiano para o plano polar. (a) Pode ser dois pontos arbitrários de uma imagemImagem de ônibus com filtro de aguçamento e de suavização. [9, p. 98] A Figura 6.20(c) mostra como fazemos a representação digital do espaço de coordenadas polares, usamos uma matriz onde esse espaço é subdividido em várias células, chamadas células acumuladoras. Os valores de \\(\\theta_{\\text{min}}\\) e \\(\\theta_{\\text{max}}\\) são, geralmente, \\(-90^{\\circ}\\leq \\theta\\leq90^{\\circ}\\) e os valores de \\(\\rho_{min}\\) e \\(\\rho_{max}\\) são \\(-D\\leq\\rho\\leq D\\), onde \\(D\\) é o comprimento da diagonal da imagem, ou seja, \\(D=\\sqrt{altura^2+largura^2}\\). Essa etapa consiste em andar por todos os pontos, \\((x,y)\\), de borda da imagem de entrada e calcular o valor de \\(\\rho\\), a partir da Equação (6.28), variando o ângulo \\(\\theta\\). Com isso, a cada valor do ângulo, teremos um \\(\\rho\\) diferente e, então, na célula acumuladora \\((\\rho,\\theta)\\) da matriz, 6.20(c), incrementa-se 1, uma espécie de voto a candidato de reta. Ao final de todo o processo, temos determinadas células com valores mais altos, conhecidas como picos, que correspondem ao cruzamento de duas ou mais curvas senoidais do plano \\((\\rho,\\theta)\\) e a uma linha que liga pontos no plano \\(xy\\). A seguir temos um exemplo, que nos ajuda a entender e ver o funcionamento da transformada de Hough na prática. A Figura 6.21(a) contém uma imagem de tamanho 101x101 com um ponto no centro, ou seja, \\((x,y)=(50,50)\\). A Figura 6.21(b) contém a matriz acumuladora da transformada, que podemos ver a curva senoidal formada pelo ponto. Verificando os valores nela, vemos os resultados com diferentes \\(\\theta\\): para \\(\\theta=-90^{\\circ}\\) \\[\\rho=50 \\cdot \\cos(-90^{\\circ})+50\\cdot\\text{sen}(-90^{\\circ}) = -50\\] para \\(\\theta=90^{\\circ}\\) \\[\\rho=50\\cdot \\cos(90^{\\circ})+50\\cdot\\text{sen}(90^{\\circ})=50\\] para \\(\\theta=45^{\\circ}\\) \\[\\rho=50\\cdot \\cos(45^{\\circ})+50\\cdot \\text{sen}(45^{\\circ})\\approx70,71\\] para \\(\\theta=-45^{\\circ}\\) \\[\\rho=50\\cdot\\cos(-45º)+50\\cdot \\text{sen}(-45º)=0\\] Figura 6.21: Transformada de Hough para um ponto. Na Figura 6.22(a), temos dois pontos, \\(a\\) e \\(b\\), onde foi realizada a transformada de Hough que tem a Figura 6.22(b) como espaço de saída. A reta \\(c\\) que passa por esses dois pontos, representada em pontilhado na Figura 6.22(a) e, na Figura 6.22(b), temos o ponto no plano que representa essa reta, ou seja, uma reta a uma distância \\(\\rho\\approx70,71\\) da origem com o ângulo de \\(45^{\\circ}\\). Figura 6.22: Transformada de Hough para um ponto em 45º. Na Figura 6.23(a) temos mais um exemplo, desta vez com um ponto localizado a sua direita, diferentemente da anterior, esses dois pontos formam uma reta de \\(-45^{\\circ}\\), fato que pode ser visto na Figura 6.23(b) onde o ponto de encontro das duas curvas acontece em \\(\\theta=-45^{\\circ}\\) com um valor de \\(\\rho=0\\) já que a reta cruza a origem, ou seja, não possui distância em relação a ela. Figura 6.23: Transformada de Hough para um ponto em -45º. Nosso último exemplo contém uma imagem com três pontos, onde temos três tipos de retas possíveis. Observando a figura 6.24(a), podemos ver os pontos \\(a\\), \\(b\\) e \\(c\\) e as retas \\(d\\), \\(e\\) e \\(f\\) que passam por eles e, na Figura 6.24(b), temos a transformada de Hough para essa imagem, algo interessante de se notar é o fato de a reta que passa pelos pontos \\(b\\) e \\(c\\) pode ser detectada duas vezes, isso se deve a uma característica da transformada de Hough chamada relação de adjacência reflexiva, ou seja, isso acontece como resultado pela maneira como \\(\\rho\\) e \\(\\theta\\) mudam de sinal quando chegamos as extremidades de \\(\\pm90^{\\circ}\\). Figura 6.24: Transformada de Hough para três pontos. Na Figura 6.25, temos nosso último exemplo na detecção de linhas, mas, desta vez, realizado em uma imagem real. Primeiramente, foi realizada a detecção de bordas pelo método de Canny, como pode ser visto na Figura 6.25(a). Logo após, foi realizada a transformação de Hough, com resultado em Figura 6.25(b) e, por fim, temos a imagem original com as linhas detectadas na Figura 6.25(c). Atenção ao fato de que nem todos os picos da transformada podem ser utilizadas como linhas, pois teríamos um número enorme delas, então utilizamos um threshold para somente as linhas que tiverem um número de votos (acumulação na matriz) superior a um valor limítrofe serem utilizadas. Figura 6.25: Resultado da transformada de Hough usada na detecção de linhas em uma imagem. 6.3.2 Transformada de Hough para detecção de círculos A transformada de Hough pode ser estendida para detecção de círculos através da substituição da equação da reta pela equação do círculo (6.29): \\[(x-x_0)^2+(y-y_0)^2=r^2 \\tag{6.29}\\] Nesse caso, também andamos por cada pixel \\((x,y)\\) das bordas da imagem e o levamos ao espaço de parâmetro com as seguintes equações (6.30) e (6.31): \\[x_0=x-r\\cos(\\theta) \\tag{6.30}\\] \\[y_0=y-\\text{sen}(\\theta) \\tag{6.31}\\] A diferença é que, neste caso, o nosso espaço de parâmetro terá três dimensões, porque, como desenhamos um círculo para cada pixel do círculo da imagem, a variação do diâmetro desse círculo deve levar a uma variação dos círculos descritos no espaço de parâmetros, então, também devemos variar os valores de \\(r\\) além dos valores de \\(x\\) e \\(y\\). Uma representação disso é vista na Figura 6.26(a), a qual temos três pixels que definem um círculo, na Figura 6.26(b), a qual temos os círculos no espaço de parâmetros, e, na Figura 6.26(c), podemos ver uma representação de um espaço de parâmetros com diferentes raios. Figura 6.26: Transformada de Hough para círculos [19, p. 255]. A Figura 6.27 contém uma imagem com algumas moedas. Na figura 6.28(a), temos as bordas da imagem detectada com o método de Canny. Logo após, na Figura 6.28(b) - (f), temos a representação do espaço de Hough para diferentes valores de raio. E, na figura 6.29, temos o resultado da detecção de círculo após encontrados os picos do espaço de parâmetros. Figura 6.27: Imagem original de moedas. Figura 6.28: Canny e espaço de parâmetros. Figura 6.29: Resultado final da transformada de Hough para círculos. 6.4 Detecção de Quinas Quinas são pontos chaves na visão computacional por serem muito úteis na descrição e correspondência de objetos usando poucos dados. E existem diferentes métodos para identificação dos pontos, dentre eles o mais comum é o de Harris, que é o sucessor do de Moravec [19, p. 178]. 6.4.1 Detector de Quinas de Moravec Moravec obtém sua medida de curvatura através de uma variação média de intensidade em quatro direções principais: \\((0,1), (0,-1), (1,0)\\) e \\((-1,0)\\). Isso é feito através da seguinte Equação (6.32), considerando a análise sobre o pixel \\((x,y)\\), o deslocamento \\((u,v)\\) e a janela \\(2w+1\\) [19, p. 185]. \\[E_{u,v}(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}[P_{x+i,\\ y+j} - P_{x+i+u,\\ y+j+v}]^2 \\tag{6.32}\\] Essa equação também aproxima a função de autocorrelação na direção \\((u,v)\\) [19, p. 186]. O detector de Moravec apesar de ser intuitivo seu funcionamento, ele considera apenas um pequeno conjunto de mudanças possíveis. Então, Harris propôs ainda avaliar a autocorrelação, mas por uma expressão analítica [19, p. 185]. 6.4.2 Detector de Quinas de Harris O detector de Harris é desenvolvido na ideia de Moravec e sua equação, mas com uma abordagem mais complexa. Harris assume que \\(P_{x+i+u,\\ y+j+v}\\) possa ser estimado pela série de Taylor de primeira ordem [19, p. 193]. Dessa forma (6.33), \\[P_{x+i+u,\\ y+j+v} = P_{x+i,\\ y+j} + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial x}u + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial y}v \\tag{6.33}\\] Substituindo a equação anterior, (6.33), na equação de Moravec, (6.32), produz-se (6.34) \\[E_{u,v}(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}[\\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial x}u + \\frac{\\partial{P_{x+i,\\ y+j}}}{\\partial y}v]^2 \\tag{6.34}\\] E expandindo a potência, (6.35) \\[E_{u,v}(x,y) = A(x,y)u^2 + 2C(x,y)uv + B(x,y)v^2 \\tag{6.35}\\] Esta última equação pode ser representada na forma de matriz (6.36). Representação útil para compreensão mais à frente neste tópico [19, p. 187]. \\[ \\begin{split} E_{u,v}(x,y) &amp;= \\begin{bmatrix}u &amp; v\\end{bmatrix} \\begin{bmatrix}A(x,y) &amp; C(x,y)\\\\ C(x,y) &amp; B(x,y)\\end{bmatrix} \\begin{bmatrix}u \\\\ v\\end{bmatrix} \\\\&amp;= D^TMD \\end{split} \\tag{6.36}\\] onde \\[A(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w}(\\frac{\\partial P_{x+i, y+j}}{\\partial x})^2 \\tag{6.37}\\] \\[B(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w} (\\frac{\\partial P_{x+i, y+j}}{\\partial y})^2 \\tag{6.38}\\] \\[C(x,y) = \\sum_{i=-w}^{w} \\sum_{j=-w}^{w} (\\frac{\\partial P_{x+i, y+j}}{\\partial x})(\\frac{\\partial P_{x+i, y+j}}{\\partial y}) \\tag{6.39}\\] Como \\(E_{u.v}(x,y)\\) tem a forma de uma função quadrática, então possui dois eixos principais. Então, podemos rotacioná-la a fim de alinhar seus eixos com os do sistema de coordenadas, obtendo \\(F_{u,v}(x,y)\\), Equação (6.40) [19, p. 187]. \\[F_{u,v}(x,y) = \\alpha(x,y)^2u^2 + \\beta(x,y)^2v^2 \\tag{6.40}\\] ou em sua forma matricial (6.41). Note que são rotacionados os eixos definidos por \\(D\\). \\[ \\begin{split} F_{u,v}(x,y) &amp;= R^TD^TMDR\\\\ &amp; = D^TR^TMRD\\\\ &amp; = D^TQD \\end{split} \\tag{6.41} \\] \\[Q = \\begin{bmatrix} \\alpha &amp; 0\\\\ 0 &amp; \\beta \\end{bmatrix} \\tag{6.42}\\] Os valores de \\(\\alpha\\) e \\(\\beta\\) são proporcionais à função de autocorrelação nos principais eixos. Dessa forma, \\(\\alpha\\) e \\(\\beta\\) serão pequenos se o pixel \\((x,y)\\) for de uma região com intensidade constante, um será de valor grande e outro pequeno se estiverem em uma borda reta, e ambos terão valores grandes se estiverem em uma borda com curvatura acentuada. Portanto, a medida de curvatura em um determinado ponto é definida como \\(k_k(x,y)\\), (6.43) [19, p. 187]. \\[k_k(x,y) = \\alpha \\beta - k(\\alpha + \\beta)^2 \\tag{6.43}\\] No qual \\(k\\) controla a sensibilidade do detector. Como \\(Q\\) é uma composição ortogonal de \\(M\\). Os elementos de Q são chamados de autovalores [19, p. 188]. Inferimos que \\[Q = R^TMR \\tag{6.44}\\] Então, a partir da equivalência de determinantes e traços, é possível produzir uma relação equivalente a \\(Q\\) com os valores da matriz \\(M\\), (6.45) e (6.46) [19, p. 188]. \\[\\alpha \\beta = A(x,y)B(x,y) - C(x,y)^2 \\tag{6.45}\\] \\[\\alpha + \\beta = A(x,y) + B(x,y) \\tag{6.46}\\] Assim, conforme (6.45) e (6.46), a medida de curvatura, (6.43), pode ser obtida por (6.47): \\[ \\begin{split} k_k(x,y) &amp;= \\alpha \\beta - k(\\alpha + \\beta)^2 \\\\&amp;= A(x,y)B(x,y) - C(x,y)^2 - k(A(x,y) + B(x,y))^2 \\\\&amp;=det(M) - k(trace(M))^2 \\end{split} \\tag{6.47}\\] A Figura 6.30(a) é a imagem original. A Figura 6.30(b) foi gerada usando o detector de Harris com uma vizinhança 5x5 (\\(w=2\\)) para cada deslocamento \\((u,v)\\), com a derivada sendo calculada pelo Operador de Sobel (3x3) e com sensibilizador \\(k=0.01\\). Limiarizou-se a imagem de curvatura, descartando os valores que não fossem maiores que 9% do valor máximo. E nas posições \\((x,y)\\) da imagem que continham as curvaturas, foi destacado em rosa. Observe que a imagem identificou as quinas do tabuleiro de xadrez e do cubo mágico, porém não detectou outras quinas como as das árvores. Além disso, foi encontrado quinas que não são próprias dos objetos, e sim da iluminação. Variando tanto o sensibilizador da função, \\(k\\), como o limiar é provável que consigamos encontrar mais quinas, com o custo de também poder classificar ruídos que foram identificados como bordas também como quinas. Entretanto, já vimos que o filtro gaussiano pode ser que nos ajude nesse problema. Figura 6.30: Exemplo de detecção de Quinas pelo método de Harris. 6.5 Detecção de Blobs Blobs, traduzido para português como bolhas, são regiões da imagem em que os pixels têm valores, aproximadamente, iguais. Uma boa representação - um tanto quanto artificial - disso é a função gaussiana, como pode ser vista sua representação 3-D, na Figura 6.31(a), e sua representação 2-D, na Figura 6.31(b), temos, em ambas, um conjunto de pixels com valores bem próximos, o que caracteriza um blob. Figura 6.31: Função gaussiana em 3-D e 2-D. Apesar do exemplo, a detecção de blobs não se restringe aos elementos circulares, mas a qualquer conjunto de pixels. 6.5.1 LoG Esse método utiliza o Laplaciano da Gaussiana, que já foi apresentado anteriormente, mas que, resumidamente, é o cálculo de derivadas de segunda ordem em uma imagem convolucionada com um filtro gaussiano. Isso gerará fortes respostas positivas em blobs escuros e negativas em blobs claros de tamanho \\(\\sqrt{2\\sigma}\\). Como existe uma relação entre as respostas e o tamanho do \\(\\sigma\\), é necessário realizar a operação com vários valores de \\(\\sigma\\), pois, assim, detecta-se blobs de diferentes tamanhos. Figura 6.32: Imagem de Campo Ultraprofundo do Hubble [20]. Como podemos ver na Figura 6.31 com diferentes valores de \\(\\sigma\\), consegue-se detectar objetos de tamanhos variados, por exemplo, na Figura 6.33(a), detectam-se as estrelas da Figura 6.32 que apresentam uma menor resposta ao filtro laplaciano. Figura 6.33: Laplaciano do Gaussiano com diferentes valores de sigma. Na Figura 6.34, tem-se o resultado da detecção dos blobs utilizando LoG. Note que nem todas as estrelas foram detectadas, isso se deve ao fato do uso de um valor de threshold, o qual queremos as detecções acima de um determinado limiar. Na Figura 6.35, pode-se ver o resultado com um valor de limiar menor, onde muito mais objetos foram localizados. Figura 6.34: Resultado da detecção de blobs com LoG. Figura 6.35: Resultado da detecção de blobs com LoG com um threshold menor. 6.5.2 DoG O método de DoG é, basicamente, o mesmo do anterior, mas possui uma certa vantagem, que é o fato de ele ser mais eficiente. Como também já foi mencionado no tópico na seção anterior, Marr-Hildreth, é possível aproximar o Laplaciano da Gaussiana através da Diferença de Gaussianas (DoG), ou seja, primeiramente, realiza-se a filtragem gaussiana com dois \\(\\sigma\\) diferentes e se faz a subtração entre os dois. Realizamos esse processo para diferentes pares de valores, dessa forma, obtém-se o mesmo espaço de escala construído com o processo do LoG. Na Figura 6.36, tem-se um exemplo de detecção por DoG. Figura 6.36: Resultado da detecção de blobs com DoG. 6.5.3 DoH Uma matriz Hessiana é uma matriz que contém as derivadas de uma função. No nosso caso, utilizamos a Hessiana de ordem 2, pois estamos trabalhando com imagens, logo duas dimensões. Ela pode ser representada da seguinte maneira (6.48): \\[H[f(x_1, x_2, \\dots,x_n)]= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\ldots &amp; \\frac{\\partial^2 f}{\\partial x^2_n} \\end{bmatrix} \\tag{6.48}\\] A matriz Hessiana tem muita utilidade, pois ela possibilita descrever a curvatura em um ponto da função multivariável, o que, no nosso caso, pode ajudar a detectar os blobs, já que eles são aglomerados de pixels e devem estar separados do restante da imagem, ou seja, um aglomerado claro em um fundo escuro e vice-versa. Isso irá fazer com que sua função tenha uma mudança de sinal que pode ser detectada através das informações da matriz. Além disso, como dito por Herbert Bay et al.[21], os detectores baseados na Hessiana são mais estáveis e repetíveis (tem a mesma resposta para a mesma imagem com diferentes ângulos, iluminações etc.). Um dos principais algoritmos que fazem uso dessa matriz se chama Speeded Up Robust Features (SURF)[21]. Esse método faz uso de várias técnicas que o tornam muito rápido, como seu próprio nome sugere. Uma dessas técnicas é o cálculo da integral da imagem, realizado a partir da soma de todos os pixels de uma área retangular a partir do \\(x\\) atual, sendo que este varia a medida que se anda pela imagem. Figura 6.37: Integral de uma imagem [22]. Como pode ser visto na Figura 6.37, a integral de uma imagem contém a soma das regiões, por exemplo, a primeira posição contém a soma de somente uma célula, no caso \\(1\\). A segunda posição tem a soma de duas células. Note que, na primeira linha, está, basicamente, somando as células de uma só linha. Na segunda linha, começa-se a formar regiões retangulares, por exemplo, na segunda linha e na terceira coluna há o valor \\(6\\), resultante da soma das seis células da primeira linha com a segunda. Com a integral, pode-se calcular a área de qualquer região com apenas quatro operações, da seguinte forma, (6.49): \\[soma = D+A-B-C \\tag{6.49}\\] onde \\({A,B,C,D}\\) formam uma região, como exemplo, o cálculo da área 2x2 no canto inferior direito da Figura 6.37, (6.50): \\[soma = 9 + 1 - 3 - 3 = 4 \\tag{6.50}\\] Isso nos ajuda na aplicação de box filters, já que precisaríamos da soma de determinadas áreas, porque, com isso, aumentamos a velocidade do método. Sendo \\(X=(x,y)\\) um ponto em uma imagem, sua matriz Hessiana em \\(X\\) a uma escala \\(\\sigma\\) é dada por: \\[H(X,\\sigma)=\\begin{bmatrix} L_{xx}(X, \\sigma) &amp; L_{xy}(X, \\sigma)\\\\ L_{xy}(X, \\sigma) &amp; L_{yy}(X, \\sigma) \\end{bmatrix} \\tag{6.51}\\] onde \\(L_{xx}(X, \\sigma)\\) é a convolução da imagem no ponto \\(X\\) com a derivada de segunda ordem gaussiana \\(\\frac{\\partial^2g(\\sigma)}{\\partial x^2}\\) e assim por diante [21]. Aqui, entra em cena mais um elemento para melhorar a velocidade do algoritmo, Bay, Herbert et al. utilizam box filters para aproximar o filtro gaussiano. Como podemos ver, na Figura 6.38, os dois primeiros filtros são os derivativos gaussianos discretizados e os dois últimos são os aproximados a partir de box filters. Figura 6.38: Filtro gaussiano discretizado e aproximado nas direções \\(y\\) e \\(xy\\) [21]. Chamamos as derivadas realizadas na imagem de \\(D_{xx}\\), \\(D_{yy}\\), \\(D_{xy}\\). Essas derivadas não são realizadas com somente um valor de \\(\\sigma\\), seguem o mesmo raciocínio que os detectores anteriores, usam uma sequência de valores para, assim, criar um espaço de escalas e conseguir detectar blobs de diferentes tamanhos. A determinante da Hessiana é dada por (6.52): \\[det(H_{\\text{aprox}}) = D_{xx}D_{yy}-(0.9D_{xy})^2 \\tag{6.52}\\] O valor \\(0.9\\) é um peso introduzido pelos autores Bay, Hebert et al. para corrigir as respostas quando utilizamos várias escalas de sigma e obter uma invariância escalar. Na Figura 6.39, tem-se o resultado de uma detecção de blobs realizada pela Determinante do Hessiano. Figura 6.39: Resultado da detecção de blobs com DoH. 6.6 Limiarização A limiarização, como próprio nome sugere, é um ou mais limiares, valores limites, que são responsáveis por segmentar uma imagem em regiões com base nos valores de intensidade e/ou propriedades desses valores [2, p. 486]. Sua boa repercursão dada em segmentações de imagem é por sua simplicidade de implementação, velocidade computacional e propriedades intuitivas [2, p. 486]. É importante salientar que a chance de sucesso da limiarização de intensidade é proporcional à largura e à profundidade do(s) vale(s) que separam os modos (ou classes) do histograma. E os principais fatores que afetam as propriedades do(s) vale(s) são [2, p. 487]: A separação entre picos: quanto mais distantes forem os picos entre si, melhores as possibilidades de separação da imagem; Índice de ruído da imagem: os modos ficam mais largos com o aumento do ruído; O tamanho relativo dos objetos e do fundo; A uniformidade da fonte de iluminação; A uniformidade da reflexão da imagem. Suponha os histogramas de intensidade de duas imagens composta por objetos claros sobre um fundo escuro, conforme Figura 6.40, de tal forma que os pixels de seus objetos e do fundo tenham valores de intensidade agrupados em modos (ou grupos, ou classes), para segmentar as regiões da imagem, que também pode ser visto como segmentar os modos do histograma, devem ser encontrados os limiares entre os modos capazes de ter a melhor segmentação2. Então, no histograma da Figura 6.40(a), usa-se um limiar, pois há um vale bem definido, já na Figura 6.40(b), há dois vales, logo é possível usar dois limiares para segmentar as regiões da imagem. A segmentação do objeto claro da imagem que possui o histograma da Figura 6.40(a) é dada por \\(g(x,y)\\), sua imagem de saída: \\[g(x,y) = \\begin{cases} 1,\\ se\\ f(x,y) &gt; T \\\\ 0,\\ se\\ f(x,y) \\leq T \\end{cases} \\tag{6.53}\\] Note que se cria uma binarização, uma imagem preta e branca, que pode ser vista como uma máscara a ser aplicada na imagem do histograma, multiplicando a imagem pela máscara. E, analogamente ao caso anterior, após definido os limiares de uma imagem com mais modos, por exemplo, numa imagem com três modos, as regiões poderiam ser identificadas pelas cores branca, cinza e preta. Figura 6.40: Histogramas de intensidade que podem ser divididos por um limiar único, (a), e limiares duplos, (b). [2, p. 486] Quando \\(T\\) é uma constante aplicável em uma imagem inteira, o processo é conhecido como limiarização global. Caso \\(T\\) mude ao longo da imagem, usamos o termo limiarização variável. E quando \\(T\\) denotar uma limiarização variável na qual o valor \\(T\\) em qualquer ponto \\((x,y)\\) em uma imagem depende das propriedades de sua vizinhança, por exemplo, a intensidade média dos pixels da vizinhança, o chamamos de limiarização local ou regional3 [2, p. 486]. Os problemas de segmentação que exigem mais do que dois limiares são difíceis (muitas vezes impossíveis) de se resolver e seus melhores resultados, geralmente, são obtidos através dos métodos como a limiarização variável ou aumento da região [2, p. 486]. O papel do ruído da limiarização O ruído de uma imagem é capaz de fazer com que fique difícil achar um limiar ideal para segmentar a imagem sem processamentos adicionais, pois o(s) vale(s) da imagem podem desaparecer [2, p. 487]. Observe que, conforme os exemplos da Figura 6.41 e seus respectivos histogramas, o aumento no desvio padrão nos níveis de intensidade do ruído gaussiano faz com que o vale que separava os dois modos desapareça, o que torna difícil a segmentação do fundo e do objeto. Figura 6.41: (a) Imagem de 8 bits livre de ruído, produção típica de uma Computação Gráfica. (b) Imagem com ruído gaussiano aditivo com \\(\\mu = 0\\) e \\(\\sigma\\) de 10 níveis de intensidade. (c) Imagem com ruído gaussiano aditivo com \\(\\mu = 0\\) e \\(\\sigma\\) de 50 níveis de intensidade. (d) a (f) Histogramas correspondentes [2, p. 487]. O papel da iluminação e refletância O problema da iluminação é quando não é possível ter uma incidência uniforme de luz, causando um sombreamento. O mesmo efeito acontece quando o problema não é na iluminação, mas nas características da superfície do objeto; pois a iluminação e refletância produzem problemas equivalentes. Note que, pela Figura 6.42, o histograma deixou de ser bimodal. Logo não é simples de segmentar imagens com problemas de iluminação e refletância. Figura 6.42: (a) é a imagem ruidosa. (b) é a rampa de intensidade no intervalo \\([0.2, 0.6]\\). (c) é o produto de (a) e (b). (d) a (f) são os histogramas correspondentes [2, p. 488]. Há três abordagens básicas para se resolver os problemas de iluminação e refletância para uma boa segmentação. Corrigir diretamente o padrão de sombreamento através de uma multiplicação com o comportamento inverso do sombreamento, por exemplo, uma iluminação não uniforme, porém fixa, como a da Figura 6.42, pode ser corrigida multiplicando a imagem pelo inverso do padrão de iluminação, que pode ser obtida na aquisição de uma imagem de uma superfície plana de intensidade constante. Outra alternativa é corrigí-lo por meio do processamento, por exemplo, utilizando a transformada top-hat. E a terceira abordagem é a de contornar isso utilizando a limiarização variável [2, p. 488]. 6.6.1 Limiarização Global Simples A limiarização global simples é um método iterativo básico e que não é o mais eficiente. Ele é um processo iterativo que denomina o limiar ideal como aquele que produz menor diferença entre as médias de intensidade dos modos, o segmentado e o desprezado [2, p. 488]. Ele consiste em: Selecionar uma estimativa inicial para o limiar global, \\(T\\). Segmentar a imagem usando \\(T\\). Isso dará origem a dois grupos de pixels: \\(G_{1}\\), composto por todos os pixels com valores de intensidade \\(&gt; T\\), e \\(G_{2}\\), composto pelos pixels com valores \\(\\leq T\\). Calcular os valores de intensidade média dos grupos: \\(m_{1}\\) e \\(m_{2}\\). Calcular um novo valor de limiar: \\(T = \\frac{m_{1}+m_{2}}{2}\\). Repita as Etapas 2 a 4 até que a diferença entre os valores de \\(T\\), com a iterações sucessiva e a atual, seja menor que o parâmetro predefinido \\(\\Delta T\\). A Figura 6.43 mostra um exemplo da aplicação de Limiarização Global Simples. A Figura 6.43(a) é a imagem de uma digital com ruído. A Figura 6.43(b) mostra que seu histograma possui um vale bem nítido e, pela aplicação do algoritmo, que usa \\(\\Delta T = 0\\) e inicia \\(T\\) como a média de intensidade da imagem, após três iterações, encontra-se o limiar \\(T = 125.4\\). A Figura 6.43(c) mostra a digital segmentada pelo limiar encontrado, \\(T = 125\\); como esperado, pelo nítido vale, foi satisfatório o resultado. Figura 6.43: (a) Impressão digital ruidosa. (b) Histograma. (c) Segmentação resultante usando um limiar global. [2, p. 489] 6.6.2 Limiarização pelo Método de Otsu O método de Otsu é um método estatístico que produz o chamado limiar ótimo a partir do histograma. O limiar ótimo é denotado por aquele que maximiza a variância entre classes ou minimiza a variância intraclasse [2, p. 489]. O primeiro passo é obter o histograma da imagem normalizado, isto é, no qual os pesos de cada intensidade são a probabilidade da ocorrência daquela intensidade na imagem. Segue abaixo a Equação (6.54) que representa um histograma normalizado, no qual \\(L\\) representa a quantidade de níveis de intensidade e \\(p_{i}\\), a probabilidade de ocorrência da intensidade \\(i\\) na imagem [2, p. 490]. \\[\\sum_{i=0}^{L-1}{p_{i} = 1,\\ p_{i} \\geq 0} \\tag{6.54}\\] Para entender a equação principal de Otsu, Equação (6.58), precisa-se compreender algumas equações que a compõe. A probabilidade de ocorrência do modo \\(1\\) é dada pela Equação (6.55) [2, p. 490]: \\[P_{1}(k) = \\sum_{i=0}^{k}{p_{i}} \\tag{6.55}\\] E o valor da intensidade média dos pixels da classe \\(1\\), \\(C_{1}\\), para dado limiar \\(k\\) pode ser calculado pela Equação (6.56) [2, p. 490] \\[\\begin{split} m_{1}(k) &amp; = \\sum_{i=0}^{k}{iP(i/C_{1})}\\\\ &amp; = \\sum_{i=0}^{k}{iP(C_{1}/i)P(i)/P(C_{i})}\\\\ &amp; = \\frac{1}{P_{1}(k)}\\sum_{i=0}^{k}{ip_{i}}\\\\ \\end{split} \\tag{6.56}\\] E a média acumulada (intensidade média) até o nível \\(k\\) ou da classe \\(C_{1}\\) é dada pela Equação (6.57) [2, p. 490]: \\[m(k) = \\sum_{i=0}^{k}{ip_{i}} \\tag{6.57}\\] Entendidas as equações anteriores, chegamos a equação principal, Equação (6.58), que denota a variância entre classes. \\[\\sigma_B^2(k) = \\frac{[m_GP_{1}(k) - m(k)]^{2}}{P_1(k)[1 - P_1(k)]} \\tag{6.58}\\] Então, o limiar ótimo é o valor que maximiza a variância entre classes, denominado \\(k^*\\), demonstrado pela Equação (6.59) [2, p. 491]. E conforme informado anteriormente, esse resultado é o mesmo que minimiza a variância dentro das classes; isso se deve a uma propriedade estatística que relaciona a variância global da intensidade da imagem, a variância interclasse e a variância intraclasse. \\[\\sigma_B^2(k^*) = \\max_{0 \\leq k \\leq L-1} \\sigma_B^2(k) \\tag{6.59}\\] Se a máxima variância entre classes existir para mais de um valor, é habitual se calcular a média desses valores \\(k\\) [2, p. 491]. Uma métrica adimensional, \\(\\eta\\), pode ser usada para obter uma estimativa quantitativa da separabilidade das classes, o que dá uma idéia da facilidade de segmentação e do resultado esperado. Seu cálculo é demonstrado na seguinte Equação (6.60). Para o cálculo de \\(\\eta\\), é necessário conhecer a variância global da intensidades da imagem, que pode ser calculada por (6.61). \\[\\eta = \\frac{\\sigma_B^2(k^*)}{\\sigma_G^2} \\tag{6.60}\\] \\[\\sigma_G^2 = \\sum_{i=0}^{L-1}{(i - m_G)}^2p_i \\tag{6.61}\\] Tendo o limiar ótimo, \\(k^*\\), segmentamos a imagem como já visto na introdução desta seção, Limiarização. Resumo do algoritmo de Otsu [2, p. 492]: Calcular o histograma normalizado da imagem de entrada. Designar os componentes do histograma como \\(p_i, i = 0, 1, 2, ..., L-1\\); Calcular as somas acumuladas, \\(P_1(k)\\), para \\(k = 0, 1, 2, ..., L-1\\); Calcular as médias acumuladas \\(m(k)\\), para \\(k = 0, 1, 2, ..., L-1\\); Calcular a intensidade média global, \\(m_G\\); Calcular a variância entre classes, \\(\\sigma_B^2(k)\\), para \\(k = 0, 1, 2, ..., L-1\\); Obter o limiar ideal de Otsu, \\(k^*\\), caso haja mais de um, faz-se a média dos valores; Obter a medida de separabilidade, \\(\\eta^*\\), a fim de estimar a qualidade da segmentação. A Figura 6.44(a) mostra uma imagem de microscópio ótico de células polimerosomas e a Figura 6.44(b), seu histograma. O objetivo deste exemplo é segmentar as moléculas do fundo. A Figura 6.44(c) é o resultado pela limiarização global simples. Como o histograma não tem vales distintos e a diferença de intensidade entre o fundo e os objetos é pequena, o algoritmo não conseguiu alcançar a segmentação desejada. A Figura 6.44(d) mostra o resultado obtido pelo método de Otsu. Esse resultado, obviamente, é superior ao da Figura 6.44(c). O valor do limiar calculado pelo algoritmo simples foi o de \\(169\\), enquanto o limiar calculado pelo método de Otsu era o de \\(181\\), que está mais próximo das áreas mais claras da imagem que define as células. A medida de separabilidade \\(\\eta\\) foi \\(0.467\\). Figura 6.44: (a) Imagem original. (b) Histograma (os picos elevados foram cortados para realçar os detalhes nos valores mais baixos). (c) Resultado da segmentação pela limiarização global simples. (d) Resultado da segmentação pelo método de Otsu. [2, p. 492] 6.6.3 Uso de suavização para limiarização O objetivo da suavização é tentar separar os histogramas de imagens ruidosas, que tendem a ser unimodais, em modos com vales mais profundos; pois, quanto mais profundo o vale, melhor será a segmentação da imagem. Atente-se ao tipo de média e ao tamanho do kernel, aconselha-se o filtro gaussiano, pois ele minimiza o borramento de fronteira, e suaviza o ruído ainda que de maneira mais branda do que um filtro de média. A Figura 6.45(a) mostra uma imagem ruidosa, a Figura 6.45(b) mostra seu histograma, a Figura 6.45(c) mostra o resultado do método de Otsu. Já a Figura 6.45(d) mostra a imagem da Figura 6.45(a) suavizada usando uma máscara de média de tamanho 5x5 e a Figura 6.45(e) é seu histograma e a Figura 6.45(f) é resultado da limiarização pelo método de Otsu. Figura 6.45: Exemplo de suavização antes da aplicação do método de Otsu [2, p. 493]. Apesar do filtro de média poder nos ajudar, nem sempre será capaz disso. A Figura 6.46(a) mostra uma imagem ruidosa e a Figura 6.46(b) mostra o seu histograma, observe que o pontinho branco parece nem estar presente no histograma. E após aplicado o método de Otsu, a Figura 6.46(c), observe que não foi obtida a segmentação desejada. Então, tentou-se um filtro de média 5x5, que reduz o ruído, Figura 6.46(d). O resultado no histograma foi a redução do espalhamento do histograma, Figura 6.46(e), mas a distribuição ainda é unimodal, resultando em falha na segmentação, o que é visto na Figura 6.46(f). Figura 6.46: Exemplo de insucesso na segmentação por Otsu, mesmo com prévia suavização [2, p. 493]. Portanto, note que, se a região que deseja segmentar for muito pequena em relação ao background e houver ruído, o que pode surgir na captura da imagem, a chance de não dar certo pelos métodos vistos é grande; pois, como os métodos que até agora vimos operam apenas no histograma da imagem, sem uso de maiores recursos. Como visto, imagens com essa característica, um mínimo ruído persiste e nem foi obtido um vale considerável entre as duas regiões. Isso pode ser atribuído ao fato de que a região é tão pequena que sua contribuição para o histograma é insignificante em comparação à intensidade da propagação causada pelo ruído [2, p. 493]. A solução para isso é o uso de máscaras de borda, que será detalhado a seguir. 6.6.4 Uso de bordas para limiarização Em uma imagem com ruído na qual a região a ser segmentada é muito pequena, é como se não houvesse aquela região e houvesse apenas o background. Isso é observado na aparência unimodal do histograma. Portanto, fica difícil estimar um limiar ideal pelos algoritmos supracitados. E, como visto anteriormente, é preciso uma aparência bimodal para uma boa segmentação, então, precisamos de um histograma equilibrado; para isso, tomamos o histograma das bordas mais destacadas da imagem. Isso pode ser resumido em gerar uma máscara de gradiente ou laplaciano da imagem, limiarizá-la com um valor alto e usar como máscara para imagem original e prosseguir com o processo de segmentação do objeto a partir dessa amostra, pois, dessa forma, é gerado um histograma simétrico e com um vale destacado, porque, com a máscara de borda, a probabilidade de um píxel estar no background ou foreground tende a ser equilibrada [2, p. 494]. O que se espera com os tipos de máscaras de borda, conforme visto no estudo detecção de bordas, é que a de gradiente produzirá bordas mais grossas e menor detecção aos ruídos da imagem, e a de laplace, bordas mais finas e maior detecção de ruídos, além de apresentar melhor custo computacional. Entretanto, é possível modificar este algoritmo para que tanto a magnitude do gradiente quanto o valor absoluto das imagens laplacianas sejam utilizadas; nesse caso, poderíamos especificar um limiar para cada imagem e formar a lógica OU dos dois resultados para obter a imagem marcadora, essa abordagem é útil quando se deseja ter mais controle sobre os pontos que foram considerados como sendo pontos válidos de borda [2, p. 494]. Resumo das etapas de segmentação pela identificação de bordas do objeto [2, p. 494]: Calcular uma imagem de borda da imagem capturada, \\(f(x,y)\\), ora como a magnitude do gradiente, ora como o valor absoluto do laplaciano, usando qualquer um dos métodos; Especificar um valor de limiar, \\(T\\); Limiarizar a imagem a partir da Etapa 1, utilizando o limiar estabelecido na Etapa 2 para produzir uma imagem binária, \\(g_{T}(x,y)\\). Esta imagem é usada como uma imagem de máscara na etapa seguinte para selecionar os pixels de \\(f(x,y)\\) que correspondem aos pixels “fortes” da borda; Calcular um histograma utilizando apenas os pixels de \\(f(x,y)\\), que correspondem aos endereços de pixel avaliados com o número \\(1\\) em \\(g_{T}(x,y)\\); Use o histograma da Etapa 4 para segmentar \\(f(x,y)\\) globalmente, utilizando, por exemplo, o método de Otsu. A Figura 6.47(a) e (b) mostram as mesmas imagens da Figura 6.46 e seu histograma. Vimos que essa imagem não adianta ser suavizada. Entretanto, usamos a estratégia de máscara de borda que obteve um ótimo resultado. A Figura 6.47(c) mostra o gradiente já limiarizado, A Figura 6.47(d) e (e) mostra a máscara multiplicada a imagem original, que tem um histograma mais relevante à segmentação. E a Figura 6.47(f) mostra o resultado da segmentação pelo novo histograma, 6.47(e), através do Método de Otsu. O limiar foi de \\(134\\), que fica aproximadamente a meio caminho entre os picos no histograma. Figura 6.47: Exemplo de limiarização por meio da máscara de borda do gradiente [2, p. 495]. Já a Figura 6.48(a) e (b) mostra uma imagem de 8 bits de células de levedura e seu histograma. A tentativa em detectar em segmentar os pontos claros pelo método de Otsu sem prévia etapa não foi sucedida, embora o método seja capaz de isolar algumas das regiões das células muitas da regiões segmentadas à direita não estão separadas. O limiar calculado foi de \\(42\\) e a medida de separabilidade foi de \\(0.636\\). A Figura 6.48 (d) mostra a imagem \\(g_T(x,y)\\) obtida pelo cálculo do valor absoluto da imagem laplaciana e a limiarização com \\(T\\) definido a \\(115\\) em uma escala de intensidade no intervalo \\([0, 255]\\). Este valor de \\(T\\) corresponde aproximadamente ao percentil \\(99.5\\) dos valores da imagem laplaciana absoluta; assim, a limiarização a este nível deve resultar em um conjunto de pixels reduzido, como mostra esta Figura. A Figura 6.48 (e) é o histograma dos pixels diferentes a zero no produto de (a) e (d). Finalmente a Figura 6.48 (f) mostra o resultado da segmentação global da imagem original utilizando o método de Otsu baseado no histograma da Figura 6.48 (e). Este resultado está de acordo com as localizações dos pontos claros na imagem. O limiar calculado pelo método de Otsu foi \\(115\\) e a medida de separabilidade foi de \\(0.762\\), sendo que ambos são superiores aos valores obtidos utilizando o histograma original [2, p. 495]. Figura 6.48: Exemplo de limiarização por meio da máscara de borda laplaciana [2, p. 496]. 6.6.5 Limiares Múltiplos A diferença entre os limiares múltiplos e o que vimos até agora é que se usa mais de um limiar para segmentar a imagem a fim de produzir uma melhor medida de separabilidade entre as classes, por conseguinte, melhor segmentação. Entretanto, como as aplicações que requerem mais de dois limiares, geralmente, são resolvidas com mais do que apenas valores de intensidade. Ao invés disso, o caminho é usar descritores adicionais (por exemplo, cor) e o problema é moldado para reconhecimento de padrões, como explicado a seguir em limiarização baseada em diversas variáveis [2, p. 497]. No caso das classes \\(K, C_1, C_2, ..., C_K\\), a variância entre classes se generaliza pela expressão (6.62) \\[\\sigma_{B}^{2} = \\sum_{k=1}^K{P_k(m_k-m_G)^2} \\tag{6.62}\\] na qual \\[P_k = \\sum_{i\\in C_k}{p_i} \\tag{6.63}\\] \\[m_k = \\frac{1}{P_k} \\sum_{i \\in C_k}{ip_i} \\tag{6.64}\\] As classes \\(K\\) são separadas por \\(K-1\\) limiares cujos valores, \\(k^*_1, k^*_2, ..., k^*_k-1\\): \\[ \\sigma_{B}^{2}(k^*_1, k^*_2, ..., k^*_{K-1}) = \\max_{0&lt;k_1&lt;k_2&lt;...k_{n-1}&lt;L-1}{\\sigma_{B}^{2}(k_1, k_2, ..., k_{K-1})} \\tag{6.65}\\] Como observado na Equação (6.65), o valor máximo é obtido por testes com todas as possibilidades de valores para cada limiar, mas não faz sentido assumir limiares para \\(0\\) e \\(L-1\\), pois são os extremos da faixa de intensidade. Também pode ser feito a avaliação de sua medida de separabilidade, Equação (6.66). Como exemplo, tomemos um histograma com três classes. [2, p. 497] \\[\\eta(k^*_1, k^*_2) = \\frac{\\sigma^2_{B}(k^*_1,k^*_2)}{\\sigma^2_{G}} \\tag{6.66}\\] A Figura 6.49(a) mostra a imagem de um iceberg. É notório que será possível dividí-la com dois limiares4 a partir das predominâncias de três grupos de intensidade. Olhando no histograma, 6.49(b), pelos vales bem destacados também é observado isso. Encontra-se pelo método de Otsu dois limiares, \\(80\\) e \\(177\\), com uma excelente medida de separabilidade de \\(0.954\\). Limiarizando a Figura 6.49(a) o resultado que se obtém é uma segmentação muito boa, Figura 6.49(c) [2, p. 498]. Figura 6.49: (a) Imagem de um iceberg. (b) Histograma. (c) Imagem segmentada em três regiões usando os limiares duplos de Otsu. [2, p. 496] 6.6.6 Limiarização variável Vimos perante subseções anteriores, que fatores como ruído e iluminação são impecílios para uma boa segmentação. Também foi visto que suavização e informações das bordas podem ser usadas para resolver isto. No entanto, é frequente o caso que essas estratégias são ineficientes ou nem possíveis. Como solução para tal, usamos limiares variáveis. 6.6.6.1 Particionamento da imagem O particionamento da imagem consiste em fracionar a imagem em retângulos suficientemente pequenos de maneira que eles tenham iluminação e refletância uniformes e aplicar o método de Otsu em cada um deles. O sucesso do método é análogo ao da máscara de bordas, ele produz, para cada fração, histogramas simétricos com vales profundos [2, p. 498]. A Figura 6.50(a) e (b) mostra uma imagem e seu histograma. Pelo seu histograma é plausível que não resultaria em uma boa segmentação, seja pelo método de Otsu, Figura 6.50(c) ou pelo método iterativo, Figura 6.50(d). Após fracionada a imagem, Figura 6.50(e), a segmentação por Otsu teve sucesso, Figura 6.50(f). [2, p. 498] Figura 6.50: Exemplo da técnica de particionamento da imagem [2, p. 498]. Figura 6.51: Representa o histograma das subimagens da Figura 6.50(e) [2, p. 498]. 6.6.6.2 Limiarização variável baseada nas propriedades locais da imagem A limiarização variável baseada nas propriedades locais da imagem é uma técnica em que se calcula um limiar para cada ponto, \\((x,y)\\), com base em uma ou mais propriedades calculadas em sua vizinhança. Apesar disso parecer trabalhoso, os algoritmos e hardwares modernos permitem o processamento rápido da vizinhança, especialmente para as funções comuns, como as operações lógicas e aritméticas [2, p. 499]. Utilizaremos como abordagem básica duas propriedades, \\(\\sigma_{xy}(x,y)\\) e \\(m_{xy}(x,y)\\), já que indicam o grau de contraste e intensidade média na vizinhança. Seguem cálculos da limiarização usando apenas a intensidade do ponto, sendo \\(T_{xy}\\), o limiar local. As equações seguintes, (6.67) e (6.68) são formas comuns de limiares variáveis locais [2, p. 499]: \\[T_{xy} = a\\sigma_{xy} + bm_{xy} \\tag{6.67}\\] em que \\(a\\) e \\(b\\) são constantes não negativas, e \\[T_{xy} = a\\sigma_{xy} + bm_{G} \\tag{6.68}\\] na qual \\(m_G\\) é a média global da imagem. Também pode ser usado predicados a fim de determinar o limiar, \\(T_{xy}\\), de segmentação. No entanto, o preço dessa limiarização mais rebuscada é um aumento no custo computacional [2, p. 499]. E a imagem de saída, \\(g_{xy}\\), pode ser representada como a Equação (6.69): \\[g(x,y) = \\begin{cases} 1,\\ se\\ f(x,y) &gt; a\\sigma_{xy}\\ \\ E\\ \\ f(x,y) &gt; bm_{xy}\\\\ 0,\\ caso\\ contrário \\end{cases} \\tag{6.69}\\] A Figura 6.52(a) é a imagem de células de levedura da Figura anterior 6.48(a). A Figura 6.52(b) é um exemplo da segmentação da Figura 6.52(a) com dois limiares. Entretanto, note que as células do canto superior direito foram segmentadas de forma unida. A Figura 6.52(c) é a imagem dos desvios padrão locais da vizinhança de tamanho 3x3 de cada píxel. E foi escolhida a média global ao invés da local, pois geralmente produz melhores resultados quando o fundo é quase constante e todas as intensidades de objeto estão acima ou abaixo da intensidade do fundo. Os pesos \\(a=30\\) e \\(b=1.5\\) foram assumidos. E, por fim, foi limiarizada pelo predicado exemplificado na Equação (6.68) e não pela intensidade de um ponto, Figura 6.52(d). Figura 6.52: Exemplo de limiarização variável baseada nas propriedades locais [2, p. 500]. 6.6.6.3 Usando média de movimento O método de médias móveis é usado geralmente quando os objetos de interesse são pequenos (ou finos) em relação ao tamanho da imagem, uma condição que as imagens de texto digitado ou manuscrito possuem [2, p. 501]. Segundo Gonzalez, essa aplicação é muito útil no processamento de documentos [2, p. 500]. O procedimento consiste em um kernel 1D que percorre a imagem linha por linha e calcula média móvel com base em um intervalo de um dado tamanho fixo. A regra inicial é usar um intervalo de tamanho cinco vezes maior que a largura média do objeto que deseja limiarizar [2, p. 501]. Digamos que \\(z_{k+1}\\) denota a intensidade do ponto encontrado na sequência de digitalização na Etapa \\(k+1\\). A média móvel (intensidade média) com este novo ponto é dada pela Equação (6.70) \\[\\begin{split} m(k+1) &amp; = \\frac{1}{n} \\sum_{i = k+2-n}^{k+1}{z_i}\\\\ &amp; = m(k) + \\frac{1}{n}(z_{k+1} - z_{k-n}) \\end{split} \\tag{6.70}\\] na qual \\(n\\) é o tamanho do intervalo ou número de pixels utilizados no cálculo da média e \\(m(1)=\\frac{z_1}{n}\\). Este valor inicial não é rigorosamente correto porque a média de um único ponto é o valor do ponto em si. No entanto, o usamos para que cálculos especiais não sejam necessários quando é executada pela primeira vez. Já que a média móvel é calculada para cada ponto da imagem, a segmentação é baseada no limiar \\(T_{xy}=bm_{xy}\\), em que \\(b\\) é constante e \\(m_{xy}\\) é a média móvel no ponto \\((x,y)\\) na imagem de entrada [2, p. 500]. A diferença desse método ao explicado anteriormente, limiarização variável baseada nas propriedades locais da imagem^(limiarização-variável-baseada-nas-propriedades-locais-da-imagem), é que, neste, usa-se um kernel 1D que avalia linha por linha da imagem pegando amostras de uma certa quantidade de pixels, na qual, conforme mencionado anteriormente neste tópico, a quantidade de pixels tem que ser cinco vezes mais larga que a largura do objeto que se deseja segmentar. Na Figura 6.53(a) mostra uma imagem de texto escrito à mão sombreada por um padrão de intensidade. Esta forma de sombreamento de intensidade é típica de imagens obtidas com um flash fotográfico. A Figura 6.53(b) é o resultado da segmentação pela limiarização global de Otsu. A Figura 6.53(b) mostra uma segmentação bem sucedida com limiarização local usando médias móveis, usando \\(n=20\\), já que a largura média do traço era de \\(4\\) pixels, e \\(b=0.5\\). Figura 6.53: Exemplo de aplicação da limiarização por médias móveis em um documento corrompido por um sombreamento típico de flash fotográfico. (a) Imagem original. (b) Aplicado método de Otsu. (c) Aplicado método de médias móveis. [2, p. 501]. Já na Figura 6.54(a) mostra uma imagem de texto escrito à mão corrompida por um sombreamento senoidal. Esta forma de sombreamento de intensidade é típica de quando o fornecimento de energia em um digitalizador de documentos não é apropriado. A Figura 6.54(a) é a imagem original, a Figura 6.54(b) é o resultado da limiarização por Otsu e a Figura 6.54(c) é o resultado pela média móvel. Os parâmetros utilizados foram o mesmo do anterior, sendo \\(n=20\\) e \\(b=0.5\\), o que mostra relativa robustez do método. Figura 6.54: Exemplo de aplicação da limiarização por médias móveis em um documento corrompido por um sombreamento típico de problemas em scanner em que o fornecimento de energia não é o apropriado. (a) Imagem original. (b) Imagem original aplicado o método de Otsu. (c) Imagem original aplicado método de médias móveis. [2, p. 502]. 6.6.7 Limiarização baseada em diversas variáveis Até agora, falamos apenas da limiarização baseada em uma única variável: intensidade dos tons de cinza. Em alguns casos, um sensor pode disponibilizar mais de uma variável para identificar cada pixel em uma imagem e, assim, permitir uma limiarização multivariada. Um exemplo notável é a imagem em cores, na qual os componentes são vermelho (R), verde (G) e azul (B). Neste caso, cada “pixel” é identificado por três valores e pode ser representado como um vetor 3-D, \\(z = (z_1+z_2+z_3)^T\\), cujos componentes são as cores RGB em um ponto. Estes pontos 3-D são frequentemente chamados de voxels para denotar elementos volumétricos em oposição aos elementos de imagem [2, p. 501]. Numa limiarização focada na intensidade de cinza, de apenas uma variável, avaliamos apenas a intensidade, um gráfico de duas variáveis (histograma convencional). A sua limiarização é simples. Já no R, G, B, gráfico tridimensional, avaliamos a distância dos pixels da imagem a um píxel de referência, e o limiar é representado pelo contorno de uma figura geométrica simétrica, como a esfera abaixo, na qual contém os pixels segmentados e tem como seu centro o pixel de referência. Conforme demonstrado na Figura 6.55. Figura 6.55: Segmentação multivariada no RGB. Gráfico 3-D [2, p. 295]. Suponha que queiramos extrair de uma imagem colorida todas as regiões com uma faixa de cor específica: por exemplo, tons avermelhados da Figura 6.56(a). Vamos denotar a cor avermelhada média em que estamos interessados, a amostra da imagem é demarcada pelo retângulo de bordas claras e finais em Figura 6.56 (a). Uma forma de segmentar uma imagem colorida com base neste parâmetro é calcular uma medida de distância, \\(D(z, a)\\), entre um ponto de cor arbitrária, \\(z\\), e a cor média, \\(a\\) para assim segmentar. A Figura 6.56(b) mostra o resultado desse algoritmo. \\[ g= \\begin{cases} 1,\\ se\\ D(z,a)\\ &lt;\\ T\\\\ 0,\\ caso\\ contrário \\end{cases} \\tag{6.71}\\] Figura 6.56: Exemplo de segmentação multivariada no RGB [2, p. 295]. Porém, esse cálculo de distância dos pontos ao centro em formato esférico é trabalhoso para o computador. Uma maneira mais eficiente é usar um delimitador cúbico. Nessa metodologia, o cubo é centralizado em \\(a\\) e suas dimensões ao longo de cada um dos eixos de cor são escolhidas em proporção ao desvio padrão das amostras da imagem ao longo de cada um dos eixos (R, G e B). Portanto, o procedimento da Figura 6.56(a) consistiu em calcular o vetor médio \\(a\\) utilizando os pontos de cor contidos no retângulo. Em seguida, calculou-se o desvio padrão dos componentes vermelho, verde e azul dessas amostras. Um cubo foi centralizado em \\(a\\), e as dimensões ao longo de cada um dos eixos RGB são o valor de \\(1.25\\) multiplicado pelo \\(\\sigma\\) ao longo dos eixos correspondentes, por exemplo, no eixo \\(R\\), vermelho, a dimensão do cubo é de \\((a_R - 1.25\\sigma_R)\\) até \\((a_R + 1.25\\sigma_R)\\), no qual \\(a_R\\) indica o valor do componente vermelho de \\(a\\). E, por fim, limiarizou-se a Figura. Refêrencias "],["deepLearning.html", "Capítulo 7 Deep Learning em visão computacional 7.1 Caracterização de IA, Machine Learning e Deep Learning 7.2 Redes neurais convolucionais (CNN) 7.3 Redes Neurais Siamesas", " Capítulo 7 Deep Learning em visão computacional Antes de iniciarmos o estudo sobre Deep Learning, e mais especificamente sobre redes neurais artificiais convolucionais, é importante termos uma visão ampla sobre a área e suas subdivisões para conseguirmos nos localizar em meio a essa área que cresce cada vez mais. Por isso começaremos falando um pouco sobre inteligência artificial e suas subdivisões, além de sua conexão e uso com visão computacional, que é o nosso foco. 7.1 Caracterização de IA, Machine Learning e Deep Learning Esses três termos costumam causar certa confusão, principalmente em pessoas que estão começando a estudar essa área. De maneira geral, o termo Inteligência Artificial (IA) denomina uma área que possui muitas vertentes e tópicos de estudos, onde a maioria tem o foco em conseguir fazer os computadores realizarem tarefas complexas, que anteriormente eram realizadas exclusivamente por humanos. No começo dos estudos sobre IA foram tentados e resolvidos muitos problemas que eram considerados difíceis para seres humanos, mas relativamente fáceis para os computadores [23, p. 1]. Esses eram problemas que podiam ser descritos formalmente, por meio de regras matemáticas, como exemplo, temos o jogo de xadrez, onde, em 1997, o campeão Garry Kasparov perdeu para o IBM Deep Blue (Figura 7.1). Figura 7.1: IBM Deep Blue [24]. Com o tempo começamos a perceber que a dificuldade não residia nesses problemas, mas naqueles que são realizados facilmente, até instintivamente e intuitivamente pelos humanos, como reconhecer rostos familiares, entender linguagens, etc. A questão é que os seres humanos, no dia a dia, recebem e processam quantidades enormes de informações, então tentar fazer os computadores realizarem essas atividades somente com regras descritas por nós não era algo viável, por isso os pesquisadores começaram a desenvolver técnicas onde o próprio computador, através de algoritmos, aprendesse a abstrair essas regras e informações sozinho de bases de dados brutos, a isso chamamos de Machine Learning (Aprendizado de Máquina). Dentro da área de Machine Learning, temos um conjunto de técnicas e áreas de pesquisa, sendo que uma delas utiliza um modelo baseado em cérebros biológicos, contendo neurônios e conexões conhecidas como Redes Neurais. Na Figura 7.2, temos uma representação dos neurônios do córtex cerebral humano, onde podemos ver as conexões formadas por eles, que se assemelham com o modelo de redes neurais da Figura 7.3. Figura 7.2: Representação da conexão de neurônios no córtex cerebral [25, p. 363]. Atualmente, como ouvimos muito se falar sobre IA’s, temos a tendência de pensar que essa é uma técnica moderna, mas a ideia de fazer os computadores imitarem o esquema de funcionamento do cérebro remonta a 1943, quando Warren McCulloch e Walter Pitts sugeriram a ideia em seu artigo “A logical calculus of the ideas immanent in nervous activity” [26]. Como pode ser visto na Figura 7.3, as redes neurais são formadas por camadas, sendo que os dados entram pela camada Input, são processados nas camadas Hidden e temos os dados de saída na camada Output. Cada uma dessas camadas é formada por um número de neurônios (representados pelos círculos) e suas conexões são representadas pelas setas. Por enquanto, não será aprofundado tanto no funcionamento das redes neurais, isso será abordado na Seção Redes Neurais Artificiais. Figura 7.3: Rede neural artificial [27]. Nos últimos anos, temos visto um leque de aplicações cada vez maior para as técnicas de IA. Nosso objetivo nesse material é introduzir, principalmente, o uso das redes neurais artificiais na área da visão computacional, que é identificada como uma das subáreas da Inteligência Artificial pois busca reproduzir algumas das capacidades humanas a partir de sistemas autônomos. O principal interesse da visão computacional é fazer com que computadores desempenhem funções semelhantes à visão humana, sendo capazes de receber dados visuais e com eles realizar reconhecimentos, classificações e análises. Análogo ao processo de aprendizado dos seres humanos, identifica-se que a melhora no desempenho da visão computacional está fortemente interligada com a evolução do aprendizado de máquina (machine learning), outro segmento da inteligência artificial. Antes de entrarmos realmente no assunto de redes neurais, vamos apresentar, resumidamente, alguns tópicos principais da área de machine learning, pois, como dito anteriormente, o deep learning e as redes neurais estão dentro dessa área, e o entendimento desses tópicos podem auxiliar no pleno entendimento dos tópicos futuros. 7.1.1 Aprendizado supervisionado e não supervisionado Dentro dos algoritmos de machine learning, existe uma característica que os separa em diferentes tipos, baseado em sua forma de aprendizado: os algoritmos de aprendizado supervisionado e não supervisionado. Na aprendizagem supervisionada, os algoritmos têm previamente os pares de entrada-saída, ou seja, para cada entrada temos o conhecimento prévio de sua a saída [28, p. 695], e, a partir disso, nosso algoritmo deve aprender a generalizar bem as entradas. Podemos formalizar isso da seguinte forma [28, p. 695]: Dado um conjunto de treinamento de \\(n\\) pares \\((x_1,y_1), (x_2,y_2),\\dots,(x_n,y_n)\\) onde \\(x_i\\) são as entradas e \\(y_i = f(x_i)\\), as saídas, nosso algoritmo deve descobrir a função \\(h\\), conhecida como hipótese, que melhor aproxime \\(f\\). Para sabermos se nossa hipótese aproxima bem \\(f\\), após ter treinado o algoritmo, utilizamos um conjunto de testes - que contém exemplos diferentes do conjunto de treinamento - e avaliamos o quão bem o algoritmo generaliza (dá respostas corretas) as novas entradas. Já na aprendizagem não supervisionada, não há nenhuma resposta para as saídas do algoritmo, ou seja, ele recebe somente os dados de entrada. Por isso, uma das principais tarefas designadas a esses tipos de algoritmos é a de clustering (agrupamento), onde o algoritmo aprende a encontrar padrões nos dados de entrada e os separam em grupos. 7.1.2 Redes Neurais Artificiais Parte da base teórica que fundamenta o aprendizado profundo surgiu inicialmente como modelos para entender o aprendizado, ou seja, como o cérebro funciona. Desta forma, estas teorias ficaram conhecidas como Redes Neurais, uma das áreas do aprendizado profundo que mais cresceram nos últimos anos [23, p. 1]. Atualmente, os conceitos de Redes Neurais abordam princípios mais genéricos, não restritos à perspectiva da neurociência. Mesmo que as Redes Neurais não sejam capazes de explicar muito sobre o cérebro, não podendo ser sugeridas como modelos realistas da função biológica, vários aspectos do aprendizado ainda continuam sendo inspirações. As redes foram pensadas para adquirir o conhecimento por um processo de aprendizagem. Semelhante ao que ocorre no cérebro, as interações entre os neurônios, ou pesos sinápticos, são responsáveis por armazenar o conhecimento. Em termos práticos, o conhecimento de uma rede seria a capacidade de uma máquina em realizar funções complexas de forma autônoma, como classificações e reconhecimentos de padrões. As redes também são capazes de generalizar a informação aprendida, extraindo características essenciais de exemplos e garantindo respostas coerentes aos novos casos [29, p. 28]. Mesmo que o termo Rede Neural só tenha começado a ganhar destaque nos últimos anos, os primeiros estudos teóricos começaram por volta de 1940 [23, p. 12]. Um dos primeiros trabalhos publicados foi “A Logical Calculus of the Ideas Immamente in Nervous Activity” de 1943, em que os autores, Warren McCulloch e Walter Pitts, apresentaram um modelo artificial de um neurônio a partir da teoria de redes lógicas de nós [23, p. 14]. A Figura 7.4 apresenta uma simplificação de um neurônio biológico, dividido em três partes principais: o corpo da célula, os dendritos e o axônio. Um neurônio recebe informações, ou impulsos nervosos, a partir dos dendritos. Estas informações são processadas no corpo celular e novos impulsos são transmitidos através do axônio para outros neurônios. A comunicação entre os neurônios, a sinapse, controla a transmissão dos impulsos, determinando o fluxo de informações com base na intensidade do sinal recebido [29, p. 36]. Figura 7.4: Representação de um neurônio biológico [30]. Por analogia ao parágrafo anterior, McCulloch e Pitts descreveram matematicamente um neurônio artificial como um modelo com vários terminais de entrada \\(x_m\\), representando os dendritos, e apenas um ponto de saída \\(y_k\\), como axônio (Figura 7.5). Para simular o comportamento das sinapses, cada entrada \\(x_m\\) é associada com um peso \\(w_{km}\\), sendo que o somatório representa a intensidade de sinais recebidos (\\(v_k\\)). Figura 7.5: Representação matemática de um neurônio artificial [29, p. 36]. O sinal de resposta é estabelecido por uma função de ativação \\(\\varphi\\) aplicada ao valor da soma ponderada, essa função apresenta comportamento limiar, Equação (7.1), em que a saída é \\(0\\) ou \\(1\\) conforme o valor limite (Figura 7.6). O modelo também pode incluir um bias (\\(b_k\\)) no somatório para aumentar o grau de liberdade da função de ativação e garantir que um neurônio não apresente saída nula mesmo que os sinais recebidos sejam nulos. O valor do bias é ajustado junto com os pesos sinápticos [29, p. 37]. \\[y_k=\\varphi(\\upsilon_k)= \\begin{cases} 1 \\text{ se } \\upsilon_k &gt; 0 \\\\ 0 \\text{ se } \\upsilon_k \\leq 0 \\end{cases} \\tag{7.1}\\] Figura 7.6: Função de ativação de limiar [29, p. 36] O modelo proposto por McCulloch e Walter Pitts poderia realizar classificações em duas categorias, entretanto, os pesos precisavam ser ajustados manualmente pois não tinham a capacidade de aprender [23, p. 14]. Uma das primeiras discussões sobre regras de aprendizagem nas correções dos pesos sinápticos foi publicada em 1949 no livro de Donald Hebb “The Organization of Behavior” [29, p. 64]. No postulado de Hebb, apresenta-se que a conexão entre os neurônios é fortalecida cada vez que esta é utilizada, assim, os caminhos neurais no cérebro são continuamente modificados e formam agrupamentos. A primeira rede neural com capacidade de aprender os pesos das categorias foi o Perceptron apresentado por Frank Rosenblatt em 1958 [29, p. 65]. O Perceptron tinha arquitetura semelhante à Figura 7.7, uma rede de camada única além da de entrada e de aprendizado supervisionado. Inicialmente, foram lançadas grandes expectativas sobre as possíveis aplicações do Perceptron, porém, as limitações logo começaram a ser destacadas, muitas descritas no livro de Marvin Minsky e Seymour Papert publicado em 1969. Uma das limitações é que o Perceptron de camada única realiza apenas a classificação de padrões linearmente separáveis em duas categorias, não podendo, por exemplo, representar o operador de lógica XOR, que não é linearmente separável [23, p. 14]. Figura 7.7: Arquitetura Perceptron [29, p. 47]. A imagem negativa sobre o Perceptron e suas limitações tecnológicas diminuiram a popularidade das redes neurais, o que reduziu o número de pesquisas na área até os anos 80 [23, p. 16]. O interesse pelas redes neurais começou a aumentar, principalmente, pelo uso da abordagem de processamento paralelo distribuído, como o aplicado no algoritmo de retropropagação (backpropagation) apresentado por Rumelhart, Hinton e Williams (1986). O backpropagation é o algoritmo mais utilizado para aprendizado profundo até hoje e foi crucial para o treinamento dos Perceptrons de múltiplas camadas (MLP, multi-layer Perceptron) [29, p. 184]. 7.1.2.1 Rede MLP Para que a rede de Perceptrons de múltiplas camadas pudesse aprender seria necessário a retropropagação dos erros de trás para frente entre as camadas, tornando possível a minimização da função custo. A necessidade do cálculo da derivada do erro implicou no aparecimento de funções de ativação diferentes da utilizada no modelo original do Perceptron, que não houvessem uma ativação abrupta, \\(0\\) ou \\(1\\), Figura 7.6 [29, p. 184]. Considerando que as funções de ativação são um dos elementos utilizados para a inclusão de não linearidade, ponto chave para que os modelos não se limitem aos padrões linearmente separáveis, a abordagem foi a incorporação de funções não lineares, mas “bem comportadas“, ou seja, que são “quase” lineares contínuas e deriváveis. Como as funções de ativação são responsáveis pelo intermédio das respostas entre as camadas, deveriam ser considerados formatos não lineares que não alterassem de forma radical a resposta da rede. Os perfis que mais se aproximavam destes comportamentos são os das funções sigmóides: a tangente hiperbólica e a função logística [31]. A função sigmóide tem seu formato em S, em que nas extremidades da função tem um comportamento constante, o que fica evidente no gráfico da função logística (Figura 7.8). O parâmetro \\(a\\) da equação logística, Equação (7.2), permite parametrizar o comportamento da função, alterando a inclinação. Quanto maior o valor de \\(a\\), mais a função sigmóide se aproxima da função de limiar, Figura 7.6. Figura 7.8: Função sigmóide [29, p. 39] \\[\\varphi(\\upsilon)=\\frac{1}{1+\\exp(-a\\upsilon)} \\tag{7.2}\\] Diferente da função limiar que assume valores \\(0\\) ou \\(1\\), a função logística tem resultados em um intervalo contínuo entre \\(0\\) e \\(1\\) [29, p. 40]. A função sigmóide também é diferenciável, enquanto que a função de limiar não. Uma forma anti-simétrica da sigmóide é a função tangente hiperbólica, Equação (7.3). A função tangente hiperbólica é definida no intervalo \\(-1\\) a \\(1\\), o que permite a função sigmóide assumir também valores negativos [29, p. 40]. \\[\\varphi(\\upsilon)=\\tanh(a\\upsilon) \\tag{7.3}\\] Ao se propor um método eficiente no treinamento dos Perceptrons de múltiplas camadas se tornou interessante a inclusão de uma ou mais camadas de neurônios ocultos entre a camada de entrada e de saída. A combinação de mais camadas permitiu que a rede fosse implementada para problemas mais complexos, não se restringindo às transformações lineares do modelo original do Perceptron. Por meio das camadas ocultas, é possível extrair de forma progressiva características importantes que definem os padrões de entrada [29, p. 184]. O neurônio matemático proposto inicialmente foi estendido para uma estrutura de conexões de elementos de processamento, os nós da rede. Os elementos foram organizados em camadas, e foram propostas diferentes configurações de conexões. Os formatos mais populares são definidos como uma arquitetura de rede neural, reconhecida pelo número de camadas da rede, número de nós em cada camada e tipo de conexão entre os nós. A arquitetura da rede MLP é composta por uma camada de entrada que recebe o sinal, uma camada de saída que retorna o resultado, e entre elas um número arbitrário de camadas ocultas (Figura 7.9). Geralmente, a escolha do número de nós na camada de entrada e saída é direta. Por exemplo, em uma aplicação com imagens, o número de neurônios na camada de entrada pode corresponder ao número de pixels da imagem e o da camada de saída poderia ser apenas um único neurônio indicando a probabilidade de ser de fato o que se procura, a chance de um resultado positivo. Já o arranjo das camadas intermediárias não é tão simples, muitas vezes é definido empiricamente com base nas características dos dados de entrada e na complexidade do problema [32]. Figura 7.9: Arquitetura da rede MLP [29, p. 186]. Uma classificação comum das arquiteturas é com base no padrão de conexões, sendo identificadas duas classes principais: redes diretas (feedforward) e redes recorrentes (feedback) [29, p. 46]. O modelo MLP tem arquitetura do tipo feedforward, em que a propagação da informação ocorre em uma única direção e os nós de uma mesma camada não são conectados entre si. A saída de uma camada é usada como entrada na próxima, sem loopings, ou seja, não são enviadas de volta [29, p. 47]. Já nas tipologias recorrentes ocorre o feedback, um processo de realimentação, em que as saídas de nós são reinseridas como entradas em nós anteriores (Figura 7.10). O comportamento dos ciclos é dinâmico controlado por atrasos unitários [29, p. 49]. A ideia do modelo é estimular sinais em efeito cascata com dependência temporal. Figura 7.10: Arquitetura rede recorrente [29, p. 49]. 7.1.2.2 Backpropagation Para explicar o algoritmo backpropagation no treinamento de redes neurais, utilizaremos um exemplo de aplicação de rede MLP para o reconhecimento de números. O código da rede é uma implementação do livro online “Neural Networks and Deep Learning” escrito por Michael Nielsen. Os dados de treinamento foram retirados do MNIST dataset, que contém mais de 60000 imagens escaneadas de números escritos juntamente com os rótulos de classificação. As informações foram coletadas pelo Instituto Nacional de Padrões e Tecnologia dos Estados Unidos (NIST), sendo que as imagens são em escala de cinza e de tamanho \\(28\\text{ x }28\\) pixels como na Figura 7.11. Figura 7.11: Um exemplo de número zero selecionado do MNIST dataset [33]. O conjunto de dados originais do MNIST é dividido em duas partes, uma que contém 60000 imagens para treinamento e a outra com 10000 imagens para a fase de testes, em que se avalia a acurácia da rede treinada para reconhecer os dígitos. No exemplo do autor Michael Nielsen, os dados de treinamento original também foram reorganizados em dois grupos, o primeiro com 50000 imagens que foram utilizados no treinamento e a outra parte, 10000 imagens, que foi reservada para a validação em que se definiu os hiperparâmetros da rede. Considerando imagens de \\(28 \\text{ x } 28\\) pixels, os dados de entrada foram definidos como um vetor \\(x\\) de dimensão \\(784\\), em que cada posição corresponde a um valor de pixel da imagem. Para o vetor \\(y\\) de saída da rede se estabeleceu a dimensão \\(10\\), em que cada posição faz referência a um dígito de \\(0\\) a \\(9\\). Assim, se uma entrada corresponde ao número \\(3\\) então a saída esperada será o vetor transposto na forma \\(y(x)=(0,0,0,1,0,0,0,0,0,0)^T\\). Com base no formato dos dados de entrada e saída da rede, o exemplo foi construído com uma rede MLP de três camadas como na Figura 7.12, com a primeira camada tendo \\(784\\) nós e a última camada com \\(10\\) nós. Na camada do meio, a camada oculta, utilizaremos \\(30\\) nós, mas vale destacar que o autor Michael Nielsen definiu o número de nós após alguns testes otimizando a escolha dos parâmetros da rede. Figura 7.12: Rede MLP com uma camada oculta [34]. Os dados são retirados do arquivo zip “mnist.pkl.gz’”, subdivididos em treinamento, validação e de teste, e em seguida configurados no formato proposto da rede. No Code Block 2, a rede é construída a partir do comando “Network([784, 30, 10], cost=QuadraticCost)”, em que cada argumento corresponde ao número de nós na camada. Os atributos da classe “Network” incluem o número de camadas “num_layers”, o número de nós em cada camada “sizes”, os pesos e bias iniciais, que são gerados aleatoriamente pelo método “default_weight_initializer()”, e a função custo “cost”. A função custo aplicada neste exemplo é definida na classe QuadraticCost, e foi usada a erro quadrático (Mean Squared Error - MSE). class Network(object): def __init__(self, sizes, cost=QuadraticCost): self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost A seguir apresentaremos um resumo da teoria matemática do método backpropagation e para facilitar este processo utilizaremos a nomenclatura dos elementos de uma rede neural com base no livro “Introduction To The Theory Of Neural Computation” [34, p. 116]. No treinamento de uma rede como a da Figura 7.12 é apresentado um conjunto de treinamento \\(\\{\\xi_k^\\mu,\\zeta_i^\\mu\\}\\), em que cada padrão apresentado \\((\\mu=1, 2,\\dots, p)\\) corresponde a um par: entrada (\\(\\xi_k^\\mu\\)) e saída esperada (\\(\\zeta_i^\\mu\\)). Neste exemplo, o número de padrões no treinamento é \\(p=50000\\). O índice \\(k\\) na camada de entrada faz referência ao valor em cada nó da camada, e o índice \\(i\\) aos nós da camada da saída. A resposta final da rede é identificada como \\(O_i\\) e o sinal de saída da camada oculta é \\(V_j\\). A conexão entre a camada de entrada e a oculta é estabelecida pelos pesos \\(w_{jk}\\), e os pesos \\(W_{ij}\\) conectam a camada de saída com a intermediária. O backpropagation é um método supervisionado em que o treinamento ocorre em duas fases [29, p. 163]. Na etapa forward, uma entrada é apresentada para a rede e de acordo com as conexões estabelecidas entre as camadas é propagado sucessivamente os sinais de respostas até a camada de saída, gerando um resultado que se espera ser o mais próximo do padrão. Cada nó de uma camada seguinte se conecta com todos os nós da camada anterior, sendo que o sinal recebido por este nó é uma ponderação dos pesos de todas as conexões. O sinal de entrada de cada nó recebe um bias e é passado para a próxima camada como uma resposta de uma função de ativação (\\(g\\)). A resposta de saída de um nó será denominada \\(V_j\\) se o sinal for para uma camada intermediária, ou \\(O_i\\) se for direcionado para a camada de saída. Imagine que um nó (\\(j\\)) da camada intermediária recebe como entrada: \\[h_j^\\mu=\\sum_{k}w_{jk}\\xi_k^\\mu \\tag{7.4}\\] e produz como resposta: \\[V_j^\\mu=g(h_j^\\mu)=g(w_{jk}\\xi_k^\\mu) \\tag{7.5}\\] Assim, um nó na camada de saída recebe como entrada o sinal propagado: \\[h_i^\\mu=\\sum_kW_{ij}V_j^\\mu=\\sum_kW_{ij}g(\\sum_kw_{jk}\\xi_k^\\mu) \\tag{7.6}\\] gerando como resposta da saída da rede: \\[O_i^\\mu=g(h_i^\\mu)=g(\\sum_kW_{ij}V_j^\\mu)=g(\\sum_kW_{ij}g(\\sum_kw_{jk}\\xi_k^\\mu)) \\tag{7.7}\\] No Code Block 3, a fase forward é representada pelo seguinte método: feedforward(self, a): for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a No Code Block 4, a função de ativação é a função logística definida pelo método “sigmoid()” e a sua derivada é calculada no método “sigmoid_prime()”. sigmoid(z): return 1.0/(1.0+np.exp(-z)) sigmoid_prime(z): return sigmoid(z)*(1-sigmoid(z)) Na segunda fase, backward, os pesos e bias são corrigidos camada a camada, no sentido da saída da rede até a sua entrada, em um processo iterativo de forma que a saída \\(O_i\\) fique cada vez mais próxima do padrão esperado \\(\\zeta_i\\), reduzindo o erro [29, p. 163]. Uma forma de avaliar como o erro é reduzido em relação às alterações dos parâmetros é determinando uma função erro, ou custo, dependente dos pesos e bias. Conforme Equação (7.8), adotamos o erro quadrático (MSE) como função custo. \\[E[w]=\\frac{1}{2}\\sum_{\\mu i}[\\zeta_i^\\mu-O_i^\\mu]^2 = \\frac{1}{2}[\\zeta_i^\\mu W_{ij}g(\\sum_kw_{jk}\\xi_k^\\mu)] \\tag{7.8}\\] No Code Block 5, a função custo (MSE) é apresentada no método “fn()” na classe “QuadraticCost”: fn(a, y): return 0.5*np.linalg.norm(a-y)**2 A redução do erro envolve um processo de otimização, denominado descida em gradiente, em que se busca determinar os parâmetros (pesos e bias) que minimizam a função custo [33]. Neste método, a variação do erro pode ser escrita como derivadas parciais do erro em função dos pesos, compondo o vetor gradiente do erro. Como o vetor gradiente aponta no sentido de maior acréscimo do erro, a variação dos pesos é dada pelo negativo do gradiente, garantindo a redução mais rápida do erro. Assim, conforme Equação (7.9), a regra do gradiente descendente aplicada nas conexões entre a camada oculta e de saída pode ser escrita como: \\[\\Delta W_{ij}=-\\eta\\frac{\\partial E}{\\partial W_{ij}}=\\eta\\sum_\\mu[\\zeta_i^\\mu-O_i^\\mu]g&#39;(h_i^\\mu)V_j^\\mu=\\eta\\sum_\\mu\\delta_i^\\mu V \\tag{7.9}\\] \\[\\delta_i^\\mu=[\\zeta_i^\\mu-O_i^\\mu]g&#39;(h_i^\\mu) \\tag{7.10}\\] A fórmula de modificações dos pesos é conhecida como regra delta e recebe o termo \\(\\eta\\), a taxa de aprendizagem, para promover uma correção gradativa, sem alterações bruscas [33]. O termo \\(g’\\) se refere a derivada da função de ativação e surge na fórmula devido a derivação da função erro. A regra delta aplicada nas conexões entre a camada oculta e de entrada utiliza a regra da cadeia pois as derivadas são em relação aos pesos \\(w_{jk}\\), que se apresentam como dependência mais implícita ao erro. A correção dos pesos pode ser representada pela Equação (7.11): \\[\\begin{split} \\Delta w_{ij}&amp;=-\\eta\\frac{\\partial E}{\\partial w_{jk}}=-\\eta\\sum_\\mu\\frac{\\partial E}{\\partial V_j^\\mu}\\frac{\\partial V_j^\\mu}{\\partial w_{jk}}=\\eta\\sum_{\\mu i}[\\zeta_i^\\mu-O_i^\\mu]g&#39;(h_i^\\mu)W_{ij}g&#39;(h_j^\\mu)\\xi_k^\\mu \\\\ \\\\&amp;=\\eta\\sum_{\\mu i}\\delta_i^\\mu W_{ij}g&#39;(h_j^\\mu)\\xi_k^\\mu=\\eta\\sum_\\mu\\delta_j^\\mu\\xi_k^\\mu \\end{split} \\tag{7.11}\\] \\[\\delta_j^\\mu=g&#39;(h_j^\\mu)\\sum_i\\delta_i^\\mu W_{ij} \\tag{7.12}\\] Esta regra também pode ser estendida para redes com mais de uma camada oculta [31]. A regra delta generalizada para a m-ésima camada de uma rede pode ser descrita pela Equação (7.13): \\[\\Delta w_{pq}^m=\\eta\\sum_\\mu\\delta_p^{m,\\mu}V_q^{m-1,\\mu} \\tag{7.13}\\] \\[ \\delta_p^{m, \\mu} = \\begin{cases} \\text{se m for a camada Output, } &amp;[\\zeta_p^\\mu-O_p^\\mu]g&#39;(h_p^{m,\\mu})\\\\ \\text{senão, } &amp;g&#39;(h_p^{m,\\mu})\\sum_r\\delta_r^{m+1,\\mu}w_{rp}^{m+1} \\end{cases} \\tag{7.14} \\] A correção dos pesos ocorre considerando as conexões entre cada duas camadas, uma mais próxima da saída (\\(p\\)) e a outra mais interna (\\(q\\)). O vetor \\(V_q\\) representa o sinal de ativação recebido pela camada dos nós “\\(p\\)”, e quando o cálculo envolve a camada de entrada e a primeira camada oculta este vetor é o padrão de entrada (\\(\\xi_k^\\mu\\)). O fator delta (\\(\\delta\\)) funciona como uma memória das respostas das camadas mais externas, ou seja, para modificar os pesos de trás para frente é necessário que as conexões das camadas mantenham memória das camadas que foram alteradas anteriormente. O algoritmo do backpropagation é utilizado na etapa de treinamento, Code Block 6, por meio do método “backprop()”: backprop(self, x, y): nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] zs = [] for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) for l in range(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(),delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) Como destacado anteriormente, a primeira fase do backpropagation é o feedforward. Nesta etapa é recebido um padrão de entrada (\\(x\\)) e os pesos e bias inicializados aleatoriamente. Após o somatório das ponderações dos pesos e bias entre duas camadas, este valor é salvo no vetor “zs[ ]”, e o resultado da ativação deste valor é salvo em “activations[ ]”. A entrada da próxima camada é o sinal de ativação salvo em “actvivation”. Este processo ocorre da entrada até a camada de saída, salvando os sinais de ativação das camadas ocultas (\\(V_j\\)) em “activations[ ]”. Na fase backward pass, calcula-se primeiro o delta (\\(\\delta\\)) a partir da resposta da camada de saída salva como o último elemento do vetor “activations[ ]” e do padrão de saída esperado (\\(y\\)). O valor de delta, neste caso, é calculado a partir do método “delta()” da classe “QuadraticCost” como o produto entre a diferença da resposta de saída de rede (\\(a\\)) e do valor esperado (\\(y\\)) com a derivada do sinal de ativação da última camada, conforme visto no Code Block 7: delta(z, a, y): return (a-y) * sigmoid_prime(z) Após o cálculo do primeiro delta, é determinado o incremento dos pesos (\\(\\Delta W_{ij}\\)) entre a última camada e a camada oculta como o produto do delta (\\(\\sigma_i\\)) pelo vetor de ativação (\\(V_j\\)) que a ultima camada recebeu como entrada. Os incrementos dos pesos são salvos no vetor “nabla_w[ ]”. Os deltas e incrementos dos pesos das camadas ocultas são obtidos de forma iterativa na estrutura de repetição. O cálculo do delta da camada m depende do somatório dos produtos do delta calculado anteriormente, da camada mais externa, com o vetor peso da camada m. O valor do somatório é multiplicado pela derivada do sinal de ativação da camada m. Em seguida, o valor do incremento dos pesos é obtido pelo produto do delta atual com o valor de ativação recebido pela camada m. Após realizar este mesmo processo para todas as camadas, a função retorna um vetor com os incrementos dos pesos com base em um padrão (\\(\\xi_k^\\mu,\\zeta_i^\\mu\\)), o que ocorre para todos os padrões de treinamento. Para acelerar o processo de aprendizagem, em vez de atualizar os pesos cada vez que se apresenta um padrão, o autor Michael Nielsen sugere no seu exemplo a utilização do método gradiente descendente estocástico. A ideia é agrupar de forma aleatória os padrões de entrada formando o que ele chama de “mini-batch”. No método update_mini_batch(), a função backprop retorna o incremento do peso calculado para cada padrão dentro do agrupamento, e estes são somados em nabla-w até todo o agrupamento ser apresentado, e então os pesos e os bias são ajustados. Em seguida são apresentados os outros “mini-batch” até que todo o conjunto de treinamento seja utilizado, encerrando uma época de treinamento. Ou seja, em cada época, o conjunto de treinamento é subdividido em agrupamentos, e os pesos são atualizados apenas no final de apresentação de cada agrupamento como demonstrado no Code Block 8: update_mini_batch(self, mini_batch, eta, lmbda, n): nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] O treinamento ocorre a partir do método “SGD()”, sigla para descida do gradiente estocástico, em que são passados como parâmetros o conjunto de treinamento, o número de épocas, o tamanho do agrupamento “mini_batch_size” e a taxa de aprendizagem. net.SGD(training_data,30,10,0.5, evaluation_data=test_data,monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_accuracy=True, monitor_training_cost=True) É no método “SGD()”, Code Block 10, que ocorre a subdivisão dos padrões de treinamento em agrupamentos “mini_batch”. Em seguida, é chamada a função “update_mini_batch()” para cada agrupamento até finalizar uma época, e isso se repete para todas as épocas. SGD(self, training_data, epochs, mini_batch_size, eta,lmbda = 0.0 evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): for j in range(epochs): random.shuffle(training_data) mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta, lmbda, len(training_data)) print (&quot;Epoch %s training complete&quot; % j) Dentro do método “SGD()” é possível configurar para avaliar o erro total e acurácia da rede após cada época de treinamento, tanto considerando os dados de treinamento quanto os dados de teste ou de validação. Para selecionar os dados de teste, eles devem ser passados como parâmetros no “evaluation_data”. Ao selecionar as opções “monitor_evaluation_cost” ou “monitor_training_cost” é chamado o método “total_cost()”, Code Block 11 que retorna a soma dos erros avaliados para todo o conjunto de dados. total_cost(self, data, lmbda, convert=False): cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) return cost Após cada época se estabelece um conjunto de pesos e bias, e ao utilizar o método “feedforward()” são estes parâmetros que definem a resposta de saída da rede (\\(a\\)) para cada padrão de entrada (\\(x\\)). Ao comparar a resposta (\\(a\\)) com o valor esperado (\\(y\\)) dentro da função custo MSE, método “fn()” da classe “QuadraticCost”, quantifica-se o erro para cada padrão. O método “accuracy()”, Code Block 12, é utilizado dentro do “SGD()” quando se configura “monitor_evaluation_accuracy= True” ou “monitor_training_accuracy= True”. Esta função retorna a soma de resultados em que os valores de saída da rede corresponderam ao valor esperado (\\(y\\)). O sinal da rede é calculado pela função “feedforward()”, que é utilizada para cada valor (\\(x\\)) do conjunto de dados, seja de treinamento ou de avaliação accuracy(self, data, convert=False): results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) Considerando que os valores do erro total e da acurácia são calculados para cada época, o método de treinamento “SGD()” retorna quatro vetores dentro de uma tupla, cada um com o número de posições correspondentes ao número total de épocas. Assim, se o treinamento ocorrer em \\(30\\) épocas, então a primeira lista da tupla terá \\(30\\) elementos correspondentes ao custo total dos dados de avaliação no final de cada época. return evaluation_cost, evaluation_accuracy,training_cost, training_accuracy Os resultados salvos podem ser plotados em gráficos para avaliar visualmente o desempenho da rede. Um gráfico muito comum para acompanhar o treinamento da rede é o de custo de treinamento, principalmente, porque o aprendizado é guiado pela minimização desta curva. Na Figura 7.13, apresenta-se a curva de custo para uma configuração que utiliza o conjunto total de treinamento (50000 imagens) e com 30 épocas. Entretanto, não é indicado ter apenas este gráfico como base para estabelecer os hiperparâmetros da rede, como a taxa de aprendizagem e o número de épocas de treinamento. Por exemplo, a Figura 7.14 também se refere a uma função de custo, mas para uma outra configuração de treinamento, que utiliza apenas 1000 imagens para treinamento e 100 épocas. Figura 7.13: Curva de custo no treinamento com 30 épocas e 50000 imagens. Figura 7.14: Curva de custo no treinamento com 100 épocas e 1000 imagens. No fim do treinamento, as duas redes apresentaram erros na mesma ordem de grandeza, mas a capacidade de reconhecer números é bem diferente entre as duas. Esta diferença pode ser percebida ao comparar os gráficos de acurácia (Figuras 7.15 e 7.16) considerando tanto os dados de treinamento quanto os de validação. O resultado do treinamento com todo o conjunto de dados mostra que a acurácia da rede para os dados de validação é bem próxima do resultado para os valores de treinamento, uma diferença de \\(1\\%\\). Já para a situação que utilizou apenas 1000 dados de treinamento, as curvas de acurácia para os dados de validação e de treinamento estão mais afastadas, apresentando uma diferença próxima de \\(14\\%\\). Figura 7.15: Curvas de acurácia para rede treinada com \\(30\\) épocas e \\(1000\\) imagens. Figura 7.16: Curvas de acurácia para rede treinada com \\(100\\) épocas e \\(1000\\) imagens. Ao observar apenas a curva de erro se imagina que a rede esteja aprendendo até o final do treinamento, visto que o erro continua diminuindo. Entretanto, ao analisar as curvas de acurácia se identifica que a acurácia determinada pelos dados de validação aumenta rapidamente até uma determinada época, próximo de 40 no segundo caso, e em seguida fica estagnada. Assim, após a 40ª época, a rede não está mais aprendendo a generalizar para os dados de validação, está ocorrendo overfitting, ou seja, o treinamento não está melhorando a capacidade da rede. Mesmo que a acurácia do treinamento esteja aumentando depois desta época, pode ser que a rede esteja apenas decorando os dados de treinamento, pois não está mais se atendo apenas às informações gerais necessárias para reconhecer os números de forma geral [33]. Os casos mais comuns de se ocorrer overfitting é quando o número de dados do treinamento é muito baixo, como neste segundo caso com apenas 1000 imagens. Nesta situação, a rede tem poucos exemplos para extrair informações gerais, precisando muitas vezes aumentar o número de épocas de treinamento para que se alcance um desempenho mínimo. Quanto maior o número de épocas pode ser mais evidente o efeito de overfiting, por isso se recomenda observar quando a acurácia da validação começa a estagnar e a ficar muito distante da curva de treinamento [33]. Observar o comportamento da acurácia da validação é um dos métodos para definir até quando a rede deve ser treinada, ou seja, o número de épocas. Os dados de validação ajudam no teste de diferentes configurações de hiperparâmetros da rede, como épocas de treinamento, taxa de aprendizado e número de nós. Só depois de definir estes parâmetros e treinar a rede que se recomenda a utilização dos dados de teste para verificar realmente a acurácia da rede, utilizando dados que ela ainda não teve contato [33]. Um teste com dados não conhecidos permite verificar se os parâmetros da rede podem ser aplicados em casos mais gerais ou se enquadram apenas em particularidades dos dados treinados. Por esta razão, na maioria dos casos os dados são divididos em três conjuntos - treinamento, validação e teste. 7.2 Redes neurais convolucionais (CNN) A área de deep learning tem conseguido ótimo desempenho em aplicações, principalmente, pelo desenvolvimento da área e pelo aumento do poder computacional e da quantidade de dados disponíveis [35, p. 431]. Um dos tipos de redes neurais, conhecida como Redes Neurais Convolucionais (Convolutional Neural Networks - CNN), também tem conseguido resultados ótimos, um dos motivos pelos quais foram adotadas com relevância pela área de visão computacional, substituindo muitas das técnicas antigas, que por utilizarem algoritmos mais “estáticos” eram difíceis de serem aplicados em diferentes áreas. 7.2.1 Blocos de construção de uma CNN Nas redes neurais mais simples usamos basicamente neurônios e conexões entre eles para realizar a construção de um modelo. Já as redes neurais convolucionais contam com algumas estruturas a mais que são o porquê de sua eficiência no trabalho com imagens. Veremos elas a seguir. 7.2.1.1 Operador de convolução A operação que dá nome a rede, a convolução, é uma operação realizada entre duas funções como explicada em Convolução, na seção de Filtros Digitais. No nosso caso, como estamos trabalhando com imagens, usamos a convolução discreta. Um ponto importante a se frisar é que matematicamente o que chamaremos de convolução é na verdade uma correlação, sendo que as duas são quase idênticas, a não ser pelo fato de que na convolução giramos o filtro (kernel) em \\(180^\\circ\\). A única vantagem que ganhamos em girar o filtro antes da operação é que ganhamos a propriedade comutativa, o que é útil matematicamente para derivação de provas mas não é importante na implementação de deep learning [23, p. 332]. Na literatura e nas bibliotecas de deep learning, incluindo CNN’s, se tornou comum chamar as duas operações de convolução [23, p. 333], então também usaremos essa convenção, utilizando a convolução sem girar o filtro, assim, uma correlação. Relembrando do Tópico de Convolução, a fórmula da correlação discreta é dada por: \\[g(x,y)=w(x,y)\\bigstar f(x,y)=\\sum_{s=-a}^a\\sum_{t=-b}^bg(s,t)f(x+s,y+t) \\tag{7.15}\\] Onde \\(w\\) é o nosso filtro (kernel) e \\(f\\), a nossa imagem. E relacionado a ela temos a fórmula da convolução: \\[g(x,y)=w(x,y)\\ast f(x,y)=\\sum_{s=-a}^a\\sum_{t=-b}^bg(s,t)f(x-s,y-t) \\tag{7.16}\\] Como podemos perceber observando as duas equações, essas operações são bem simples, sendo basicamente uma soma de produtos. Na Figura 7.17, temos uma representação de um passo da convolução, onde podemos observar a seguinte operação: \\[ \\text{w}\\text{*f}\\left(0,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,0+t\\right)\\,\\text{=}\\, \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,1\\right) \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,1\\right) \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(-1,-1\\right) =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0 +2\\cdot0+0\\cdot2+\\left(-2\\right)\\cdot1 +1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3 =0\\,-2-3\\,=\\,-5 \\tag{7.17} \\] Figura 7.17: Convolução de uma imagem de tamanho 5x5 com um kernel de tamanho 3x3 e seu respectivo resultado. O exemplo anterior, Figura 7.17, foi bem simples, mas sabemos que em várias aplicações não teremos a imagem de entrada representada por apenas uma matriz (configurando uma imagem em tons de cinza) mas em grande parte das vezes estaremos utilizando imagens que contém três dimensões, ou seja, teremos uma imagem no modelo RGB, onde estarão presentes três matrizes, cada uma representando um canal de cor. Na Figura 7.18, há um exemplo de convolução em imagens RGB, podemos ver que agora nosso kernel é também formado por três matrizes. Uma coisa importante a se notar é que o número de camadas do filtro têm que ser igual ao número de canais da imagem para que a operação de convolução possa ser feita. Figura 7.18: Convolução de uma imagem de tamanho 5x5x3 com um kernel de tamanho 3x3x3 e seu respectivo resultado. Na Figura 7.19, temos uma representação de uma camada de convolução com mais de um filtro. Para cada um dos filtros temos uma saída e, consequentemente, temos no resultado final um conjunto de dados onde o número de camadas de profundidade (também conhecidas como feature map, ilustrada pelas três matrizes em azul) corresponderão ao número de filtros aplicados a entrada. Essa saída então pode ser enviada para frente na rede, passando por mais convoluções e tendo mais características extraídas. Figura 7.19: Convolução com múltiplos kernels. Os dois exemplos anteriores, Figuras 7.17 e 7.19, também servem para nos mostrar uma das características da convolução que a fazem ser uma boa escolha para se trabalhar com imagens, chamamos essa característica de iterações esparsas (também conhecida como conectividade esparsa) [23, p. 335]. Esse atributo evidencia o fato de que cada unidade da saída, ou pixel, é conectada a somente uma fração das unidades de entrada. No nosso exemplo anterior, cada saída é conectada a uma região dos 243 pixels de entrada, 9x9x3. Isso é muito útil, pois nossa imagem pode ter milhões de pixels, e usando kernels de tamanhos menores, conseguiremos detectar pequenas características, como bordas, quinas, etc [23, p. 335]. Nas camadas de convolução, os valores que a rede deverá aprender são os valores presentes nos filtros, então dessa maneira teremos menos parâmetros para aprender e armazenar. Em uma rede neural simples, como vimos no tópico Rede MLP, uma imagem na entrada significa que cada pixel seria conectado a cada neurônio na próxima camada, assim, resultaria em uma rede excessivamente grande. Uma outra característica importante é o de compartilhamento de parâmetros, já que o mesmo filtro é aplicado a diferentes regiões da imagem utilizando os mesmos valores, diferentemente de uma rede neural sem camadas de convolução, onde temos uma matriz com pesos que são usados para somente uma conexão. O compartilhamento de parâmetros nos proporciona uma outra característica, que é a invariância à translação, isso quer dizer que se movemos a posição de um objeto na imagem de entrada, sua representação também será movida na imagem resultante [23, p. 339]. 7.2.1.1.1 Padding Nos exemplos de convolução, Figuras 7.17 e 7.19, vemos que conforme aplicamos o kernel na imagem de entrada, o tamanho da imagem de saída é reduzido. De fato, ao convolucionar uma imagem de tamanho \\(m \\text{ x }n\\) com um filtro de tamanho \\(k_m \\text{ x } k_n\\) a imagem resultante terá uma altura de \\(m - k_m + 1\\) e um comprimento de \\(n - k_n + 1\\). Esse tipo de convolução, onde a imagem resultante é menor geralmente é chamada de “valid” (válida). Se queremos a imagem de saída com o mesmo tamanho da imagem de entrada, temos que adicionar mais linhas e colunas em nossa imagem, isso é conhecido como padding. Nesse caso, utilizamos a fórmula \\(m + 2p - k_m + 1\\) e \\(n + 2p - k_n + 1\\) onde \\(p\\) representa o padding. Por exemplo, nas Figuras anteriores, Figuras 7.17 e 7.19, se quiséssemos uma saída de igual tamanho a da entrada, teríamos que utilizar um padding de \\(6 + 2p - 3 + 1 = 6 \\Rightarrow p = 1\\). 7.2.1.1.2 Stride Os exemplos de convolução que vimos anteriormente utilizavam passos de deslocamento de um em um, mas podemos também utilizar passos maiores, pois assim reduzimos o custo computacional ao realizar esses passos intervalados. Isso, claramente, tem um impacto no resultado final, diminuindo sua resolução, mas em casos onde não precisamos extrair características delicadas isso se torna uma boa opção [23, p. 348]. Quando utilizamos um valor de stride maior que um, isso também afetará o tamanho da saída, que será governada pela seguinte relação [36, p. 184]: \\[\\frac{m+2p-k_m}{s}+1 \\ \\times \\ \\frac{n+2p-k_n}{s}+1 \\tag{7.18}\\] Onde \\(m\\) e \\(n\\) são as dimensões da imagem, \\(p\\) é o padding, \\(k_m\\) e \\(k_n\\) são as dimensões do kernel e \\(s\\) é o stride. Na Figura 7.20 temos um exemplo com os passos (Figuras 7.20(a-d)) de uma convolução com \\(\\text{stride} = 2\\) utilizando um kernel de tamanho \\(3 \\text{ x } 3\\) sobre uma imagem de tamanho \\(5 \\text{ x } 5\\) e \\(\\text{padding} = 0\\). Figura 7.20: Representação de uma convolução com \\(\\text{stride} = 2\\). Adaptado de [37]. 7.2.1.2 Pooling Essa é uma camada muito importante, que tem como objetivo realizar a subamostragem (subsampling) da imagem para reduzir seu tamanho, e, consequentemente, diminuir o total de memória, processamento e parâmetros necessários, além de refrear o risco de overfitting [35, p. 442] [36, p. 187] [38, p. 114]. Como nas camadas de convolução cada unidade da saída é conectada a uma região de entrada, então, também devemos levar em consideração o tamanho, stride e padding. Mas, diferentemente da convolução, o “kernel”, ou, em outras palavras, a região que nos conectará com a entrada, não terá pesos mas apenas realiza uma operação, sendo as mais comuns o máximo ou a média [35, p. 442]. Na Figura 7.21, temos um exemplo de max pooling, onde podemos ver seu funcionamento em passos (Figuras 7.21(a-d)). Este exemplo utiliza uma região de \\(2 \\text{ x } 2\\), o que é muito comum [36, p. 187], com \\(\\text{stride}=1\\). Figura 7.21: Exemplo de aplicação do max pooling. Na Figura 7.22, temos outro exemplo de max pooling, mas desta vez realizado com uma entrada de maiores dimensões, podemos ver que a operação é realizada em cada uma das camadas do objeto de entrada, e que sua saída contém o mesmo número de camadas da entrada, sendo que é isso que tipicamente ocorre nesse tipo de operação [35, p. 443]. Figura 7.22: Exemplo da aplicação de max pooling em uma imagem com mais dimensões, uma RGB por exemplo. Apesar do pooling ser uma técnica muito difundida, podemos encontrar redes onde seus autores preferiram não utilizar pooling para realizar a subamostragem, mas utilizarem camadas de convolução com valores de stride e padding maiores para conseguir essa redução de dimensão [38, p. 117] [36, p. 188]. Essa maneira de trabalhar foi proposta por Springenberg et al. em seu artigo “Striving for Simplicity: The All Convolutional Net” de 2014, onde demonstram que mesmo redes sem camadas de pooling podem ter resultados bons em diferentes bases de dados, como o CIFAR-10 e ImageNet. 7.2.1.3 Camadas totalmente conectadas As CNN’s geralmente tem várias camadas de convolução seguidas por camadas de ReLU que por sua vez são seguidas por camadas de pooling, e esse processo vai diminuindo as dimensões \\(m \\text{ x }n\\) e aumentando a profundidade, ou seja, a quantidade de camadas de características (conhecida como feature maps) [38, p. 119] [35, p. 446]. Na Figura 7.23, temos uma representação desse processo através da topologia da rede. Ao final dessa rede temos uma quantidade grande de camadas com as características extraídas da imagem de entrada, e precisamos utilizar essas informações. Nessa mesma Figura, podemos ver que no final temos camadas totalmente conectadas (fully connected) que é uma rede neural regular, uma MLP [38, p. 119]. Figura 7.23: Típica arquitetura de uma rede neural convolucional [35, p. 447]. Na Figura 7.24, temos a representação da utilização dessas informações abstraídas da imagem, onde recebemos o resultado das camadas de convolução, um bloco de dados de \\(5 \\text{ x } 5 \\text{ x } 40\\) da última camada de pooling, que é então planificado (flattened) em um vetor contínuo de uma dimensão e dado como entrada a uma rede MLP que, ao final, tem uma camada Softmax que faz a classificação da imagem de entrada. Figura 7.24: Camada totalmente conectada [38, p. 120]. 7.2.2 Por que usar convoluções Até agora entendemos os blocos de construção das CNN’s e os motivos pelos quais são usados. Devemos saber que a convolução não é apenas usada por ser mais eficiente no tratamento de imagens, mas também pela inspiração em nosso próprio sistema visual. 7.2.2.1 Córtex visual Como as próprias redes neurais, as CNN’s foram bio-inspiradas em estudos sobre o córtex visual do cérebro que começaram a ocorrer desde 1980 [35, p. 431], principalmente a partir dos trabalhos de David H. Hubel e Torsten Wiesel, onde foram realizados experimentos em animais, que permitiu eles deduzirem o funcionamento da estrutura do córtex visual. De uma maneira simplificada, os sinais de luz recebidos pela retina são transmitidos ao cérebro através do nervo óptico, após isso chegam ao córtex visual primário que é formado principalmente por dois tipos de células [23, p. 365]: Células simples: essas células têm comportamentos que podem ser representados por funções lineares em uma imagem em uma pequena área conhecida como campo receptivo [23, p. 365] [35, p. 432]. Esse tipo de célula inspirou as unidades detectoras mais simples nas CNN’s. Células complexas: também respondem a características da imagem, como as células simples, mas são invariantes a posição, ou seja, não fazem grande distinção de onde a característica aparece. Esse tipo de célula inspirou as unidades de pooling, que veremos mais adiante [23, p. 365]. Anatomicamente, quanto mais nos aprofundamos nas camadas do cérebro, mais camadas análogas à convolução e pooling são passadas, e encontramos células mais especializadas que respondem a padrões específicos sem serem afetadas por transformações na entrada. Sendo que até chegar nessas camadas mais profundas, é realizado uma sequência de detecções seguidas de camadas de pooling [23, p. 365]. 7.2.3 Redes CNN’s clássicas 7.2.3.1 LeNet Após estudar os principais blocos de construção de uma rede convolucional (CNN) - camada de convolução (convolutional layer), camada de redução (pooling layer) e camadas totalmente conectadas (fully connected layer) - torna-se mais fácil comparar as arquiteturas CNN’s e perceber que mesmo com as diferenças elas apresentam um padrão na combinação das camadas. Normalmente, as arquiteturas CNN’s possuem uma sequência intercalada de camada de convolução, seguida por uma camada pooling, que se repete até a ponta da rede onde se encontram algumas camadas totalmente conectadas com estrutura semelhante às redes MLP. Esta estrutura básica pode ser vista na Figura 7.25. À medida que são realizadas as operações ao longo da rede se percebe que os mapas ficam cada vez menores e que as camadas ficam mais profundas, ou seja, aumentam a quantidade de mapas em uma mesma camada. Como a camada de saída geralmente se apresenta como um vetor de probabilidades para as classes de predição, existe uma transição da representação dos dados em mapas para vetor, a partir do processo de flatten, que ocorre antes da primeira camada totalmente conectada. Figura 7.25: Rede Convolucional LeNet - A entrada é uma imagem de um número escrito à mão e a saída um vetor com a probabilidade para cada um dos dez dígitos de 0 à 9 [39, p. 250]. A rede LeNet foi uma das primeiras CNN’s que demonstrou potencial de aplicação em visão computacional. A rede foi criada por Yann LeCun em 1998 com propósito de reconhecimento de números manuscritos. A LeNet foi posteriormente adaptada para reconhecer dígitos para os depósitos em máquinas ATM, e ainda existem caixas eletrônicos que executam o código desenvolvido por Yann e seu colega Leon Bottou [39, p. 248]. A rede também foi amplamente utilizada para reconhecimento de dígitos do dataset do MNIST [35, p. 449], este dataset foi abordado no Tópico Backpropagation. Na Figura 7.25, consideramos como entrada uma imagem padrão do MNIST, de tamanho \\(28 \\text{ x } 28\\) pixels e com um canal em escala de cinza. No esquema geral da rede LeNet (Figura 7.26), temos uma visão mais clara da combinação das camadas, em que após a camada de entrada existem duas camadas convolucionais, intercaladas com duas camadas de pooling e na ponta três camadas totalmente conectadas. Cada camada de convolução utiliza um filtro \\(5 \\text{ x } 5\\) e uma função de ativação sigmóide. A primeira camada de convolução tem seis canais ou mapas, enquanto a segunda tem 16. A operação de pooling envolve um filtro \\(2 \\text{ x } 2\\) que calcula a média, por isso é identificada como “AvgPool”, e utiliza \\(\\text{stride}=2\\) para que cada mapa da camada anterior tenha uma redução pela metade ao longo da largura e da altura, descartando \\(75\\%\\) das ativações. O tamanho das três camadas totalmente conectadas (fully conected) são respectivamente, 120, 84, e 10. A última camada, “FC(10)”, corresponde ao número possível de classes, neste caso, 10 em razão dos dígitos de 0 à 9. A função de ativação na última camada é uma função gaussiana. Figura 7.26: Esquema geral das camadas na rede LeNet - Esquema da rede LeNet com a sequência de camadas convolucionais (“Conv”), pooling (“AvgPool”) e camadas totalmente conectadas (“FC”) [39, p. 252]. Para compreender os efeitos de cada camada sobre o conjunto de dados, apresentamos na Tabela 7.1, as dimensões das saídas de cada camada. Comentamos que de um bloco de convolução para o outro ocorre um aumento do número de canais (C) de 6 para 16, entre as camadas de pooling estes valores não são alterados, pois o processo só reduz a largura (W) e altura (H) dos canais. Nas camadas totalmente conectadas, as dimensões são reduzidas até se obter o tamanho do número de classes. É comum nas redes CNN’s que a quantidade de canais praticamente dobre depois de uma camada pooling visto que ocorre uma redução pela metade nas dimensões dos mapas. Assim, é possível aumentar o número de mapas, tornando mais sensível a identificação de características de baixo nível, como bordas e texturas, sem aumentar drasticamente o número de parâmetros e recursos computacionais [39, p. 258]. Como a primeira camada de convolução aplica \\(\\text{padding}=2\\), os mapas mantêm na saída a mesma dimensão que a imagem original (\\(28 \\text{ x } 28\\)), entretanto na segunda camada não se tem o padding, o que reduz em 4 pixels a largura e altura dos mapas. Tabela 7.1: Configurações, parâmetros e informações das camadas na LeNet - Resumo das configurações das principais camadas da LeNet, como o número de canais e tamanho dos filtros. Apresentação de uma estimativa do número de parâmetros e da quantidade de memória para treinar a rede. Layer Canais - C Tamanho - H e W Filtro - K Memória (kB) Parâmetros Inputs 1 28 Convolucional 1 6 28 5 18 156 Avg Pooling 1 6 14 2 5 0 Convolucional 2 16 10 5 6 2416 Avg Pooling 2 16 5 2 2 0 Flatten 400 1.6 0 Full Connect 1 120 0.5 48120 Full Connect 2 84 0.3 10164 Full Connect 3 10 0.04 850 Total 33 61706 Ao longo dos anos surgiram variações deste modelo e a diferença mais evidente entre as redes é o número de camadas, que foi aumentando ao longo dos anos tornando as redes mais profundas. Ao aumentar o número de camadas se percebia que o desempenho das redes tendia a melhorar, entretanto foram surgindo algumas limitações. Quanto maior a quantidade de dados, mais memória computacional é exigida, sendo que esta capacidade depende dos requisitos do hardware. Para avaliar a quantidade de memória utilizada no treinamento da rede LeNet vamos utilizar um cálculo aproximado com base na quantidade de elementos de saída em cada camada. O número de elementos é multiplicado pela quantidade de bytes necessária para armazenar cada elemento [40]. Considerando que os dados em ponto flutuante ocupam \\(32 \\text{ bits}\\), logo \\(4 \\text{ bytes}\\) por elemento, para facilitar a visualização dos resultados é utilizado a medida kilobyte (kB), e por isso foram divididos pelo fator 1024 já que \\(1 \\text{ kB} = 1024 \\text{ B}\\). Na Equação (7.19), exemplificamos o cálculo da quantidade de memória para a primeira camada de convolução da rede LeNet: \\[ \\begin{split} \\text{Quantidade de memória necessária }&amp; = \\text{CxHxW}\\\\ &amp;= 6 \\text{ x } 28 \\text{ x } 28\\\\ &amp;= 4704 \\text{ elementos da saída}\\\\ &amp;= 4704 \\text{ x }4 \\text{ bytes} = 18816 \\text{ bytes}\\\\ &amp;= 18.38 \\text{ kilobytes} \\end{split} \\tag{7.19} \\] O parâmetro C identifica o número de canais ou mapas da camada e o termo H e W, a altura e largura do elemento de saída da camada, respectivamente. Como identificado na Equação (7.19) e na Tabela 7.1, a quantidade aproximada de memória para a primeira camada seria \\(18 \\text{ kB}\\), e no total para a rede \\(33 \\text{ kB}\\). As primeiras camadas tendem a precisar de mais memória devido a maior dimensão (W e H) dos canais [40]. Aumentar o número de camadas também exige que mais parâmetros sejam considerados, o que afeta tanto o tempo de treinamento quanto o seu desempenho, pois se não ocorrer uma otimização adequada dos parâmetros pode ser maior a probabilidade de ocorrer overfitting [38, p. 230]. Para determinar aproximadamente a quantidade de parâmetros relacionados com cada camada considerou-se os pesos relacionados aos filtros de cada mapa, calculados como o produto entre as dimensões do filtro (\\(\\text{K x K}\\)), a quantidade de canais do elemento de entrada e a quantidade de canais de saída da camada [40]. Considerou-se também como parâmetros, os bias associados a cada canal de saída. Nas camadas totalmente conectadas, o número de parâmetros é determinado como produto da quantidade de elementos de entrada com a quantidade de elementos da saída da camada somado com o número de bias. A Equação (7.20) exemplifica os cálculos para a primeira camada de convolução: \\[ \\begin{split} \\text{Número de Pesos} &amp;= \\text{C}_\\text{saída} \\text{ x } \\text{C}_\\text{saída} \\text{ x } \\text{K} \\text{ x } \\text{K}\\\\ &amp;= 6\\text{ x }1 \\text{ x } 5 \\text{ x } 5\\\\ &amp;= 150\\\\\\\\ \\text{Número de Bias} &amp;= 6\\\\\\\\ \\text{Número de parâmetros} &amp;= \\text{Número de Pesos} + \\text{Número de parâmetros}\\\\ &amp;= 150 + 6\\\\ &amp;= 156 \\end{split} \\tag{7.20} \\] Considerando que o filtro na primeira camada é de tamanho \\(5\\text{ x }5\\), que a entrada só apresenta um canal e que são 6 canais na camada de convolução, a primeira camada de convolução considera aproximadamente 156 parâmetros. Na Tabela 7.1, também está a quantidade de parâmetros relacionados com cada camada e o total aproximado de parâmetros para a rede LeNet é 61706. À medida que aumenta o número de canais na camada de convolução mais parâmetros são necessários. De maneira geral, a maior parte dos parâmetros se deve às camadas totalmente conectadas devido ao maior número de conexões [40]. 7.2.3.2 AlexNet Atualmente existem várias arquiteturas de redes CNN’s utilizadas para aplicações na visão computacional. A evolução destas redes pode ser compreendida a partir dos resultados da competição ImageNet Large Scale Visual Recognition Challenge (ILSVRC). O principal objetivo da competição era avaliar algoritmos para detecção de objetos e classificação de imagens. A primeira edição da competição, em 2010, envolvia 1,2 milhões de imagens para o treinamento, sendo 1000 categorias de objetos. Nos dois primeiros anos de competição as redes CNN’s ainda não tinham sido as primeiras colocadas, porém a partir de 2012 os modelos CNN’s começaram a liderar a competição [41]. O progresso das redes pode ser avaliado com base na taxa de erro dos modelos que em sete anos caiu de aproximadamente \\(26\\%\\), no segundo ano da competição, para \\(2.3\\%\\) na última edição da competição em 2017 [40], como apresentado no gráfico da Figura 7.27. Figura 7.27: Taxa de erro dos modelos de melhor desempenho na competição ImageNet - O desempenho dos modelos na competição ImageNet Large Scale Visual Recognition Challenge (ILSVRC) era avaliado principalmente pela taxa de erro. No gráfico são apresentados os modelos que venceram em cada edição da competição, que ocorreu de 2010 à 2017, e também redes que se tornaram populares como a VGG [40]. Para conhecer um pouco dos diferentes modelos de redes CNN’s e perceber algumas diferenças, e estruturas que tiveram bom desempenho e permaneceram em modelos mais atuais, destacaremos a seguir quatro arquiteturas que ficaram bastante conhecidas e tiveram destaque na competição. A rede AlexNet foi a primeira CNN que venceu a competição ImageNet, no ano de 2012 com uma taxa de erro de \\(16.4\\%\\). A rede VGG não liderou a competição em 2014, porém é um dos modelos com bastante popularidade e que apresentou uma estrutura em blocos estabelecidos com base em regras. No ano de 2014, a CNN GoogLeNet que venceu a competição foi o ponto de partida para as redes Inceptions. A rede Residual (ResNet) em 2015 além de aproveitar as técnicas de maior desempenho das outras redes também implementou uma abordagem que possibilitou aumentar para mais de 100 camadas. A rede AlexNet foi desenvolvida por Alex Krizhevsky, Ilya Sutskever, e Geoffrey Hinton [35, p. 450]. Esta rede é bem semelhante a LeNet-5, porém apresenta mais camadas. Por ser uma rede mais profunda, exigindo maior quantidade de memória, a rede original precisou ser distribuída entre dois GPU’s de 3 GB de forma física [42]. Desta forma, a rede foi desenhada como na Figura 7.27, com uma estrutura de fluxo de dados duplo para que cada GPU recebesse metade do modelo. Figura 7.28: Arquitetura da rede AlexNet - representada como a combinação de duas redes idênticas, pois originalmente o treinamento ocorreria com a distribuição dos dados entre duas GPU’s [42]. Como esquematizado na Figura 7.28, a AlexNet tem 5 camadas de convoluções, sendo as três primeiras intercaladas por camadas pooling. A diferença mais visível entre as arquiteturas AlexNet e LeNet são as três camadas de convolução a mais na rede AlexNet, que estão seguidas uma depois da outra sem camada pooling entre elas. Como as imagens de entrada são maiores que do dataset MNIST abordado na rede LeNet, os filtros de convolução na entrada são maiores (\\(11\\text{ x }11\\)) e se utiliza \\(\\text{stride} = 4\\). Na segunda camada de convolução, os filtros têm tamanho \\(5\\text{ x }5\\), e nas demais camadas de convolução se utilizam filtros \\(3\\text{ x }3\\). Com a descoberta de que as funções de ativação ReLU’s nas camadas de convolução e que o maxpooling melhoram o desempenho das redes, a maioria dos modelos foram construídos utilizando estes artifícios [39, p. 250]. Os filtros maxpooling de tamanho \\(3\\text{ x }3\\) e \\(\\text{stride} = 2\\) reduzem a dimensão dos canais com base no maior valor do campo de recepção. Com exceção da primeira camada, todas as demais camadas de convolução têm padding para que a dimensão dos canais não seja alterada após as convoluções. As três últimas camadas são totalmente conectadas e apresentam respectivamente os tamanhos, 4096, 4096 e 1000. A camada de saída tem dimensão 1000 devido ao número de classes possíveis da competição ImageNet e a função de ativação é a Softmax. Figura 7.29: Comparação das redes AlexNet e LeNet. (a) é a Rede LeNet e (b), a Rede AlexNet. Estes esquemas gerais das camadas apresentam que a principal diferença das redes é que a AlexNet é mais profunda, com três camadas de convolução a mais do que a LeNet [39, p. 261]. O mesmo padrão para as dimensões dos elementos de saída das camadas visto em LeNet é visto na Tabela 7.2 para a rede AlexNet. Enquanto o tamanho dos canais diminui entre uma camada de convolução para outra, a quantidade de canais cresce, sendo 96 na primeira, seguida por 256, 384, 384 e 256. Após o processo de flatten, a dimensão das camadas é reduzida até se estabelecer o tamanho do vetor de classes de predição. Ao comparar a quantidade de memória e o número de parâmetros aproximados como foi descrito no Tópico de LeNet observa-se que certamente a quantidade de memória exigida aumenta e o número de parâmetros também. Os cálculos aproximados indicam que enquanto a memória necessária para o treinamento da rede LeNet seria de \\(33 \\text{ kB}\\), na AlexNet seria aproximadamente \\(3\\text{ GB}\\). O número de parâmetros calculados para LeNet foi 62 mil e para AlexNet 62 milhões. Mas de maneira geral, nos dois modelos, as primeiras camadas demandam mais memória, enquanto que as camadas totalmente conectadas precisam de mais parâmetros. Tabela 7.2: Configurações, parâmetros e informações das camadas na AlexNet - Resumo das configurações das principais camadas da AlexNet, como o número de canais e tamanho dos filtros. Apresentação de uma estimativa do número de parâmetros e da quantidade de memória para treinar a rede. Layer Canais - C Tamanho - H e W Filtro - K Memória (kB) Parâmetros Inputs 3 227 Convolucional 1 96 55 11 1134 35 Max Pooling 1 96 27 3 273 0 Convolucional 2 256 27 5 729 615 Max Pooling 2 256 13 3 169 0 Convolucional 3 384 13 3 254 885 Convolucional 4 384 13 3 254 1327 Convolucional 5 256 13 3 169 885 Max Pooling 3 256 6 3 36 0 Flatten 9216 36 0 Full Connect 1 4096 16 37753 Full Connect 2 4096 16 16781 Full Connect 3 1000 4 4097 Total 3090 62378 7.2.3.3 VGG A rede VGG foi construída dentro do grupo Visual Geometry Group (VGG) na Universidade de Oxford pelos pesquisadores Karen Simonyan e Andrew Zisserman [39, p. 265]. Até agora vimos que as redes LeNet e AlexNet apresentam a sua estrutura em duas partes principais, uma com as camadas iniciais contendo uma combinação de camadas convolucionais e pooling, e na outra parte estão as camadas totalmente conectadas (fully conected) na ponta da rede. Nestas redes, geralmente, é necessário selecionar individualmente vários parâmetros, por exemplo, nas camadas de convolução, selecionam-se o número de canais, tamanho dos filtros, do padding e do stride. Na camada pooling, os hiperparâmetros são o tamanho do filtro e do stride. Em geral, estas duas redes não apresentam um guia geral de como selecionar os parâmetros, o que torna mais complexo desenhar novas redes e que sejam mais profundas. O que se destacou na VGG em relação aos dois modelos anteriores é a introdução de princípios para estabelecer a estrutura da rede, o que permitiu a construção de modelos mais profundos [39, p. 265]. Outro aspecto característico da VGG é a estrutura em blocos na parte da rede com as camadas convolucionais, em que cada bloco apresenta camadas convolucionais em sequência e na ponta uma camada pooling. Enquanto o modelo AlexNet, na Figura 7.30, apresenta 5 camadas convolucionais, a VGG apresenta cinco blocos com número variável de camadas de convolução, mas que em geral os primeiros blocos apresentam menos camadas. Da mesma forma que a AlexNet, na ponta da rede estão as três camadas totalmente conectadas, com dimensões também iguais nos dois modelos, e uma função de ativação Softmax na saída. Na Figura 7.30, está a representação da arquitetura VGG com 16 camadas, em que os primeiros dois blocos apresentam duas camadas convolucionais e os três últimos blocos possuem três camadas convolucionais. As camadas de convolução dobram de tamanho a cada bloco, sendo que cada camada no primeiro bloco tem 64 canais, no seguinte 128 e assim por diante até 512 no último bloco. A utilização da função de ativação ReLu na camada de convolução e pooling pelo valor máximo são estratégias que apresentaram bom desempenho na AlexNet e continuaram em outros modelos, como no VGG. Os principais princípios de design da VGG estabelecem que todos os filtros de convolução são \\(3\\text{ x }3\\) com \\(\\text{stride} = 1\\) e \\(\\text{padding} = 1\\), e que os filtros maxpooling são \\(2\\text{ x }2\\) com \\(\\text{stride} = 2\\). Após cada camada pooling, o número de canais dobra na camada de convolução. A ideia de fixar o tamanho dos filtros convolucionais partiu da percepção de que a combinação de dois filtros \\(3\\text{ x }3\\) apresenta um campo receptivo equivalente a um filtro \\(5\\text{ x }5\\), e que três filtros \\(3\\text{ x }3\\) equivalem a um de \\(7\\text{ x }7\\) [38, p. 212]. Fixando o tamanho dos filtros e seus \\(\\text{stride} = 1\\) e \\(\\text{padding} = 1\\) estabelece que a dimensão dos canais não se altere entre as camadas convolucionais, o único hiperparâmetro que precisa ser otimizado é a quantidade de camadas em cada bloco. Ao utilizar filtros \\(3\\text{ x }3\\), que são menores, porém, em maior quantidade que os utilizados na AlexNet (\\(11\\text{ x }11\\) e \\(5\\text{ x }5\\)), inclui-se mais não linearidade, permitindo que a rede aprenda mais características de baixo nível [38, p. 212]. Aumentando a profundidade da rede com mais camadas de convolução são incluídas mais funções não lineares de ativação. Mesmo sendo redes mais profundas, esta estratégia de utilizar filtros menores diminui o número de parâmetros. Considerando que duas camadas em sequência tem C canais cada uma, ao utilizar dois filtros \\(3\\text{ x }3\\), o número total de parâmetros é \\(2\\text{ x }3\\text{ x }3\\text{ x }\\text{C}^2 = 18\\text{C}^2\\), que é menor ao comparar à situação de um único filtro \\(5\\text{ x }5\\) com \\(25\\text{C}^2\\) parâmetros [40]. Certamente dobrar o número de canais entre os blocos deve fazer com que o número de parâmetros cresça rapidamente, e por isso se padronizou os filtros maxpooling para que se reduza as dimensões dos canais pela metade. Controlando o número de ativações que passam para as próximas camadas é possível manter aproximadamente constante o número de operações. Avaliando superficialmente que o número de operações é dado como a quantidade total de multiplicações e adições, podemos calcular para cada camada como produto de quatro parâmetros [40]: tamanho do filtro (\\(\\text{K x K}\\)), as dimensões do canais de entrada (\\(\\text{H x W}\\)), a quantidade de canais de entrada (\\(\\text{C}_{\\text{entrada}}\\)) e canais na saída (\\(\\text{C}_{\\text{saída}}\\)): \\[ \\begin{split} \\text{Número de operações} &amp;= \\text{Número de elementos de saída x Operações por elemento de saída}\\\\ &amp;= (\\text{C}_{\\text{saída}} \\text{ x H x W}) \\text{ x } (\\text{C}_{\\text{entrada}} \\text{ x K x K})\\\\ &amp;= (2\\text{C x HW}) \\text{ x } (\\text{2C x 3 x 3})\\\\ &amp;= 36 \\text{HWC}^2 \\end{split} \\tag{7.21} \\] No caso de duas camadas de convolução com filtros \\(3\\text{ x }3\\) e separadas por um pooling, com redução pela metade da dimensão dos canais (\\(\\text{2H x 2W} \\rightarrow \\text{H x W}\\)) e dobrando o número de canais (\\(\\text{C} \\rightarrow \\text{2C}\\)), a quantidade de pesos aumenta de \\(9\\text{C}^2\\) para \\(36\\text{C}^2\\), porém o número de operações se mantêm em \\(36\\text{HWC}^2\\). Figura 7.30: Comparação das redes VGG e AlexNet - Comparação das redes VGG e AlexNet com base na estrutura geral das camadas. Enquanto a parte final das redes é semelhante em relação às camadas totalmente conectadas, a VGG se diferencia por ser mais profunda e apresentar um padrão das camadas convolucionais organizadas em blocos [40] 7.2.3.4 GoogLenet e Inception Ao acompanhar a evolução das CNN’s podemos perceber que a principal estratégia para aumentar o desempenho na classificação das imagens foi aumentar o número de camadas que guardam os pesos das redes. As redes AlexNet e VGG-16 foram desenvolvidas com 8 e 16 camadas, respectivamente. À medida que as redes se tornavam mais profundas surgiu o dilema de como tornar os algoritmos mais eficientes, visto que mais camadas significava maior quantidade de parâmetros e operações, exigindo maiores recursos computacionais. Comparando as redes na Figura 7.31 é possível verificar a acurácia das redes, a quantidade de parâmetros e o número de operações. Verifica-se que para a rede VGG-16 alcançar resultados melhores que a rede AlexNet foi necessário mais do que duplicar a quantidade de parâmetros, de aproximadamente 65 milhões na AlexNet para um pouco mais que 130 milhões na VGG-16. Na competição da ImageNet de 2014, um grupo de pesquisa da Google liderado por Christian Szegedy propôs a arquitetura GoogLeNet que deveria ao mesmo tempo garantir boa performance e ser mais eficiente que os modelos existentes [35, p. 452]. O modelo não só ganhou a competição como atendeu os seus requisitos, pois mesmo sendo uma rede com 22 camadas, mais que a VGG-16, precisou de 12 vezes menos parâmetros, 13 milhões em vez de 138 milhões [38, p. 217]. Figura 7.31: Gráfico da evolução das redes neurais CNN’s - O desempenho das redes é avaliado pela acurácia versus o número de operações necessárias para uma única etapa forward. O raio dos círculos é proporcional ao número de parâmetros, sendo que a legenda no canto inferior direito indica uma referência de \\(5\\text{ x }10^6\\) à \\(155\\text{ x }10^6\\) [43]. Para compreender a rede GoogLenet podemos dividi-la em três partes (Figura 7.32), na primeira parte, as camadas da entrada são semelhantes às redes AlexNet e VGG, na segunda parte, são os blocos inceptions característicos desta rede, e a última parte se refere a estrutura de classificação. A primeira parte contém dois blocos com uma sequência de camadas convolucionais intercaladas com pooling \\(3\\text{ x }3\\). No primeiro bloco, tem apenas uma camada de convolução \\(7\\text{ x }7\\), com \\(\\text{stride} = 2\\) e \\(\\text{padding}= 3\\), e uma camada pooling com \\(\\text{stride} = 2\\). Ao final destas duas camadas, o elemento tem 64 canais e teve uma redução por 4 em sua dimensão (H e W). No segundo bloco, são duas camadas de convolução, a primeira com filtro \\(1\\text{ x }1\\) e 64 canais e a segunda \\(3\\text{ x }3\\) com 192 canais, sendo que apenas o pooling \\(3\\text{ x }3\\) na ponta do bloco altera as dimensões dos canais pela metade. O principal papel destes dois blocos é reduzir de maneira considerável as dimensões da imagem, visto que a maior parte da memória exigida se deve às primeiras camadas [40]. Considerando que nesta etapa ocorre uma redução em 8 vezes das dimensões da imagem, uma entrada \\(224\\text{ x }224\\) ao se reduzir para \\(28\\text{ x }28\\) utilizará aproximadamente \\(7.5 \\text{ MB}\\) de memória enquanto a mesma redução no VGG-16 precisa de \\(42.9 \\text{ MB}\\), quase \\(6\\) vezes mais que a GoogLenet [40]. Também ao passar para as próximas camadas uma imagem de menor dimensão também se reduz o número de operações e a quantidade de parâmetros para treinar a rede. Figura 7.32: A estrutura geral da rede GoogLenet pode ser dividida em três partes: Parte A - Semelhante a AlexNet e LeNet, contém uma sequência de camadas convolucionais e pooling para reduzir as dimensões da imagem; Parte B - Módulos Inceptions separados por camadas pooling; Parte C - Camada de pooling global e uma Full Conect para classificação [38, p. 224]. Outra técnica para tornar a rede mais eficiente foi incluir uma camada de AvgPool global antes da camada de classificação [35, p. 455]. Nos modelos CNN’s anteriores era comum incluir um flattering para converter os dados em um vetor, perdendo a informação espacial, para ser compatível com as camadas totalmente conectadas que faziam a classificação. Estas últimas camadas acabam sendo as responsáveis pela maior parte dos parâmetros. No modelo VGG-16, por exemplo, as 3 camadas totalmente conectadas geram aproximadamente \\(123.6\\) milhões de parâmetros, quase \\(90\\%\\) dos parâmetros totais [40]. Em vez de adotar o flattering, o GoogLenet utiliza um filtro de média de mesma dimensão do elemento de entrada, retornando a média dos mapas para cada posição do vetor. Como o vetor de saída já tem um tamanho reduzido é necessário incluir apenas uma camada totalmente conectada com as \\(1000\\) classes. Como a camada de média global não precisa de parâmetros, e considerando que retorna um vetor \\(1024\\), são necessários aproximadamente \\(1\\) milhão de parâmetros na camada totalmente conectada, \\(100\\) vezes menos que na VGG [40]. Na última camada, assim como no VGG, associa-se uma ativação Softmax, enquanto nas camadas de convolução é a ReLu. Já foi comentado sobre a primeira parte e a última da rede GoogLenet, a seção intermediária que estudaremos inclui os módulos Inceptions que se tornaram elementos característicos das redes mais modernas intituladas Inceptions. Cada módulo se assemelha aos blocos do VGG, em que estão presentes algumas camadas convolucionais em sequência e na ponta uma camada pooling. No caso do VGG, viu-se que para reduzir o número de hiperparâmetros se fixou o tamanho dos filtros em \\(3\\text{ x }3\\), e o parâmetro variável ficou sendo o número de camadas convolucionais. A ideia dos Inceptions (Figura 7.33) é não se preocupar nem com tamanho dos filtros e nem com o número de camadas no módulo, pois cada módulo consiste em uma combinação de filtros de diferentes tamanhos arranjados de uma forma fixa [38, p. 217]. Figura 7.33: Módulo Incepetion da rede GoogLenet - A parte intermediária da rede GoogLenet é formada por uma sequência de módulos Inceptions separados por camadas pooling. Cada módulo apresenta quatro caminhos para o mesmo dado de entrada, e na saída, os resultados são concatenados [39, p. 274]. A partir da entrada do módulo inception, as cópias do elemento de entrada seguem ao mesmo tempo por quatro caminhos. Ao final destes caminhos não se altera o tamanho da imagem, porém o número de canais é alterado de formas diferentes, sendo a escolha do número de canais de cada camada um hiperparâmetro. Na saída do módulo ocorre uma concatenação de todos estes canais, formando um único elemento de mesma dimensão que na entrada e com número de canais que é soma de todos que resultaram de cada caminho. O primeiro caminho tem apenas uma convolução \\(1\\text{ x }1\\), conhecida como bottleneck, que tem como principal função preservar as dimensões (altura e largura) mas diminuir o número de canais, o que reduz o custo computacional e o número de parâmetros [38, p. 220]. Como esta convolução inclui mais não linearidade com baixo custo, inclui-se também nas camadas de entrada uma convolução \\(1\\text{ x }1\\), contribuindo para uma otimização na primeira parte da rede [35, p. 453]. Esta mesma camada foi acrescentada no início de cada um dos caminhos 2 e 3 para reduzir a complexidade do modelo. Após reduzir o número de camadas se inclui filtros maiores, possibilitando processar as informações em diferentes escalas, sendo que no segundo caminho os filtros são \\(3\\text{ x }3\\) e no quarto caminho \\(5\\text{ x }5\\) [39, p. 274]. Todos os caminhos, até mesmo o quarto que inclui uma camada de MaxPool, apresentam apropriados padding para manter a mesma dimensão dos canais que na entrada. Como o MaxPool não altera o número de canais, é incluído no final do quarto caminho uma convolução \\(1\\text{ x }1\\), reduzindo o volume. Ao concatenar no final todos os canais de cada caminho, o módulo inception segue a hipótese de que a informação visual pode ser processada em várias escalas e que os resultados agregados permitem ao próximo nível extrair várias características de diferentes escalas ao mesmo tempo [38, p. 222]. Na rede GoogLenet da Figura 7.32, vê-se três agrupamentos de módulos inception intercalados por Maxpooling \\(3\\text{ x }3\\), totalizando 9 módulos. O diagrama anterior da GoogLenet é uma das representações mais simplificadas do modelo, pois como é visto na Figura 7.34, a arquitetura original inclui dois classificadores que correm paralelamente com os outros blocos descritos anteriormente, um que se inicia depois do terceiro módulo inception e o outro depois do sexto módulo [35, p. 456]. Cada classificador funciona parecido com a parte final da rede, em que ocorre a classificação [40]. Os classificadores são formados por uma camada AvgPooling, seguida por uma de convolução, duas camadas totalmente conectadas e na saída uma função de ativação Softmax. Existe uma peculiaridade ao treinar as redes mais profundas, pois, na retropropagação dos erros, as taxas reduzem a valores muito próximo de zeros, dificultando a convergência do algoritmo. A técnica adotada pelo GoogLenet para garantir a convergência foi incluir o cálculo do gradiente dos erros destas classificações intermediárias na retropropagação do erro [35, p. 456]. Figura 7.34: Arquitetura da rede GoogLenet com classificadores intermediários - A rede GoogLenet com dois classificadores, um no terceiro módulo inception e o outro no sexto módulo. Estes classificadores intermediários reduzem o efeito de desaparecimento dos gradiente do erro [44]. 7.2.3.5 ResNet A Rede Neural Residual (ResNet) venceu a competição ImageNet em 2015 com uma taxa de erro de \\(3.6\\%\\). A rede desenvolvida por um grupo de pesquisa da Microsoft inclui várias técnicas de otimização e regularização dos modelos anteriores, principalmente da GoogLenet. Enquanto o ponto característico da rede GoogLenet são os módulos inceptions, na ResNet as unidades residuais permitiram treinar redes ainda mais profundas. Entre os principais modelos da ResNet são encontradas redes com 50, 101, 152 camadas de pesos [38, p. 230], mais que o dobro do número da GoogLenet com 22 camadas. Da mesma forma que a GoogLenet, a rede ResNet pode ser divida em três partes, sendo que a primeira e a última parte seguem a arquitetura da GoogLenet, e as camadas intermediárias incluem as unidades residuais. As camadas de entrada que permitem uma redução considerável da dimensão da imagem incluem uma camada de convolução com 64 canais e filtros \\(7\\text{ x }7\\) de \\(\\text{stride}=2\\), seguida por uma camada \\(3\\text{ x }3\\) de MaxPooling com \\(\\text{stride}=2\\). A última parte da rede, a estrutura de classificação, apresenta as mesmas camadas da GoogLenet, a camada Global de Pooling média de dimensão 1024 e apenas uma camada totalmente conectada com 1000 unidades representando as classes. Lembrando que a substituição do processo de flattering pela média global permitiu reduzir a proporção de parâmetros nas últimas camadas. Figura 7.35: Arquitetura da rede ResNet - A estrutura geral da rede ResNet também pode ser dividida em três partes: Parte A - Semelhante a GoogLenet, contém uma camada convolucional e pooling para reduzir as dimensões da imagem; Parte B - Módulos com unidades residuais; Parte C - Camada de pooling global e uma Full Conect para classificação [35, p. 458]. Como descrito no Tópico rede GoogLenet ao acrescentar mais camadas, o número de parâmetros pode aumentar de forma que dificulte o treinamento, não só pelas limitações de recursos computacionais, mas também pela possibilidade de overfitting e demora de convergência do algoritmo. O modelo GoogLenet adotou várias abordagens de otimização e regularização, incluindo camadas de classificação intermediária para garantir que o modelo pudesse convergir, evitando o efeito de desaparecimento dos pesos (vanishing gradients), que em certos momentos tendiam a zero [35, p. 456]. Uma das técnicas do ResNet que colaboraram para acelerar a convergência no treinamento foi a normalização em batch. Este método se tornou referência para vários modelos de CNN, pois até aquela época cada modelo adotava abordagens diferentes no treinamento, como as classes intermediárias do GoogLenet [40]. A normalização é aplicada individualmente por camada, sendo que a normalização dos dados ocorre com base nas estatísticas dos agrupamentos minibatch adotados no treinamento, que foram vistos rapidamente em MLP [39, p. 280]. Nas redes ResNet a normalização é aplicada nos dados no momento de transição entre a camada de convolução e a de ativação. Mesmo conseguindo convergir redes mais profundas, identificou-se nos modelos que ocorria uma degradação da acurácia ao se acrescentar mais camadas [45]. Antigamente se tinha a intuição de que ao se partir de um modelo com número reduzido de camadas para uma rede mais profunda, o erro não deveria aumentar, pois no mínimo a rede mais profunda teria uma parte das suas camadas copiadas da outra rede e a outra parte funcionaria como funções de identidade. Com mais camadas, a otimização de um modelo se torna mais complexa mesmo para problemas facilmente mapeados em redes menores [45]. A ideia na ResNet para lidar com esta complexidade foi forçar que as camadas realizassem o mapeamento da saída incluindo como referência a própria entrada, ou seja, um mapeamento residual [35, p. 457]. No mapeamento comum das camadas na Figura 7.36 se imagina que a partir de uma entrada \\(x\\) se busca com o treinamento estabelecer uma resposta esperada \\(h(x)\\). No caso do mapeamento residual se inclui um desvio do dado de entrada (\\(x\\)) na saída para forçar um modelo \\(h(x) - x\\). Com isso se espera que seja mais fácil otimizar um modelo com referência, o mapeamento residual, e que este consiga rapidamente se ajustar a funções identidade, garantindo que as camadas no mínimo estabeleçam a performance das camadas anteriores [39, p. 288]. Os desvios, conexões residuais, também minimizam os efeitos de desaparecimento dos pesos, pois permitem um fluxo alternativo dos gradientes, contribuindo para a retropropagação do erro [38, p. 231]. Figura 7.36: Mapeamento convencional e o residual da rede ResNet - a - Mapeamento tradicional da entrada e saída das camadas; b - Mapeamento em uma unidade residual da rede ResNet. No mapeamento comum, para uma entrada \\(x\\) se busca estabelecer uma resposta esperada \\(h(x)\\). No modelo residual se inclui um desvio da entrada (\\(x\\)) na saída para forçar uma resposta \\(h(x) - x\\) [35, p. 457] A conexão residual parte da entrada de uma unidade residual em direção a saída da mesma, somando a informação de entrada com a resposta antes da camada de ativação ReLu (Figura 7.37). Cada unidade residual tem duas camadas de convolução \\(3\\text{ x }3\\), com \\(\\text{stride}=1\\) e configuração de padding para manter as dimensões. Entre as duas camadas existe uma camada de normalização antecedendo uma ativação ReLu. Figura 7.37: Unidade residual da rede ResNet - Na parte intermediária da rede ResNet existe uma sequência de módulos com unidades residuais. Cada unidade residual tem duas camadas de convolução intercaladas por um pooling, e as normalizações são aplicadas na saída das convoluções [35, p. 457]. Após uma sequência de unidades residuais de mesma configuração, entre os módulos, é dobrado o número de canais e ao mesmo tempo se reduz pela metade a largura e altura dos canais. Entre as unidades residuais não são utilizadas camadas pooling, assim, para reduzir as dimensões dos canais é utilizado \\(\\text{stride}=2\\) na convolução que encerra um módulo. Desta forma, na última unidade residual dos módulos, a entrada tem dimensão diferente da saída. Para poder somar os valores é necessário corrigir as dimensões da entrada com uma convolução \\(1\\text{ x }1\\) de \\(\\text{stride}=2\\) e de mesmo número de canais da saída da unidade [39, p. 289]. A estrutura dos módulos com as unidades residuais lembram os blocos da rede VGG, em que cada bloco apresenta uma sequência de camadas de convolução com as mesmas dimensões, e sempre filtro \\(3\\text{ x }3\\). De um bloco para o outro no VGG, o número de canais também dobra e a altura e a largura são reduzidas pela metade, neste caso pelo efeito do MaxPooling. O ResNet apresenta 4 módulos, em que o número de unidades residuais varia de modelo a modelo como indicado na Figura 7.38. O ResNet-34, por exemplo, com 34 camadas de pesos contém respectivamente nos quatro módulos, três unidades residuais com saída de 64 canais, quatro unidades com 128 canais, seis unidades com 256 canais e três unidades com 512 canais [45]. Figura 7.38: Arquitetura padrão da ResNet - Existem várias versões da ResNet, sendo que a parte inicial e a final das redes são semelhantes, e o que geralmente muda é quantidade de unidades residuais em cada um dos quatro módulos, na parte intermediária da rede [35, p. 457]. 7.2.4 Aprendizado por transferência Treinar uma rede neural do zero é uma tarefa complicada, pois necessita de uma enorme quantidade de dados e poder computacional [38, p. 242]. Em muitos casos podemos não ter uma quantidade suficiente de algum dos dois, mas isso não impede que criemos nossos modelos. Para isso contamos com uma técnica conhecida como aprendizagem por transferência, que consiste basicamente em utilizar-se modelos já treinados na resolução de nossos problemas. Nesse caso assumimos que muitos dos fatores que explicam o contexto \\(P_1\\) (situação na qual o modelo foi treinado) também podem explicar nossa novo contexto \\(P_2\\) [23, p. 536]. Conseguimos utilizar essa técnica pois mesmo modelos treinados em escopos diferentes acabam aprendendo a detectar características parecidas nas camadas mais iniciais, com maior especificação nas camadas mais profundas. Podemos ver isso na Figura 7.39, onde temos a representação de quatro redes diferentes sendo a primeira voltada a trabalhar com imagem de pessoas, a segunda com carros, a terceira com elefantes e a última com cadeiras. Mesmo esses sendo objetos completamente diferentes, as redes aprendem, nas camadas iniciais a detectar bordas e quinas, que são características compartilhadas por todos os itens. Figura 7.39: Diferentes CNN’s e seus mapas de características em diferentes camadas - Na primeira coluna temos as características aprendidas por uma rede especializada no trabalho com rostos humanos, na segunda coluna com carros, na terceira com elefantes e na quarta com cadeiras [38, p. 253]. Existem três principais maneiras de se realizar a transferência de aprendizado [38, p. 254], sendo que cada uma delas se enquadra melhor em um cenário ou em outro, são elas: Usar uma rede pré-treinada como classificador Usar uma rede pré-treinada como um extrator de características Usar uma rede pré treinada para ajuste fino O método de usar uma rede pré-treinada como classificador é melhor quando nosso problema tem um domínio muito parecido com a rede já treinada que iremos utilizar, sendo que temos apenas que a encaixar em nosso uso, não sendo necessário treinamento adicional. O método usar uma rede pré-treinada como um extrator de características é útil quando estamos resolvendo um problema que tem várias características comuns com uma rede pré treinada mas não de maneira que possamos utilizar como na aproximação anterior. O que fazemos então é “congelar” as camadas da rede que extraem características (ou seja, as camadas de convolução) e substituir a rede totalmente conectada do final, colocando em seu lugar outra rede que classifique conforme nossa necessidade e realizamos seu treinamento. Figura 7.40: Rede VGG-16 - Temos, de cima para baixo, as camadas “congeladas”, as camadas que serão removidas e a camada que será adicionada e treinada [38, p. 257]. Na Figura 7.40, temos um exemplo do livro de Mohamed Elgendy [38], onde se queria realizar a classificação de cachorros e gatos. A Rede VGG-16 foi escolhida pois foi treinada na base de dados ImageNet, que contém muitos exemplos de cachorros e gatos, com isso ela poderia se adaptar bem ao nosso problema. Os dois métodos anteriores são usados quando temos um domínio parecido, já este, o terceiro método, que usa a rede pré-treinada para ajuste fino, enquadra-se em problemas que têm propriedades bem diferentes. Mesmo nestes casos conseguimos utilizar redes pré-treinadas, pelos motivos já ditos anteriormente, que as redes, principalmente em suas camadas iniciais, aprendem a detectar características parecidas, ficando mais específicas conforme sua profundidade [38, p. 259]. No método que usa a rede pré-treinada para ajuste fino, temos algumas possibilidades. Uma delas é “congelar” as primeiras camadas convolucionais e retreinar o resto da rede, o que estaremos fazendo então é um ajuste fino (também conhecido como fine-tune) nas camadas mais profundas da rede, para que consigam se adaptar ao nosso problema atual. E a outra é a de não “congelar” nenhuma camada e retreinar a rede toda, neste caso, mesmo tendo que gastar um tempo maior e ser necessário uma maior quantidade de dados, a rede tende a convergir mais rapidamente a uma solução ótima, em comparação a uma iniciação randômica [38, p. 259]. No livro “Deep Learning for Vision Systems” de Mohamed Elgendy, são apresentadas algumas dicas de como escolher a melhor técnica de transferência de aprendizado para alguns tipos de cenários, sendo elas apresentadas na Tabela 7.3: Tabela 7.3: Diferentes cenários onde podemos utilizar a transferência de aprendizado [38, p. 262]. Cenário Quantidade de dados disponíveis Similaridade entre a rede pré-treinada e a nova base de dados Método 1 Pequena Similar Usar uma rede pré treinada como classificador 2 Grande Similar Ajuste fino na rede totalmente conectada 3 Pequena Muito diferente Ajuste fino em uma parte da rede 4 Grande Muito diferente Ajuste fino em toda a rede 7.2.5 Redes neurais na prática Ao estudar os diferentes modelos de CNN’s percebemos que o principal fator para melhorar a acurácia das redes foi o aumento no número de camadas, tornando as redes mais profundas. Destaca-se que a construção de modelos mais profundos só foi possível pela evolução da performance de hardware, principalmente memória e unidades de processamento, e desenvolvimento de softwares mais específicos, conhecidos como frameworks para Deep Learning [39, p. 36]. O treinamento das redes envolve milhares de operações com elementos multidimensionais, ou seja, arrays n-dimensionais com número arbitrário de eixos, conhecidos como tensores [39, p. 42]. Tensores com apenas uma dimensão correspondem matematicamente aos vetores, enquanto os de duas dimensões são as matrizes. No caso das redes CNN’s, muitas vezes as entradas são imagens coloridas que podem ser interpretadas como tensores de três dimensões: a altura, a largura e o volume da imagem (canais RGB). Dentro da rede ocorre uma série de multiplicações e somas a partir do tensor de entrada, resultando em diferentes tensores tridimensionais. A realização de todas as operações só é possível devido às unidades de processamento das máquinas, que permitem a interação da unidade lógica aritmética (ALU) com a memória. Quando as primeiras redes foram desenvolvidas, o principal recurso de processamento eram as CPU’s, um processador de propósito geral em que a ALU realiza apenas um cálculo por vez. Como possui aplicações mais gerais precisa de acesso constante à memória para leitura de instruções e armazenamento de dados, o que é uma desvantagem em relação ao tempo de processamento, conhecida como “gargalo de von Neumann” [46]. Geralmente para tornar mais rápido o acesso a memória, são integradas tecnologias mais sofisticadas aos caches, porém de tamanho limitado devido ao custo [39, p. 260]. As limitações de armazenamento e processamento relacionados a uma única CPU inviabilizam o treinamento de redes mais profundas. O que permitiu a evolução das redes foi a adaptação de unidades de processamento com aplicações mais específicas. A maioria das redes modernas são implementadas com base em unidades de processamento gráfico (GPU’s). Originalmente estes componentes foram desenvolvidos para aplicações gráficas, principalmente para renderização dos video games [23, p. 439]. Entretanto, grande parte da abordagem de processamento da GPU também se mostrou compatível com os cálculos necessários para treinar as redes neurais. A GPU consegue maior capacidade de processamento do que uma CPU, pois integra várias ALU’s em um único processador, o que permite a realização de milhares de operações simultaneamente [46]. Esta estratégia é ideal para aplicações que se adequam ao processamento paralelo, como multiplicação de matrizes de uma rede neural, que envolvem operações independentes [23, p. 440]. As GPU’s foram construídas para realizar operações mais simples, sem envolver muitas ramificações como geralmente é necessário no fluxo de trabalho da CPU. A maior parte dos cálculos do treinamento das redes são previsíveis, com algoritmos que não necessitam de controles sofisticados [23, p. 440]. Se por um lado as operações das redes não exigem grande complexidade computacional, por outro, demandam grande quantidade de memória. Durante o treinamento é preciso armazenar os parâmetros, os valores de ativação, e os gradientes, ou seja, uma quantidade de dados além do limite da cache associada à CPU [23, p. 440]. Neste sentido, além de permitir processamento paralelo, diminuindo o tempo de treinamento, a GPU também oferece maior quantidade de memória. A utilização da GPU para o treinamento das redes se tornou ainda mais comum após a adaptação para propósitos mais gerais. Um dos principais modelos de GPU, da NVIDIA, suporta uma programação CUDA para o desenvolvimento de códigos arbitrários semelhantes à linguagem C [23, p. 440]. Assim, a GPU pode executar diferentes rotinas que não estejam só associadas a subrotinas de renderização. Para garantir códigos de GPU de alta performance, foram construídos ao longo dos anos diferentes bibliotecas que lidam com computação numérica, principalmente envolvendo convoluções e outras operações com tensores [23, p. 441]. Assim, no desenvolvimento das redes não é necessário saber a programação a nível de CUDA, que é mais complexa e envolve computação paralela e distribuída. Geralmente estas bibliotecas, ou frameworks, têm suporte tanto em GPU quanto CPU. As primeiras gerações de muitos frameworks foram desenvolvidas por meio de parcerias entre Universidades e grandes empresas com interesse em Deep Learning. Algumas das bibliotecas mais comuns, destacadas na Figura 7.41, são o PyTorch e Caffe2 mantidos pelo Facebook, o TensorFlow pela Google, o MXNet pela Amazon e o CNTK pela Microsoft [40]. Figura 7.41: Principais frameworks para Deep Learning - Para facilitar a manipulação de tensores e a utilização de GPU’s foram desenvolvidos diferentes frameworks. A maior parte das primeiras gerações dos frameworks surgiram como parcerias entre universidades e grandes empresa [40]. Mesmo com o aumento do desempenho da GPU em relação a CPU para treinar as redes, em muitos casos, uma única máquina não é suficiente para todo o processamento. Com a possibilidade do paralelismo se tornou comum distribuir a carga de trabalho entre várias máquinas. No entanto, montar uma rede própria de GPU é um investimento alto, e a tecnologia também iria depreciar em poucos anos, pois os modelos de GPU estão evoluindo rapidamente para acompanhar as redes cada vez maiores [47]. Uma alternativa para as redes locais é utilizar servidores de GPU em nuvem oferecidos por provedores como Amazon, Google, Microsoft entre outros. Nos últimos anos aumentou a oferta de serviços na nuvem para treinar CNN’s, sendo que os custos são com base no tipo de tecnologia da GPU disponível, no tempo de processamento e na quantidade de armazenamento [47]. Alguns provedores oferecem alguns serviços gratuitos. As redes podem ser desenvolvidas, diretamente, em plataformas oferecidas pelos próprios provedores, que, geralmente, apresentam uma IDE baseada em Jupyter Notebooks. Cada plataforma apresenta facilidades para determinados frameworks, geralmente para aquelas bibliotecas que também prestam suporte [47]. O GoogleCloud, por exemplo, apresenta várias ferramentas para integrar com o TensorFlow. O GoogleCloud oferece além dos serviços de GPU a alternativa de acesso às TPU’s como recursos de computação em nuvem. A TPU (unidades de processamento de tensor) foi projetada especificamente para trabalhar com redes neurais e funciona como processador de matriz especializado. Como a sequência de cálculos já é previamente conhecida, foram conectados vários multiplicadores e somadores, formando uma matriz de operadores, denominada matriz sistólica [46]. Uma vantagem da TPU em relação a CPU e a GPU é a redução do gargalo de von Neumann, pois os dados são todos carregados de uma única vez da memória, não precisando o acesso durante os cálculos [46]. No tópico sobre backpropagation foi apresentado um exemplo de aplicação de rede MLP para reconhecimento de dígitos em que implementamos na linguagem Python. No código foi desenvolvido manualmente a estrutura da rede, as etapas do treinamento e as funções, utilizando principalmente bibliotecas que lidam com cálculos em Arrays Multidimensionais, como o Numpy. No exemplo MLP, ao manipular os dados como array do numpy não surgiram inconsistências e também não nos deparamos com operações muito complexas, pois até aquele momento as aplicações não envolviam redes muito densas, que exigissem várias otimizações e regularizações. Em modelos de redes maiores, a utilização do algoritmo backpropagation é um desafio maior e muitas vezes pode ser difícil garantir a convergência apenas utilizando as bibliotecas tradicionais de computação numérica [36, p. 153]. Por outro lado, os frameworks que acabamos de apresentar, como o TensorFlow e o Pythorch, foram construídos especificamente para lidar com Deep Learning, podendo, por exemplo, disponibilizar automaticamente várias funções de otimização e regularização. Os frameworks de Deep Learning geralmente apresentam uma estrutura de dados multidimensionais, tratadas como uma classe tensor, que substitui o “ndarray” do numpy [39, p. 44]. Esta estrutura de dados de tensor apresenta algumas vantagens em relação ao tradicional array do numpy que se adequam melhor a manipulação de dados nas redes. O primeiro ponto é que o numpy apresenta por default uma precisão de \\(64 \\text{ bits}\\), enquanto os tensores geralmente usam \\(32 \\text{ bits}\\). Como a precisão de \\(32 \\text{ bits}\\) é suficiente para trabalhar com as redes, ao se reduzir o tamanho dos dados, ocupa-se menos memória e torna as operações mais rápidas [35, p. 474]. Por padrão a manipulação das variáveis são direcionadas para os cálculos na CPU, entretanto, quando existe disponível diferentes dispositivos de CPU e GPU pode ser interessante distribuir os cálculos e armazenamento [39, p. 219]. Diferente do Numpy, os frameworks de Deep Learning possuem suporte para lidar com vários dispositivos ao mesmo tempo [39, p. 44]. Anteriormente destacamos que o propósito da utilização das GPU’s nas redes é acelerar o treinamento, principalmente adotando processamento distribuído e paralelo, porém, se a manipulação dos dados não for adequada, ocorrerá muitas transferências entre dispositivos, assim, o tempo de processamento pode aumentar [39, p. 222]. Geralmente, ao se utilizar os frameworks configurados com as GPU’s e CPU’s, não é necessário se preocupar na forma como ocorrem estas transferências, pois os processamentos são otimizados com base em gráficos computacionais [35, p. 368]. O que tem popularizado ainda mais as CNN’s mesmo fora dos meios acadêmicos é o desenvolvimento de interfaces de programação que tornam mais simples a construção e o treinamento das redes neurais. Keras, um dos mais conhecidos API para Deep Learning em alto nível, foi desenvolvido por François Chollet como um projeto de pesquisa, que foi disponibilizado como software livre em 2015 [35, p. 292]. Foi escrito em Python e funciona como um mecanismo de computação subjacente que roda sob diferentes frameworks de backend, alguns identificados na Figura 7.42, os quais oferecem o suporte de computação numérica, como TensorFlow, Theano, CNTK e MxNet [35, p. 292]. Grande parte do trabalho com as redes neurais pode ser implementado direto com o API de alto nível, incluindo o desenho da arquitetura, as funções, algoritmo de treinamento, otimizadores e módulos de regularização. Para implementações que necessitam de maiores customizações é necessário recorrer aos frameworks de mais baixo nível, como o TensorFlow. O TensorFlow, criado por um grupo de pesquisa da Google e liberado como software livre em 2015, tem a sua própria implementação de keras, o tf.keras [35, p. 368]. O API tf.keras tem como backend apenas o TensorFlow (Figura 7.42), porém apresenta maiores facilidades para integrar com customizações do backend. A inclusão de novos códigos pode ser feita a partir da linguagem Python e automaticamente o API converte para funções do tipo TensorFlow que possuem base no C++ [35, p. 398]. Muitas vezes essas adaptações ocorrem quando se precisa de um controle maior, ou customizar funções, camadas, módulos de regularização, processos de inicialização, ou medidas de avaliação [35, p. 398]. Figura 7.42: Implementações do API Keras - a - Implementação geral do API keras-team que é compatível com diferentes backends, como TensorFlow, Theano, CNTK e MxNet. b - O API tf.keras tem como backend apenas o TensorFlow e comporta aplicações específicas deste framework [35, p. 292] No momento a última versão estável do TensorFlow é a 2.4, sendo que a primeira versão do TensorFlow 2.0 foi lançada em março de 2019 [48]. Enquanto que nas versões 1.0, o módulo simbólico é a parte central [35, p. 396], principalmente pela preferência da abordagem por gráficos estáticos, a partir das versões 2.0 se começou a ter uma programação mais imperativa de forma semelhante ao PyTorch, e que apresenta como default gráficos dinâmicos [39, p. 516]. O módulo estático busca maior otimização do código, sendo que após a definição de todo o processo e construção do gráfico não se tem mais a dependência do código [40]. No formato dinâmico a construção e a execução dos gráficos não podem ser desvinculadas, pois ocorrem simultaneamente. Geralmente, os gráficos dinâmicos são mais fáceis de debugar e apresentam menores inconsistências [40]. 7.3 Redes Neurais Siamesas O primeiro trabalho com redes neurais siamesas em 1994 tinha a proposta de comparar duas assinaturas manuscritas, verificando se as duas eram originais ou se uma delas era falsa [49, p. 75]. Grande parte das redes neurais para classificação com imagens utiliza a estratégia de apresentar o objeto para identificar o que é, enquadrando nas classes definidas no treinamento. As redes siamesas apresentam o problema de uma forma diferente, em vez de descobrir o que foi apresentado, a ideia é comparar duas entradas, verificando se fazem parte da mesma classe ou não. Após a utilização das redes siamesas para reconhecimento de assinaturas surgiram diferentes aplicações para análise de imagens com objetivo de avaliar a similaridade entre elas. Na área da saúde, as redes siamesas já foram testadas por diferentes trabalhos para detecção de doenças a partir da análise de imagens de exames [49, p. 81]. Já foram apresentados trabalhos na área da farmácia para comparação de composições químicas, e na biologia, para avaliar sequências de DNA e classificar cromossomos [49, p. 78]. Existem adaptações destas redes em diferentes setores, como na robótica, no desenvolvimento de software e no sensoriamento remoto [49, p. 82]. Uma das aplicações que mais teve repercussão foi na verificação de faces, como no trabalho de Chopra et al. (2005) [50]. Taigman et al. (2014) apresentaram uma rede para verificação de faces a partir de fotos, como um projeto do Facebook [51]. Sabri e Kurita (2018) trabalharam com reconhecimento de faces e expressões faciais [52]. As redes neurais siamesas também ganharam popularidade no reconhecimento de imagem one-shot, em particular pelo estudo de Koch et al. (2021) [49, p. 81]. Diferente dos modelos tradicionais de redes para classificação que precisam de uma grande quantidade de entradas de treinamento para cada classe, nas redes siamesas one-shot, geralmente é necessário apenas um exemplo para cada classe [53]. O propósito da rede não é reconhecer a entrada a partir de diferentes classes, o que exige muitos dados de treinamento, e sim, comparar a similaridade entre duas imagens. Por exemplo, em um reconhecimento facial dos funcionários de uma empresa, em vez de treinar uma rede com várias fotos de cada pessoa para que aprenda a classificar um por um, pode ser interessante utilizar uma siamesa one-shot que compara uma entrada com as referências verificando se corresponde a algum funcionário. Considerando que os vídeos podem ser tratados como uma sequência de imagens, muitas das técnicas de processamento de imagens com redes siamesas foram bem adaptadas para aplicações com vídeos. Além de aspectos estáticos presentes nas imagens, os vídeos podem oferecer informações de movimento e do estado de objetos em diferentes contextos e momentos [49, p. 83]. Por exemplo, com vídeos é possível não só reconhecer um objeto em uma imagem isolada, como também rastreá-lo ao longo dos quadros de gravações [54]. Estas redes conseguiram reconhecer pessoas até mesmo com base no estilo de caminhada, como mostrou um estudo com imagens de vídeos de segurança [55]. O uso das redes com dados de vídeos tem mostrado várias oportunidades na área de segurança, além do monitoramento de pessoas também existem linhas de pesquisa para o setor de veículos e trânsito. O sistema PROVID produzido por Liu et al. (2017) é um exemplo do uso das redes siamesas para identificação de veículos com base nas placas [56]. Os estudos de redes siamesas para detecção dos limites das estradas e de contornos em vídeos na perspetiva da direção de veículos pode ajudar o desenvolvimento de automóveis autônomos [57]. As possibilidades das redes siamesas se estendem além do processamento de imagens e da visão computacional, já foram apresentados estudos para análise de áudios e no processamento de linguagem natural. Um exemplo de aplicação com áudio foi a rede siamesa de Zhang et al. (2018), desenvolvida para avaliar a similaridade entre sons originais e imitações de vozes para poder diferenciá-las [54]. A área de processamento de linguagem natural se propõe gerar e compreender automaticamente a linguagem humana a partir de dados de comunicação, como em documentos com textos e mensagens. Por exemplo, na proposta de González et al. (2019), as redes siamesas extraem as informações mais importantes de um texto para poder resumi-lo [58]. No trabalho de Das et al. (2016) foram avaliados questões de diferentes websites de perguntas e respostas, como Yahoo Answers e Stack Overflow, para poder detectar as semelhanças entre as mensagens [59]. 7.3.1 Arquitetura Como dito no tópico anterior de Redes Neurais Siamesas, o objetivo de se utilizar uma rede com arquitetura siamesa era comparar diferentes entradas e ter como resultado um valor que representasse sua semelhança ou diferença. O nome “siamesa” por si só já nos dá uma boa ideia de como será sua arquitetura, que consiste basicamente na utilização de duas redes idênticas, como na Figura 7.43, onde temos a rede proposta por Bromley et al. (1993) em seu artigo sobre verificação de assinaturas [60]. Nessa arquitetura temos uma camada de entrada de \\(200 \\text{ x } 8\\) unidades e como saída um bloco de dados de \\(16 \\text{ x } 19\\) unidades, o método utilizado para realizar a comparação entre os vetores de saída das duas redes foi calcular o cosseno do ângulo entre eles. Figura 7.43: Arquitetura utilizada por Bromley et al. (1993) com duas redes recebendo os dados sobre assinaturas após seu pré-processamento. A rede contém uma camada de entrada com \\(200 \\text{ x } 8\\) unidades e uma saída de \\(16 \\text{ x } 19\\) unidades. [60]. Na Figura 7.44 temos um exemplo de outra arquitetura, nesse caso a apresentada por Chopra et al. (2005). Um aspecto importante que devemos notar é que tanto essa rede quanto a anterior tem os mesmo pesos em ambas as suas redes internas. Nesse caso se optou por utilizar a distância euclidiana entre os dois vetores de características resultantes das duas subredes. Figura 7.44: Arquitetura utilizada por Chopra et al. [50]. Essa arquitetura de rede siamesa tem os parâmetros da rede compartilhados entre as duas. \\(X_1\\) e \\(X_2\\) são as duas entradas da rede e \\(E_w\\) é a saída da diferença euclidiana entre as saídas das duas redes (\\(||G_w(X_1)-G_w(X_2)||\\)). Das redes neurais que apresentamos para aplicações no processamento de imagens e visão computacional, como MLP, CNN’s e siamesas, as abordagens de desenvolvimento, treinamento e operação seguem um padrão semelhante ao se pôr em prática. A utilização de frameworks tem facilitado as aplicações em redes neurais, disponibilizando comandos para criar camadas, montagem da arquitetura, funções de regularização e otimização, medidas de avaliação, algoritmos de treinamento, sendo que a maior parte dos recursos é facilmente adaptável para diferentes modelos de redes. Geralmente, a maior parte do trabalho de quem desenvolve essas redes está na obtenção e preparo dos dados de entrada. Dependendo do tipo da rede, a imagem de entrada deve ser disponibilizada como um vetor, como na MLP, ou como um tensor, como na CNN’s, e no caso das siamesas pode ser que a entrada seja duas ou mais imagens ao mesmo tempo. Na maioria das vezes será necessário, dimensionar as imagens, estabelecer as classes, alterar o formato dos dados, e dividir as amostras de treinamento, teste e validação. Cada aplicação pode exigir uma abordagem diferente para que a rede aprenda o problema, o que muitas vezes exige que os dados sejam pré-configurados e organizados de forma que se adequem ao tipo de algoritmo adotado no treinamento. Nas redes estudadas, utiliza-se o método supervisionado, especificamente o backpropagation, ou seja, durante o treinamento devem ser apresentados tanto as entradas do problema como as respostas correspondentes que são previamente conhecidas. Desta forma, o modelo consegue corrigir os parâmetros da rede considerando as diferenças entre as saídas e as respostas esperadas. No algoritmo de backpropagation, esta diferença é tratada por uma função de erro que o treinamento busca minimizar. No tópico backpropagation foi descrita uma das funções de erro que foram utilizadas pelos primeiros modelos de redes neurais, o erro quadrático (MSE). Com o aprimoramento das redes, a substituição por outras funções erros demonstrou melhores resultados no treinamento, como a entropia cruzada ou Cross-Entropy [33]. Dado a expressão da função custo entropia cruzada e de sua derivada: \\[ C = -\\frac{1}{n} \\sum_{x}[y \\ln a + (1 - y) \\ln(1 - a)] \\tag{7.22} \\] \\[ \\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_{x} x_j (\\sigma(z) - y) \\tag{7.23} \\] se consegue avaliar que a derivada aumenta com o tamanho do erro (\\(\\sigma(z) - y\\)). Considerando este comportamento e que o algoritmo de treinamento fica mais rápido quanto maior for a derivada da função custo, o método da entropia cruzada tende a demonstrar menores problemas com a desaceleração do aprendizado se comparado ao erro quadrático [33]. No erro quadrático, como a derivada da função custo depende diretamente da derivada da função de ativação, o treinamento poderá ficar estagnado nos pontos em que a ativação for próximo dos limites, particularmente se as funções de ativações forem sigmóides [33]. Geralmente, as funções erro quadrático e entropia cruzada são aplicadas em modelos que se pretende que a saída corresponda a um vetor de probabilidades em relação às classes. Desta forma, é possível associar cada valor a uma classe, ou seja, dado a entrada se determina qual a saída mais provável dentro dos exemplos estabelecidos no treinamento. Outras funções de custo mais comuns nas redes siamesas são as por técnica de ranqueamento, como a perda contrastiva e as funções triplets. No ranqueamento, a saída da rede é estabelecida pelas distâncias entre os dados de entrada de acordo com uma distribuição dos exemplos apresentados no treinamento [61]. Neste caso, as entradas que pertencem a uma mesma classe tendem a ficar mais próximas, ganhando uma posição de maior prioridade que elementos com menor semelhança, os quais vão se afastando. O primeiro modelo de rede siamesa, que comparava as assinaturas [60], utilizou a função erro contrastiva [62]: \\[ L(W, Y, X_1, X_2) = (1 - Y)\\frac{1}{2}(E_w)^2 + (Y) \\frac{1}{2} {max(0,m - E_w)}^2 \\tag{7.24} \\] em que são apresentadas pelo menos duas entradas na rede com propósito de comparar (ou contrastar) e determinar a similaridade. Quando o par de entrada é semelhante (\\(X_1, X_2\\)) o parâmetro \\(Y\\) recebe valor \\(0\\), e quando são de classes diferentes (\\(X_1, X’_2\\)), \\(Y = 1\\). Os pesos da rede (\\(W\\)) são corrigidos de forma que a distância (\\(E_w\\)) entre dados semelhantes fique menor e os mais diferentes se afastem [62]. Para isto é estabelecido uma margem (\\(m\\)) de ajuste, a qual garante uma distância mínima separando o que pertence ou não a um mesmo grupo, \\(E_W (X_1, X_2) + m &lt; E_W (X_1, X’_ 2 )\\). A função de erro Triplet tem demonstrado bons desempenhos em alguns estudos com redes siamesas. Diferente da função contrastiva, na rede Triplet se utiliza três dados como entrada no treinamento, ou seja, uma siamesa com três sub-redes idênticas em vez de duas. Neste método, compara-se ao mesmo tempo o valor âncora (\\(x^a\\)), ou valor de referência, com um padrão semelhante (caso positivo \\(x^p\\)), e com um padrão diferente (caso negativo \\(x^n\\)) de acordo com a função erro [63]: \\[ C = \\sum_{i}\\left[||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \\alpha\\right] \\tag{7.25} \\] Considerando que as funções \\(f(x) \\in R\\) podem ser interpretadas como a aplicação de cada sub-rede, o algoritmo de treinamento deve buscar ordenar as entradas com base na similaridade relativa. Assim, a distância entre a âncora e o padrão positivo deve ser menor do que em relação ao caso negativo, como demonstrado na Figura 7.45. [63]. Da mesma forma que no método contrastivo é aplicada uma margem (\\(\\alpha\\)) de ajuste. Figura 7.45: Comportamento da função erro Triplet. A função Triplet reduz a distância entre os exemplos similares, ou seja, entre a âncora e o padrão positivo, e aumenta a distância entre o padrão diferente (caso negativo) e a âncora [63]. Refêrencias "],["separacaoFundo.html", "Capítulo 8 Separação Plano de Fundo 8.1 Fixo 8.2 Média Temporal 8.3 Mediana Temporal 8.4 Exemplos comparativos da Média Temporal, Média Espaço-Temporal e Mediana Temporal", " Capítulo 8 Separação Plano de Fundo A visão computacional também é usada para lidar com monitoramento de objetos, auxiliando atividades humanas como vigilância pela procura de eventualidades, como acidentes e crimes [64, p. 664]. E a ideia da separação do background é remover a paisagem para observar os objetos-alvo em movimento (foreground), em por menores, seria a remoção de objetos estáticos (como edifícios) ou que não são estáticos e compõe o plano de fundo (como um rio, as folhas de uma vegetação, movimentação das nuvens) - sim, é complexo lidar com essa movimentação de background. O foreground é o resultado da limiarização da diferença entre os pixels do frame atual em relação aos pixels do background estimado. Caso não saiba o que é limiarização, contém esse conteúdo em seções anteriores Segmentação no tópico Limiarização 6.6. 8.1 Fixo A maneira mais simples de se modelar um background é capturando um frame quando não há alvos na cena. Entretanto, isso seria legal para um ambiente controlado, não a um externo, pois como saber que ali não há alvos e de fato é o verdadeiro background e como seria resolvido os problemas de variações de iluminação e movimentos de vegetações [64, p. 669]. 8.2 Média Temporal Uma sugestão para resolver as dificuldades apresentadas no método fixo é fazer a média a partir de muitos frames sobre um recente período, tendo ou não alvos presentes e, se os alvos forem raros, obtém-se uma boa aproximação de um background ideal. Entretanto, haverá alvos que não serão eliminados, produzindo fantasmas - detecção de movimento onde não há alvo [64, p. 669]. Essa estratégia é chamada de média temporal ou temporal averaging. Segue exemplo de cálculo da modelagem para 5 frames (8.1). \\[ {TP}_{x,y} = \\frac{P_{1_{x,y}} + P_{2_{x,y}} + P_{3_{x,y}} + P_{4_{x,y}} + P_{5_{x,y}}}{5} \\tag{8.1} \\] Outra variação que pode ser usada para uma melhor modelagem, reduzindo detecção de fantasmas, é a de aplicar um filtro de média convencional 5x5, espacial, em cada frame antes da média temporal, cálculo esse denominado média espaço-temporal ou spatiotemporal averaging [19, p. 435], como demonstrado na equação (8.2); as Figuras 8.2 e 8.3 demonstra a aplicação com e sem suavização em um ambiente controlado e outdoor. A diferença entre os backgrounds modelados com e sem suavização, a Figura 8.2 (F) e (G), pode ser mínima aos nossos olhos, mas, quando aplicados a diferença é notável, conforme ilustram a detecção de movimento nas Figuras 8.2 (I) e (J), sem e com suavização respectivamente. \\[ {STP}_{x,y} = media5(P_{1_{x,y}}) + media5(P_{2_{x,y}}) + media5(P_{3_{x,y}}) + media5(P_{4_{x,y}}) + media5(P_{5_{x,y}}) \\tag{8.2} \\] 8.3 Mediana Temporal A Mediana Temporal consegue um melhor resultado do que o de Média Temporal, pois os outliers, que podem produzir fantasmas na cena, não influenciam no cálculo [19, p. 436]. Segue exemplo de cálculo dessa modelagem para 5 frames, (8.3). \\[ MP_{x,y} = mediana(P_{1_{x,y}}, P_{2_{x,y}}, P_{3_{x,y}}, P_{4_{x,y}}, P_{5_{x,y}}) \\tag{8.3} \\] Apesar de ser capaz de produzir melhores resultados, nada é perfeito. Observe as Figuras 8.1 (A) e (B), o ônibus que antes estava em movimento vai desaparecendo do foreground à medida que novos frames são capturados e ele se mantém parado, pois, progressivamente, o objeto ônibus vai pertencendo ao background. Porém, ao se mover novamente, ele é identificado como um alvo, mas a região em que ele estava parado ainda detecta um fantasma, por causa da diferença naquela região ainda ser praticamente equivalente, pois no novo frame não há mais ônibus ali, mas, no background, há. Conforme observa-se nas Figuras 8.1 (C) e (D). Note que à medida que novos frames são captados esse fantasma vai sumindo. Figura 8.1: Detecção de movimento através da modelagem de background a partir da Mediana Temporal [64, p. 671]. 8.4 Exemplos comparativos da Média Temporal, Média Espaço-Temporal e Mediana Temporal Os exemplos a seguir são complementares a teoria dos tópicos anteriores de modelagem de plano de fundo, a fim de mostrar os resultados dos métodos estudados aplicados em um ambiente controlado e outdoor. Desde os frames capturados, o background modelado e o foreground produzido pela limiarização da diferença entre o primeiro frame, Figura 8.2 (A), e os respectivos planos de fundo modelados. Figura 8.2: As imagens A, B, …, E são os frames capturados por uma câmera em um ambiente interno. As imagens F, G e H são os backgrounds estimados. E as imagens I, J e K são os foregrounds produzidos, as detecções de movimento estimadas. Adaptado de [19, p. 435] e [19, p. 436]. Note que apareceram pés fantasmas resultado da interação da luz com o movimento dos pés e sua sombra, mas ao ser aplicado a suavização esse problema é diminuído. Além disso, parte da camiseta da pessoa não foi detectada, isso pode tentar ser corrigido por ajustes no limiar de limiarização [19, p. 436]. A Figura 8.3 é análoga a anterior, só que em um ambiente externo. Figura 8.3: As imagens A, B, …, E são os frames capturados por uma câmera em um ambiente externo. As imagens F, G e H são os backgrounds estimados. E as imagens I, J e K são os foregrounds produzidos, as detecções de movimento estimadas [19, p. 437]. Como pôde ser observado, a modelagem de um background é bem mais difícil quando é em um ambiente externo, dado os intempéres e variações de iluminação. Uma tentativa de melhor modelagem é tentar diferentes limiares e aumentar a quantidade de frames a fim de uma melhor modelagem, mas pode ser que não seja o suficiente. Um dos métodos mais utilizados para separação de plano de fundo é o Mistura de Gaussianas, também chamado de Mistura de Distribuições de Probabilidade, mas deixa-se a interesse do leitor. Refêrencias "],["espaco3D.html", "Capítulo 9 Espaço 3D 9.1 Geometria projetiva 9.2 Homografia 9.3 Transformações de câmera 9.4 Distorção das lentes 9.5 Calibração", " Capítulo 9 Espaço 3D 9.1 Geometria projetiva Em alguns trabalhos realizados na visão computacional, a geometria euclidiana se torna insuficiente, pois as capturas de imagens com câmeras geram certos problemas, como linhas paralelas que se cruzam (como na Figura 9.1) e ângulos que não são preservados, fatos esses que ocorrem por estarmos projetando o mundo 3D em uma plano 2D [65, p. 321]. Nesses casos usamos outro tipo de geometria conhecida como geometria projetiva. Figura 9.1: Linhas paralelas se cruzam - Como podemos ver nessa imagem de um trilho de trem, duas linhas que no mundo real são paralelas se cruzam no horizonte por causa da perspectiva [66]. Uma das vantagens de se utilizar a geometria projetiva é o fato que conseguimos representar transformação por simples multiplicação de matrizes, além de que as projeções nos provêem a noção de perspectiva, ou seja, objetos mais distantes têm um tamanho reduzido em relação a objetos mais próximos. O espaço projetivo é definido por coordenadas homogêneas assim como o espaço Euclidiano é definido pelas coordenadas Cartesianas. Utilizamos esse espaço e sua geometria pois é a melhor maneira de se representar a relação entre uma imagem e o mundo físico, ou seja, a relação entre as coordenadas da câmera com as coordenadas do mundo real [19, p. 484]. 9.1.1 Coordenadas homogêneas As coordenadas homogêneas são a maneira como trabalhamos na geometria projetiva, elas diferem das coordenadas cartesianas por apresentarem uma componente a mais, ou seja, um ponto \\((X, Y)^T \\in \\Re^2\\) é representado, em coordenadas homogêneas, por \\((X_1, X_2, X_3)^T\\). A conversão entre os dois sistemas também é muito simples, um ponto \\(X_c = (x, y)^T\\) no sistema cartesiano é convertido a homogêneo da seguinte maneira: \\[ X_H = (wx, wy, w)^T \\tag{9.1} \\] e convertido ao cartesiano novamente por: \\[ x_c = \\frac{wx}{w} \\text{ e } y_c =\\frac{wy}{w} \\tag{9.2} \\] Assim, quando \\(w = 1\\) nossas coordenadas são as mesmas do sistema cartesiano, como pode ser visto na Figura 9.2, onde \\(w\\) é representado por \\(Z\\). Os pontos que estão no formato \\(X_H = (x, y, 0)^T\\) são conhecidos como pontos ideais, e são os pontos onde as linhas se cruzam no infinito [67, p. 36]. Figura 9.2: Plano euclidiano e espaço projetivo - Nesta representação podemos ver um ponto do espaço euclidiano sendo projetado no espaço projetivo. [65, p. 605]. 9.2 Homografia As homografias (ou transformações perspectivas) são as transformações realizadas no espaço projetivo. Com elas podemos relacionar uma imagem de uma plano a outra imagem desse mesmo plano obtida de uma outra posição. Podemos observar uma representação disso na Figura 9.3, onde temos uma plano do objeto também o plano de duas imagens. O ponto \\(\\tilde{x_0}\\) representa o ponto do plano do objeto na imagem 1 e o ponto \\(\\tilde{x_1}\\) representa esse mesmo ponto na imagem 2 que é a vista do mesmo ponto a partir de outra posição. Relacionando esses dois pontos temos a homografia \\(H_{10}\\), que pode ser representada por: \\[ \\tilde{x_0} = H_{10} \\tilde{x_1} \\tag{9.3} \\] Onde \\(x_0\\) e \\(x_1\\) são vetores homogêneos e H uma matriz 3x3: \\[ \\begin{bmatrix}x_0 &amp; y_0 &amp; 1_0\\end{bmatrix}^T = \\begin{bmatrix}h_{11} &amp; h_{12} &amp; h_{13} &amp; h_{21} &amp; h_{22} &amp; h_{23} &amp; h_{31} &amp; h_{32} &amp; h_{33}\\end{bmatrix} \\begin{bmatrix}x_1 &amp; y_1 &amp; 1_1\\end{bmatrix}^T \\tag{9.4} \\] Figura 9.3: Geometria de uma homografia - Temos nessa figura a representação de uma homografia que relaciona pontos entre dois planos de imagem de um mesmo plano no mundo. [67, p. 60]. E a partir da equação anterior podemos deduzir que: \\[ x_0 = \\frac{h_{11}x_1 + h_{12}y_1 + h_{13}}{h_{31}x_1 + h_{32}y_1 + h_{33}} \\text{ e } y_0 = \\frac{h_{21}x_1 + h_{22}y_1 + h_{23}}{h_{31}x_1 + h_{32}y_1 + h_{33}} \\tag{9.5} \\] então: \\[ \\begin{split} &amp;h_{31}x_1x_0 + h_{32}y_1x_0 + h_{33}x_0 = h_{11}x_1 + h_{12}y_1 + h_{13}\\\\ &amp;h_{31}x_1y_0 + h_{32}y_1y_0 + h_{33}x_0 = h_{21}x_1 + h_{22}y_1 + h_{23} \\end{split} \\tag{9.6} \\] que pode ser rearranjado em: \\[ \\begin{split} &amp;h_{31}x_1x_0 + h_{32}y_1x_0 + h_{33}x_0 - h_{11}x_1 - h_{12}y_1 + h_{13} = 0\\\\ &amp;h_{31}x_1x_0 + h_{32}y_1x_0 + h_{33}x_0 - h_{21}x_1 + h_{22}y_1 + h_{23} = 0 \\end{split} \\tag{9.7} \\] e escrito em forma matricial: \\[ \\begin{split} &amp;\\begin{bmatrix}-x_1 &amp; -y_1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_1x_0 &amp; y_1x_0 &amp; x_0\\end{bmatrix}h^T = 0 \\\\ &amp;\\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; -x_1 &amp; -y_1 &amp; -1 &amp; x_1y_0 &amp; y_1y_0 &amp; y_0\\end{bmatrix}h^T = 0 \\end{split} \\tag{9.8} \\] onde \\(H = \\begin{bmatrix}h_{11} &amp; h_{12} &amp; h_{13} &amp; h_{21} &amp; h_{22} &amp; h_{23} &amp; h_{31} &amp; h_{32} &amp; h_{33}\\end{bmatrix}\\) Para que consigamos resolver esse sistema, e achar a homografia H, precisamos de no mínimo 4 pontos correspondentes em cada uma das imagens [68, p. 88], o que nos leva ao sistema de equações: \\[ \\begin{bmatrix} -x_{11} &amp; -y_{11} &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_{11}x_{01} &amp; y_{11}x_{01} &amp; x_{01}\\\\ 0 &amp; 0 &amp; 0 &amp; -x_{11} &amp; -y_{11} &amp; -1 &amp; x_{11}y_{01} &amp; y_{11}y_{01} &amp; y_{01}\\\\ -x_{12} &amp; -y_{12} &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_{12}x_{02} &amp; y_{12}x_{02} &amp; x_{02}\\\\ 0 &amp; 0 &amp; 0 &amp; -x_{12} &amp; -y_{12} &amp; -1 &amp; x_{12}y_{02} &amp; y_{12}y_{02} &amp; y_{02}\\\\ -x_{13} &amp; -y_{13} &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_{13}x_{03} &amp; y_{13}x_{03} &amp; x_{03}\\\\ 0 &amp; 0 &amp; 0 &amp; -x_{13} &amp; -y_{13} &amp; -1 &amp; x_{13}y_{03} &amp; y_{13}y_{03} &amp; y_{03}\\\\ -x_{14} &amp; -y_{14} &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_{14}x_{04} &amp; y_{14}x_{04} &amp; x_{04}\\\\ 0 &amp; 0 &amp; 0 &amp; -x_{14} &amp; -y_{14} &amp; -1 &amp; x_{14}y_{04} &amp; y_{14}y_{04} &amp; y_{04} \\end{bmatrix} = \\begin{bmatrix} h_{11}\\\\ h_{12}\\\\ h_{13}\\\\ h_{21}\\\\ h_{22}\\\\ h_{23}\\\\ h_{31}\\\\ h_{32}\\\\ h_{33} \\end{bmatrix} = 0 \\tag{9.9} \\] Resolvendo o sistema de equações anterior, encontramos a matriz de homografia H entre as duas imagens. Algo importante a se notar é que o tamanho do sistema de equações, e por conseguinte o tamanho da matriz, não é fixo, pois cada ponto adicionará mais duas linhas (9.8) a matriz (9.9). 9.2.1 Transformação Linear Direta (DLT) A transformação Linear Direta, comumente chamada pela sigla em inglês DLT(Direct Linear Transformation) é uma das maneiras de se calcular a estimação da matriz H. Esse método funciona da seguinte maneira [68, p. 91]: Dado \\(n &gt;= 4\\) pontos correspondentes entre dois planos 2d(imagens). Para cada correspondência, crie uma matriz como a da equação (9.8) na forma matricial: \\[ \\begin{split} &amp;\\begin{bmatrix}-x_1 &amp; -y_1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_1x_0 &amp; y_1x_0 &amp; x_0\\end{bmatrix}\\\\ &amp;\\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; -x_1 &amp; -y_1 &amp; -1 &amp; x_1y_0 &amp; y_1y_0 &amp; y_0\\end{bmatrix} \\end{split} \\tag{9.10} \\] Junte todas as matrizes criadas em uma única matriz A de tamanho \\(2n x 9\\). Obtenha a decomposição em valores singulares(Singular Value Decomposition - SVD) da matriz anterior, \\(A=U\\sum{V^T}\\). O vetor singular unitário que corresponde ao menor valor singular é então a solução para h, ou seja, seja D diagonal com entradas positivas, ordenado em ordem decrescente na diagonal, então h será a última coluna de V. Fazendo um reshape do vetor h, temos a matriz de homografia H. 9.3 Transformações de câmera A formação de imagem a partir de uma câmera digital pode ser compreendida por uma sequência de transformações entre sistemas de coordenadas, que fazem a correspondência entres os pontos “P” do espaço tridimensional para os pontos “p” no plano da imagem. Para estudar estas transformações será utilizado como referência os quatros sistemas de coordenadas na Figura 9.4, apresentados por Carvalho et al. (2005) [69, p. 22]: Figura 9.4: Sistemas de coordenadas. Relação entre os sistemas de coordenadas que determina a transformação da posição do objeto de interesse no SCM para a projeção na imagem (SCI). Outros sistemas intermediários são o Sistema de coordenadas da câmera (SCC) e o Sistema de coordenadas em pixel (SCP). [69, p. 22]. Sistema de coordenadas do mundo (SCM): é tridimensional com origem no ponto “O” e as posições são indicadas por (X, Y, Z). Sistema de coordenadas da câmera (SCC): também é tridimensional, tem origem no centro óptico da câmera “Õ” e as coordenadas são referenciadas por (X’, Y’, Z’). O sistema é definido em relação ao plano de projeção \\(\\pi\\), sendo que os eixos X’ e Y’ devem ser paralelos às bordas da imagem no plano, e o eixo Z’ é perpendicular de forma que entre o centro óptico (Õ) e a intersecção deste eixo com o plano exista uma distância f, que é a distância focal da câmera. Sistema de coordenadas de imagem (SCI): um sistema bidimensional sobre o plano de projeção \\(\\pi\\), com origem no ponto “C” e com coordenadas (x, y). A origem é definida pelo ponto que marca a projeção ortogonal do centro óptico da câmera (Õ) sobre o plano da imagem. Sistema de coordenadas em pixel (SCP): também é um sistema bidimensional, com origem no canto superior (ou inferior) esquerdo da imagem e com posições representadas por (u, v). Identifica os pontos da imagem com relação a grade de pixels. As etapas de transformações entre estes sistemas estão resumidas no esquema da Figura 9.5, em que ocorrem sucessivas mudanças de coordenadas dos pontos de interesse, inicialmente referenciados no sistema SCM como (X,Y, Z), passando por todos os sistemas até o nível de pixel (u, v). Entre as transformações realizadas, estão a translação, rotação, escala e projeções que podem ser descritas como combinações de operações matriciais na sua forma homogênea [70, p. 481], seguindo as abordagens descritas no tópico coordenadas homogêneas. Figura 9.5: Mudanças de coordenadas. Etapas de transformação do referencial do objeto de interesse (SCM) para o sistema de coordenadas de pixel (SCP) no plano de projeção. São apresentados quatro sistemas de coordenadas - SCM, SCC, SCI, SCP - cada um com a identificação da origem e dos pontos, por exemplo, no SCM a origem é indicada por “O” e o ponto (X, Y, Z). A passagem do referencial real (SCM) para o da câmera (SCC) pode ser descrita como: \\[ \\tilde{P} = RP + T \\tag{9.11} \\] em que ocorre a mudança de coordenada do ponto P (X, Y, Z) em SCM para a correspondência \\(\\tilde{P}\\) (X’, Y’, Z’) em SCC. O vetor T é a posição da origem absoluta (O) no sistema SCC. A matriz R fornece a posição dos versores dos eixos do SCM com relação ao SCC [69, p. 23]. Em coordenadas homogêneas esta expressão é escrita: \\[ \\begin{bmatrix} \\tilde{X}\\\\ \\tilde{Y}\\\\ \\tilde{Z}\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} R &amp; T\\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ 1 \\end{bmatrix} \\tag{9.12} \\] Como os dois referenciais são ortogonais, R é uma matriz ortogonal (\\(RR^t = I\\)) e é possível determiná-la apenas com 3 parâmetros em vez de 9, que caracterizam a rotação que leva de um eixo a outro, geralmente tratados como componentes do vetor Rodriguez [69, p. 24]. Assim, para cada posição da câmera existem 6 parâmetros que descrevem o seu estado extrínseco, 3 referentes à matriz R e 3 do vetor T. A mudança das coordenadas em relação a câmera (SCC) para o sistema da imagem (SCI) pode ser aproximada como uma projeção perspectiva em uma câmera pinhole com distância focal f [69, p. 24]: \\[ \\begin{bmatrix} x\\\\ y\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; f &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\tilde{X}\\\\ \\tilde{Y}\\\\ \\tilde{Z}\\\\ 1 \\end{bmatrix} \\tag{9.13} \\] Como esta transformação projetiva não é inversível, um ponto da imagem pode ser associado com uma infinidade de pontos no espaço, ou seja, o ponto (x, y) da imagem é tratado como a projeção de todos os pontos do espaço que atendem a forma \\(\\lambda(x, y, f)\\), em que \\(\\lambda != 0\\) [69, p. 24]. O último nível de transformação que consideramos é do plano de formação da imagem (SCI) para a matriz retangular dos pixels (SCP): \\[ \\begin{bmatrix} u\\\\ v\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} s_x &amp; \\tau &amp; u_c\\\\ 0 &amp; s_y &amp; v_c\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\tilde{X}\\\\ \\tilde{Y}\\\\ \\tilde{Z}\\\\ 1 \\end{bmatrix} \\tag{9.14} \\] Os elementos \\(s_x\\) e \\(s_y\\) são a quantidade de pixels por unidade de comprimento, nas direções horizontal e vertical, respectivamente. Na maior parte das câmeras, o espaçamento entre linhas e colunas da matriz são iguais, ou seja, os pixels são quadrados \\(s_x = s_y\\). Os valores \\(u_c\\) e \\(v_c\\) descrevem a posição, em pixels, da projeção ortogonal do centro óptico da câmera (O’) sobre o plano de projeção, indicado como a origem C do SCI. Geralmente, o ponto C está no centro da imagem e a origem do SCP está no canto, assim, \\(u_c\\) e \\(v_c\\) são iguais a metade das dimensões da imagem. O elemento \\(\\tau\\) reflete o alinhamento das colunas de pixels com as linhas, mais especificamente, é a tangente do ângulo que as colunas formam com a perpendicular às linhas. Quando as colunas são perpendiculares às linhas, \\(\\tau = 0\\). Os cinco parâmetros (\\(s_x, s_y, u_c, v_c, \\tau\\)) mais a distância focal (f) totalizam os seis parâmetros intrínsecos da câmera, que descrevem o seu funcionamento interno, enquanto que os seis parâmetros extrínsecos (elementos da matriz R e do vetor T) estabelecem a posição e a orientação. A dependência destas variáveis nas transformações indica que para determinadas aplicações na visão computacional, como no rastreamento e na reconstrução 3D, é necessário modelar estes valores [70, p. 479] . O processo da determinação dos parâmetros extrínsecos e intrínsecos da câmera é denominado calibração [65, p. 331]. As equações anteriores podem ser combinadas como multiplicações de matrizes para estabelecer uma transformação de coordenada de um ponto de interesse (SCM) à sua projeção no referencial da matriz de pixels da câmera (SCP): \\[ [p] = \\begin{bmatrix} fs_x &amp; f\\tau &amp; u_c\\\\ 0 &amp; fs_y &amp; v_c\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} R &amp; T \\end{bmatrix} \\begin{bmatrix} P \\end{bmatrix} \\approx K \\begin{bmatrix} R &amp; T \\end{bmatrix} \\begin{bmatrix} P \\end{bmatrix} \\tag{9.15} \\] em que “P” é a coordenada em SCM e “p” em SCP. A matriz K de calibração define os parâmetros intrínsecos da câmera, enquanto [R T] representa os extrínsecos [71]. Na prática os valores de f, \\(s_x\\) e \\(s_y\\) não são determinados individualmente por esta operação, pois estão combinados como produtos (\\(fs_x\\) e \\(fs_y\\)) [69, p. 26], assim, a matriz de calibração pode ser escrita como: \\[ K = \\begin{bmatrix} f_x &amp; c &amp; u_c\\\\ 0 &amp; f_y &amp; v_c\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\tag{9.16} \\] 9.4 Distorção das lentes Os parâmetros intrínsecos foram modelados com base em uma aproximação do comportamento das câmeras do tipo pinhole, entretanto, as lentes das câmeras reais podem apresentar distorções. Geralmente as distorções mais proeminentes nas imagens são do tipo geométrica, em que os pontos no plano da imagem são deslocados de onde deveriam ser projetados [65, p. 331]. A maior parte dos modelos empíricos de distorção consideram as distorções geométricas radiais e tangenciais [72]. Figura 9.6: Distorção radial. Este tipo de distorção faz com que as linhas retas pareçam curvadas, e o efeito é maior nas bordas. (a) Imagem com distorção radial. (b) Imagem sem distorção. [71, p. 3] A distorção radial faz com que os pontos da imagem sejam transladados ao longo das linhas radiais a partir da origem do plano de projeção (C), fazendo com que as linhas retas pareçam curvadas como na Figura 9.6. A distorção radial se torna maior quanto mais longe os pontos estão do centro da imagem [72]. A imagem que deveria se formar no ponto (u, v) do plano de projeção é deslocada para o ponto (u’, v’) devido a distorção que pode ser avaliada como [69, p. 27]: \\[ (u&#39;, v&#39;) = (1 + d) (u, v) \\tag{9.17} \\] em que a distorção (d) depende da distância do ponto da imagem ao ponto de origem (C) descrita como: \\[ r = \\sqrt{(x)^2 + (y )^2} \\tag{9.18} \\] A relação entre “r” e “d” pode ser modelada por uma função polinomial [69, p. 27]: \\[ d = k_1r^2 +k_2r^4 + k_3r^6 + … \\tag{9.19} \\] sendo \\(k_n\\) os coeficientes de distorção radial. A distorção tangencial acontece pelo desalinhamento da lente com o plano da imagem, que pode não estar perfeitamente paralela, fazendo com que algumas áreas da imagem pareçam mais próximas do que o esperado [72]. Considerando as distorções radiais e tangenciais o ponto na imagem é posicionado em: \\[ u&#39; = u + \\delta_u \\text{, } v&#39; = v + \\delta_v \\tag{9.20} \\] em que os deslocamentos (\\(\\delta_u\\), \\(\\delta_v\\)) são modelados como [65, p. 330]: \\[ \\begin{bmatrix} \\delta_u\\\\ \\delta_v \\end{bmatrix} = \\begin{bmatrix} u(k_1r^2 + k_2r^4 + k_3r^6 + ...)\\\\ v(k_1r^2 + k_2r^4 + k_3r^6 + ...) \\end{bmatrix} + \\begin{bmatrix} 2p_1uv + p_2(r^2 + 2u^2)\\\\ p_1(r^2 + 2v^2) + 2p_2uv \\end{bmatrix} \\tag{9.21} \\] Geralmente, apenas 3 coeficientes (\\(k_1, k_2, k_3\\)) de distorção radial são suficientes para descrever a distorção radial, que juntamente com os 2 coeficientes tangenciais (\\(p_1, p_2\\)) são considerados como parâmetros intrínsecos adicionais, os parâmetros de distorção (\\(k_1, k_2, k_3, p_1, p_2\\)) [65, p. 330]. 9.5 Calibração Determinar os parâmetros intrínsecos e extrínsecos com base na equação (p = K [R T] [P]) envolve a resolução de um problema de otimização não linear. Geralmente, o problema é formulado como uma minimização de erro com base na diferença entre posições conhecidas da imagem (p) com os valores estimados (p’) utilizando como referência os parâmetros R, T, K e \\(\\delta\\) da solução [69, p. 28]. Assim, os parâmetros intrínsecos são estabelecidos de forma que os pontos da imagem (p’) obtidos por uma câmera em calibração sejam os mais próximos dos pontos (p) conhecidos da imagem [69, p. 28]: \\[ \\sum_{i=0}^n {||p&#39;_i - p_i||^2} \\tag{9.22} \\] Na calibração devem ser utilizadas imagens de amostra de um padrão, como de um tabuleiro de xadrez (Figura 9.7), em que seja possível identificar automaticamente na imagem pontos cujas coordenadas sejam conhecidas [69, p. 29]. No caso do tabuleiro de xadrez, os pontos de referência são as interseções dos quadrados que possibilitam identificar tanto as coordenadas na imagem (p) quanto no real (P) [71, p. 12]. Para exemplificar as etapas de calibração utilizaremos como guia o material e o código disponibilizados na documentação do OpenCv [72]. As etapas de calibração que descreveremos seguem o método Zhang, que usa um padrão bidimensional, mas que é posicionado em diferentes posições do espaço [69, p. 39]. Na prática, as imagens são obtidas a partir de uma câmera estática e o tabuleiro é colocado em diferentes posições e orientações, sendo necessário pelo menos 10 imagens [72]. Em cada imagem são utilizados os mesmos pontos de referência, por exemplo, pela Figura 9.7 se identifica 9x6 pontos de interseção no tabuleiro, assim, a entrada do algoritmo segue a forma tamanho_tabuleiro = (9,6) . Figura 9.7: Tabuleiro de Xadrez. Uma das imagens utilizadas como padrão para realizar a calibração da câmera. Os pontos de contato entre os quadrados de mesma cor são utilizados como posições de referência para o algoritmo de calibração. [72] Uma abordagem que simplifica a aplicação dos algoritmos é considerar que o tabuleiro se mantém estacionário no plano XY e que é a câmera que se move, desta forma, não é preciso se preocupar com as coordenadas do terceiro eixo (Z= 0) [72]. Os valores de X e Y são definidos como a localização dos pontos no tabuleiro, e considerando o formato em grade, são identificados como (0,0), (1,0), (2,0),…, (8,5). No código, os pontos no referencial do espaço real são identificados como pontos objeto e no plano de projeção como pontos imagem. As coordenadas dos pontos na imagem do tabuleiro podem ser determinadas pela função “cv.findChessboardCorners(imagem, tamanho_tabuleiro, flags)”, que recebe como entrada a imagem e o tamanho do tabuleiro. Esta função retorna as coordenadas dos pontos de referência na imagem (corners) e a variável “ret” recebe “True” quando isto acontece. Os pontos são salvos de acordo com as posições no tabuleiro, da esquerda para a direita e de cima para baixo. ret, corners = cv.findChessboardCorners(imagem, (9,6), None) Geralmente é utilizada a função “cv.cornerSubPix(imagem, corners, winSize, zeroZone, criterio)” para aumentar a acurácia do posicionamento dos pontos. A função “cornerSubPix” recebe a imagem e os pontos de referência calculados (corners) e avalia os pontos dentro uma vizinhança estabelecida pelo parâmetro “winSize”, que define a metade do comprimento da janela de pesquisa. O parâmetro “zeroZone” indica o tamanho da região em que os cálculos não são realizados, utilizado às vezes para evitar singularidades da matriz de autocorrelação. Quando “zeroZone = (-1,-1)”, esta estratégia não é utilizada, não existindo regiões “mortas”. Como o algoritmo da função “cornerSubPix” é iterativo é necessário estabelecer um critério de parada, como o número de interações e/ou acurácia. Após estabelecer os pontos da imagem é possível desenhá-los sobre o tabuleiro com a função “cv.drawChessboardCorners(imagem, tamanho_tabuleiro, corners, ret). criterio = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001 corners = cv.cornerSubPix(imagem, corners, (11,11), (-1,-1), criterio) cv.drawChessboardCorners(imagem, (9,6), corners, ret) As coordenadas reais identificadas como pontos objeto e as coordenadas na imagem são repassadas como entradas da função “cv2.calibrateCamera(objectPoints, imagePoints, imageSize, flags)”. Esta função retorna os parâmetros intrínsecos e extrínsecos da câmera com base no método Zhang, o qual considera inicialmente nos cálculos uma aproximação da câmera pinhole e depois integra as distorções. O problema da calibração se fundamenta em estabelecer os parâmetros das transformações que realizam o mapeamento da coordenada homogênea P=(X,Y, Z, 1) do real para a imagem p=(x, y, 1) [71, p. 11]: \\[ \\lambda \\begin{bmatrix} x\\\\ y\\\\ 1 \\end{bmatrix} = K \\begin{bmatrix} r_1 &amp; r_2 &amp; r_3 &amp; t \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ Z = 0\\\\ 1 \\end{bmatrix} K \\begin{bmatrix} r_1 &amp; r_2 &amp; t \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ 1 \\end{bmatrix} \\tag{9.23} \\] Ao adotar que o plano de calibração está sobre Z=0 podemos simplificar as coordenadas homogêneas dos pontos reais como (X, Y, 1), o que permite escrever a transformação de coordenadas como [71, p. 12]: \\[ \\lambda \\begin{bmatrix} x &amp; y &amp; 1 \\end{bmatrix} = H \\begin{bmatrix} X &amp; Y &amp; 1 \\end{bmatrix} \\tag{9.24} \\] em que dado os pontos no plano padrão de calibração (P) e os pontos na imagem (p) é possível determinar uma Homografia (H), uma transformação projetiva entre planos, para cada imagem padrão. Assim, ao calcular a Homografia como indicado no tópico homografia se consegue estabelecer a seguinte relação: \\[ H = \\lambda K\\begin{bmatrix} r_1 &amp; r_2 &amp; T \\end{bmatrix} \\tag{9.3} \\] O que permite determinar os parâmetros intrínsecos e extrínsecos da câmera. Os elementos da matriz de calibração (K) são estimados considerando a matriz simétrica \\(B = K^{-T} K^{-1}\\) de acordo com as expressões [71, p. 14]: \\[ \\begin{split} &amp;v_c = \\frac{(B_{12}B_{13} - B_{11}B_{23})}{(B_{11}B_{22} - B_{12}^2)}\\\\ &amp;\\lambda = B_{33} - \\frac{[B_{13}^2 + v_c(B_{12}B_{13} - B_{11}B_{23})]}{B_{11}}\\\\ &amp;f_x = \\sqrt{\\frac{\\lambda}{B_{11}}}\\\\ &amp;f_y = \\sqrt{\\frac{\\lambda B_{11}}{(B_{11}B_{22} - B_{12}^2)}}\\\\ &amp;c = \\frac{-B_{12}f_x^2f_y}{\\lambda}\\\\ &amp;u_c = \\frac{\\lambda v_c}{f_x} - \\frac{B_{13}f_x^2}{\\lambda} \\end{split} \\tag{9.25} \\] Após determinar os cinco elementos da matriz de calibração (\\(v_c, u_c, f_x, f_y, c\\)) e o fator de escala (\\(\\lambda\\)) calculam-se os parâmetros extrínsecos (R, T) [71, p. 15]: \\[ \\begin{split} &amp;r_1 = \\frac{1}{\\lambda}K^{-1}h_1\\\\ &amp;r_2 = \\frac{1}{\\lambda}K^{-1}h_2\\\\ &amp;r_3 = r_1 x r_2\\\\ &amp;t = \\frac{1}{\\lambda}K^{-1}h_3\\\\ \\end{split} \\tag{9.26} \\] As distorções podem ser calculadas com base nos valores estimados das posições (p’) utilizando os parâmetros da calibração, que apresentam distorções, com os pontos ideais da imagem (p). Os modelos de distorções radiais e tangenciais apresentados em são reescritos na forma matricial, o que permite determinar uma solução por mínimos quadrados com os resultados para os coeficientes de distorção radial (\\(k_1, k_2, k_3\\)) e tangencial (\\(p_1, p_2\\)) [69, p. 44]. Considera-se que os parâmetros intrínsecos são os mesmos em todas as imagens padrão, e que somente os parâmetros extrínsecos se alteram quando a imagem é reposicionada [72]. Todas as etapas anteriores são resolvidas por uma única função no OpenCV: ret, cameraMatrix, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, imagem.shape[::-1], None, None) em que se tem como saída da função a matriz de calibração (cameraMatriz) e os coeficientes de distorções (dist), que valem para todas as imagens, e o vetor de rotação R (rvecs) e de translação T (tvecs) que são calculados por padrão. Refêrencias "],["refêrencias.html", "Refêrencias", " Refêrencias "]]
