[["index.html", "Material introdutório de Processamento Digital de Imagens(PDI) Inicio", " Material introdutório de Processamento Digital de Imagens(PDI) Inicio Esse site foi desenvolvido pelo grupo COVAP (Computação Visual Aplicada) visando oferecer um material introdutório da disciplina de PDI para alunos e pessoas interessadas no assunto. Os conteúdos se encontram dividos na seguinte estrutura: Princípios básicos Formação da imagem Tipos de arquivo Espaço de cores Transformações Compressão de imagens Filtros Detectores Morfologia matemática Lembramos ainda que todo o material está hospedado no Github, agradeçemos então todo apontamento de erros e sugestões, para que assim consigamos sempre disponibilizar um conteudo revisado e livre de quaisquer erros. "],["intro.html", "Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica 1.2 Aplicações Processamento Digital de Imagens 1.3 Etapas do Processamento e Análise de Imagens", " Capítulo 1 Introdução 1.1 Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica A visão desempenha um papel importante na vida das pessoas, pois com ela é possível uma percepção incrivelmente rica do mundo ao seu redor. Para tentar reproduzir as capacidades visuais humanas por sistemas autônomos manipulados por computadores foram desenvolvidas pelo menos três grandes áreas [1, p. 2]: Processamento Digital de Imagens (PDI), Visão Computacional (VC) e a Computação Gráfica(CG), apresentados na Figura 1.1. Essas áreas, apesar de serem correlacionadas, têm objetivos e métodos diferentes, por isso a importância de distingui-las. Figura 1.1: Processos Computacionais com Imagens [1, p. 2]. O Processamento Digital de Imagens (PDI) busca realizar o pré-processamento das imagens, utilizando para isso técnicas de tratamento, como a correção da iluminação, eliminação de ruído, e a segmentação. O foco da Visão Computacional (VC) é a análise das imagens, identificando os seus componentes e obtendo informações. Diferente da VC, em que as imagens são os objetos de estudo, na Computação Gráfica (CG), as imagens são o resultado do processo. Na CG são geradas representações visuais seguindo descrições e especificações geométricas [1, p. 3]. A Tabela 1.1 apresenta de forma resumida as diferenças entre PDI, VC e CG. Na segunda linha da tabela está uma descrição simples de cada área, e na terceira linha um esquema identificando o objeto e o produto de cada processo. Tabela 1.1: Processos Computacionais com Imagens. Computação Gráfica (CG) Visão Computacional (VC) Processamento Digital de Imagens (PDI) Cria e altera imagens a partir de dados. Análise de imagem para criação de modelos. Transformação de imagem (tratamento). modelo → imagem imagem → modelo imagem → imagem As imagens tratadas em PDI têm como uma das finalidades servir de material para a Visão Computacional, como identificado na Figura 1.1. Muitas vezes as áreas de Visão Computacional e PDI são confundidas devido a dificuldade em se definir em que ponto uma termina e a outra começa. Mesmo não existindo uma linha clara entre os limites destas duas áreas é possível utilizar um paradigma que considera três níveis de processamento [2, p. 2]: Baixo nível A nível de pixel, realiza operações de pré-processamento, sendo utilizada, por exemplo, na redução de ruído, aumento de contraste e restauração. Médio nível Operações mais complexas, como segmentação, partição e reconhecimento de objetos individuais. Entrada é uma imagem mas a saída pode ser um conjunto contendo os atributos extraídos das imagens. Alto nível Interpretação do conteúdo da imagem e análise. Baseado nesses níveis, iremos considerar que o processamento de imagem atua nos primeiros dois níveis, ou seja, envolve o pré-processamento e processos de extração de elementos de imagens até o reconhecimento de componentes individuais. Como o foco deste material é o Processamento de Imagens Digitais (PDI), estes dois níveis serão apresentados com detalhes nos próximos tópicos, mas primeiro vejamos alguns exemplos de aplicações do PDI. 1.2 Aplicações Processamento Digital de Imagens As primeiras tarefas de processamento de imagens tiveram aplicações significativas por volta da década de 1960, quando se desenvolveram computadores com potencial suficiente para realizá-las. O programa espacial americano também foi um forte impulso para o contínuo desenvolvimento e aprimoramento das técnicas de processamento digital de imagem (PDI), já que imagens, como as obtidas da Lua através de sondas e transmitidas à terra, continham distorções provenientes das câmeras utilizadas. Era necessário então, a utilização de métodos para corrigir essas alterações [2, p. 4]. Outra área que também faz uso extensivo do processamento de imagens e impulsionou seu desenvolvimento é a área médica. Nessa área, o uso de imagens auxiliou no diagnóstico de doenças através de exames visuais como os de raio-x [2, p. 4]. A utilização do processamento de imagens para melhorar informações visuais, ajudando na interpretação humana, expandiu-se para diferentes setores. No sensoriamento remoto, o pré-processamento contribui para uma melhor análise de imagens aéreas e de satélite, aumentando a compreensão da superfície terrestre. Na arqueologia e nas artes, métodos de processamento de imagens podem restaurar fotografias com registros únicos de objetos raros, pinturas, documentos antigos e conteúdos em vídeos [3, p. 2]. Na física e em áreas da biologia, técnicas computacionais realçam imagens de experimentos em áreas como plasmas de alta energia e microscopia eletrônica [2, p. 5]. Com o aumento da automatização de tarefas, o processamento de imagens tem se destacado na aquisição de dados de imagens visando a percepção automática por máquinas [3, p. 3]. Técnicas de identificação de padrões podem ser aplicados no reconhecimento automático de caracteres, de impressões digitais, de faces, e de placas de veículos, contribuindo com setores de segurança. Na automação industrial tem sido utilizado no sistema de visão computacional para inspeção e montagem de produtos. Na área militar, pode ser aplicado na identificação e rastreamento de alvos em imagens de satélites, e na navegação de veículos autônomos. Nas áreas de medicina e biologia, rastreamentos automáticos em imagens radiográficas e amostras de sangue têm contribuído para os exames e testes [3, p. 3]. O processamento computacional de imagens aéreas e de satélites também é utilizado na previsão do tempo e em avaliações ambientais [2, p. 5]. Este variado campo de aplicações pode ser justificado pela capacidade dos aparelhos de processamento de imagens trabalharem com imagens de diversas fontes. Diferentemente dos seres humanos, que são limitados à banda visual do espectro eletromagnético (EM), o processamento computacional cobre todo o EM, variando de ondas gama a ondas de rádio [2, p. 1]. No processamento digital ainda é possível trabalhar com imagens geradas por fontes que os humanos não estão acostumados a associar com imagens. Essas fontes incluem acústica, ultrassom, microscopia eletrônica e imagens geradas por computador [2, p. 13]. Uma das formas mais fáceis de desenvolver uma compreensão básica da extensão das aplicações do processamento de imagens é categorizar as imagens de acordo com sua fonte. Na Figura 1.2 temos uma representação do EM, iremos a seguir explorar cada uma dessa faixas, apresentando algumas das áreas onde podem ser utilizados: Figura 1.2: Espectro eletromagnético [4]. Imagens formadas por raios gama As imagens formadas a partir de raios gama têm diferentes utilidades, sendo muito utilizadas na medicina e astronomia [2, p. 6]. Na medicina, existem procedimentos onde se injetam isótopos radioativos no paciente e por meio dos detectores de raio gama é formada uma imagem, como exemplo, escaneamento ósseo e tomografia por emissão de pósitrons (PET-scan). Na astronomia, ela pode ser utilizada para se conseguir ver detalhes astronômicos que estão presentes na faixa eletromagnética dos raios gama. Imagens formadas por raios X Imagens formadas a partir de raio X têm uma ampla gama de aplicações, desde seu uso na medicina até seu uso no meio industrial [2, p. 6]. Na indústria, pode ser utilizado para se encontrar defeitos de fabricação em produtos, e na medicina, vêm se utilizando muito o processamento de imagem e a visão computacional para ajudar no diagnóstico de doenças, como por exemplo, artérias obstruídas, fraturas e tumores. Imagens na banda ultravioleta O espectro ultravioleta também tem inúmeras aplicações, como a inspeção industrial, microscopia, imagens biológicas e observações astronômicas [2, p. 8]. Imagens na banda visível e infravermelho Essas duas bandas possuem uma gama extremamente ampla de aplicações, sendo utilizadas juntas ou separadas. Na banda visível, existem diversas aplicações, como em processos industriais, detecção de faces, detecção de placas de carros, etc [2, p. 11]. A banda infravermelho também possui inúmeras aplicações, sendo uma delas imagens a partir de satélites, onde o infravermelho nos permite ver inúmeros detalhes que somente com a banda visível não seria possível [2, p. 9]. Imagens na banda de micro-ondas e rádio Na banda de micro-ondas o melhor exemplo que temos é o radar. Essa banda tem uma peculiaridade de ser extremamente penetrante, podendo gerar imagens através de nuvens, vegetação, etc [2, p. 12]. Já a banda de rádio é muito utilizada na medicina, como exemplo na ressonância magnética e na astronomia [2, p. 12]. Como podemos observar, existem inúmeras maneiras de se conseguir imagens além da clássica imagem no espectro visível, isso nos dá a possibilidade de utilizar o PDI em inúmeras áreas e problemas. Na Figura 1.3 temos uma nebulosa observada a partir de diferentes bandas dos EM, sendo possível observar detalhes que passariam despercebidos se usássemos somente alguma delas. Figura 1.3: Nebulosa CRAB em diferentes frequências [5]. 1.3 Etapas do Processamento e Análise de Imagens Um dos objetivos deste material é servir de referência para o estudo inicial de Visão computacional, assim, para compreender as relações entre as etapas de processamento e análise de imagens apresentamos na Figura 1.4 uma sequência dos principais passos utilizados em uma aplicação de PDI. Neste material nos deteremos nos conteúdos de processamento, desde a aquisição de imagens, pré-processamento até segmentação. Figura 1.4: Etapas de aplicação de PDI [3, p. 4]. Vale ressaltar que essas etapas não são fixas e podem ser modificadas, sendo que uma base de conhecimentos é importante para orientar em uma aplicação específica [3, p. 4]. Aquisição da imagem Captura a imagem por meio de um dispositivo ou sensor e a converte em uma imagem digitalizada [3, p. 3]. Podemos citar como exemplo as câmeras fotográficas, tomógrafos médicos, satélites e scanners. Na Figura 1.5, temos um exemplo de aquisição de imagens do satélite Landsat, neste caso estão identificadas as bandas vermelha, verde e azul visíveis e o infravermelho próximo. Os detalhes sobre a aquisição de imagens serão discutidos no tópico Formação de Imagem. Figura 1.5: Aquisição de imagens de satélite.(a) a (d) mostram quatro imagens espectrais de satélite da cidade de Washington, D.C., banda vermelha, verde e azul visíveis e o infravermelho próximo, respectivamente. (e) Imagem colorida como combinação RGB de (a), (b) e (c). (f) Imagem colorida obtida pela combinação de (b), (c) e (d)[2, p. 279]. Pré processamento Essa etapa busca realizar mudanças e ajustes na imagem visando melhorar seu uso nas etapas futuras [3, p. 3]. Como exemplo temos casos onde não precisamos das cores de uma imagem, podendo então realizar a conversão para grayscale(tons de cinza), ou precisamos gerar imagens coloridas como na Figura 1.5 em que são combinadas as bandas espectrais. Além disso, podemos realizar cortes ou realces, isolando somente a parte de maior interesse na imagem como na Figura 1.6, ou também atenuar o ruído na imagem, além de outras técnicas que serão abordadas em outros tópicos. Figura 1.6: Subtração de imagens para realce de diferenças.(a) Imagem da área de Washington D.C em infravermelho. (b) Resultado ao zerar o bit menos significativo de todos os pixels de (a). (c) Diferença entre as duas imagens ajustada para a faixa [0, 255], sendo que valores em preto (0) indicam pontos nos quais não há nenhuma diferença [2, p. 49]. Segmentação Nessa etapa as informações de interesse são extraídas da imagem, geralmente, pela detecção de descontinuidades (bordas) ou de similaridades na imagem [3, p. 4]. Na Figura 1.7 é mostrado o resultado de um exemplo de segmentação por similaridade, em que o elemento de maior interesse é o rio. Figura 1.7: Extração de características de uma imagem segmentada.(a) Imagem na banda infravermelha da área de Washington, D.C. (b) Segmentação da imagem por limiarização. (c) O maior componente conexo de (b). Técnica de representação por esqueleto de (c) [2, p. 544]. Representação e Descrição Armazenar e manipular objetos de interesse extraídos da imagem. O processo de descrição visa a extração de características para discriminar classes de objetos [3, p. 4]. Na Figura 1.8, o objetivo é determinar o tamanho das ramificações do rio, para isto considerou-se que o tamanho de cada ramificação no esqueleto seria uma boa aproximação [2, p. 545]. O esqueleto é uma representação do rio, e seus elementos são discriminados dentro do maior componente conexo da imagem segmentada. Reconhecimento e Interpretação Essa etapa examina as informações produzidas na etapa anterior e classifica cada objeto como sendo de interesse ou não, atribuindo significado ao conjunto de objetos reconhecidos pelos rótulos [3, p. 3]. Uma aplicação de análise de imagens inclui a classificação de áreas em uma imagem multiespectral como na Figura 1.8. Neste exemplo utilizou-se o método bayesiano, em que cada pixel da imagem foi avaliado em relação a três classes (água, desenvolvimento urbano e vegetação). Nas Figuras 1.8, pontos pretos representam pontos classificados incorretamente, enquanto pontos brancos foram classificados corretamente [2, p. 579]. Figura 1.8: Classificação bayesiana em uma imagem multiespectral. Resultado (em branco) da classificação na classe água, desenvolvimento urbano e vegetação, da esquerda para a direita [2, p. 579]. Refêrencias "],["formação-da-imagem.html", "Capítulo 2 Formação da imagem 2.1 Câmera pinhole e geometria 2.2 Lentes 2.3 Sensor 2.4 Amostragem e Quantização 2.5 Definição de imagem digital 2.6 Resolução espacial e de intensidade 2.7 Pixels", " Capítulo 2 Formação da imagem Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é do espectro eletromagnético, mas podendo ser também, a partir da energia mecânica (ultrassom), feixe de elétrons, etc.. Cada fonte necessita de um método específico de captura, para algumas pode ser uma câmera fotográfica, porém a outras é necessário que o computador sintetize a imagem, como o microscópio eletrônico. Como já mencionado no tópico de introdução, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensitivos a essa faixa de luzes, que vêm da luz solar e nos ajuda a realizar nossas atividades. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas, como a ultravioleta[6, p. 2]. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas[7, p. 8]. A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de 0,43 a 0,79 \\(\\mu m\\). Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar[2, p. 28]. Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir. 2.1 Câmera pinhole e geometria Na figura 2.1 temos um esquema básico de como geralmente ocorre a aquisição de imagens, primeiramente a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida, sendo após isso capturada por um dispositivo, como uma câmera. Figura 2.1: Representação de uma típica captura de imagem [7, p. 8]. Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, este é conhecido como câmara pinhole(do inglês buraco de alfinete) ou câmara escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na figura 2.2, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruim. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores. Figura 2.2: Introdução de barreira para captura de imagem [7, p. 11]. Na figura 2.2 percebemos que a imagem resultante acaba invertida, isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir. Figura 2.3: Geometria de uma câmera pinhole [8, p. 5]. Na figura 2.3, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância \\(Z\\) da abertura e a uma distância \\(Y\\) o eixo óptico, podemos definir a altura \\(y\\) e a largura \\(x\\) da projeção do objeto utilizando a simetria de triângulos: \\[-\\frac{y}{f}=\\frac{Y}{Z}\\Leftrightarrow y=-f\\frac{Y}{Z} \\text{ e } -\\frac{x}{f}=\\frac{x}{f} \\Leftrightarrow x=-f\\frac{X}{Z}\\] A variável \\(f\\) nessa equação se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera. Os sinais negativos das equações significam que a imagem projetada está rotacionada a 180º verticalmente e horizontalmente, como podemos confirmar na imagem acima. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens, como precisar de um longo tempo de exposição para captura da imagem. 2.2 Lentes As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso. Figura 2.4: Ação de uma lente sobre os raios de luz [7, p. 12]. Como podemos ver na figura 2.4, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando-se o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada. O ponto \\(F\\) onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância \\(f\\), que vai do centro óptico \\(O\\) até \\(F\\) é conhecida como Distância Focal. Definindo a distância do objeto real até a lente como g e a distância até a formação da imagem após passa pela lente como b temos que: \\[\\frac{1}{g}+\\frac{1}{b}=\\frac{1}{f}\\] Como \\(f\\) e \\(b\\) estão normalmente entre 1 mm e 100mm isso mostra que \\(\\frac{1}{g}\\) não tem quase nenhum impacto na equação e significa que \\(b = f\\). Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal. Outro ponto importante das lentes é conhecido como zoom óptico. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, \\(B\\), aumenta quando \\(f\\) aumenta. Podemos representar isso na seguinte equação, onde \\(g\\) é o tamanho real do objeto: \\[\\frac{b}{B}=\\frac{g}{G}\\] Na prática \\(f\\) é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos. Se o \\(f\\) for constante, quando alteramos a distância do objeto, no caso \\(g\\), sabemos que \\(b\\) também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos \\(b\\) temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando \\(b\\) para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco. Figura 2.5: Uma imagem focada e desfocada [7, p. 11]. A figura 2.5 ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos. Figura 2.6: Profundidade de campo [7, p. 13]. A figura 2.6 apresenta outro ponto muito importante, chamado Profundidade de Campo(Depth of field), que representa a soma das distâncias \\(g_l\\) e \\(g_r\\), que representam o quando os objetos podem ser movidos e permanecerem em foco. Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão(Field of View ou FOV) que representa a área observável de uma câmera. Na figura 2.7 essa área observável é denotada pelo ângulo \\(V\\). O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as equações seguintes para o FOV vertical e horizontal: \\[FOV_x = 2*\\tan^{-1}\\left(\\frac{\\frac{comprimento\\ do\\ sensor}{2}}{f}\\right) \\text{ e } FOV_y = 2*\\tan^{-1}\\left(\\frac{\\frac{altura\\ do\\ sensor}{2}}{f}\\right)\\] Figura 2.7: Campo de visão [7, p. 14]. Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de 14mm, altura de 10mm e uma distância focal de 5mm temos: \\[FOV_x=2*tan^{-1}\\left(\\frac{7}{5}\\right)=108.0^{\\circ} \\text{ e } FOV_y=2*tan^{-1}(1)=90^{\\circ}\\] Isso significa que essa câmera tem uma área observal de 108.9º horizontalmente e 90º verticalmente. Na figura 2.8 temos o mesmo objeto fotografado com diferentes profundidades de campo: Figura 2.8: Diferentes profundidades de campo [7, p. 15]. Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris no olho humano, ela controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para capturar a imagem. 2.3 Sensor Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD, que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS, usado em casos mais gerais, como câmeras de celulares. Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na figura 2.9, conhecido como PDA(Photodiode Array): Figura 2.9: Sensor(area matricial de celulas), Single Cell(uma única celula sensora) [7, p. 17]. Como podemos ver, o sensor consiste em várias pequenas células, cada uma um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na figura 2.10 podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta(correctly exposed), logo em seguida temos uma que sofreu de superexposição(overexposed) e na terceira temos uma com subexposição(under exposed). Por último temos uma imagem que sobre com o movimento do objeto que estava sendo capturado, oque ocasionou o borramento(motion blur). Figura 2.10: Diferentes níveis de exposição [7, p. 17]. Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas em imagens coloridas, como são capturadas? Imagens coloridas utilizam, especialmente, o formato RGB, que significa Red-Green-Blue, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na figura 2.11 podemos ver uma imagem com seus componentes separados: Figura 2.11: Imagem colorida separada em seus três componentes [7, p. 28]. Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na figura 2.12. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo. Figura 2.12: Captura de imagem com três sensores [9, p. 242]. Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel, isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na figura 2.13: Figura 2.13: Filtro Bayer [7, p. 29]. Podemos perceber que ocorre uma maior ocorrência das cores verdes, isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase a sua captura. Na figura 14 temos uma esquematização de como cada pixel recebe informação de somente uma cor, por meio da filtragem, onde a luz que entra(Incoming light) é filtrada e somente a cor de interesse consegue passar, após isso ela chega a malha de sensores(sensor array): Figura 2.14: Sensores com padrão Bayer [10]. Vemos na figura 2.14 que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos. 2.4 Amostragem e Quantização Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto. 2.4.1 Amostragem Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura 2.15, na qual a figura 2.15 (a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto. A figura 2.15 (b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha AB. Nas extremidades do gráfico na figura 2.15 (b), a intensidade é mais alta devido a parte branca da imagem, já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na figura 2.15 (c). Esse procedimento de amostragem, na prática, é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo teríamos que ter um número infinito de pixels, como isso não é possível recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem [2]. Figura 2.15: Filtro Bayer [2, p. 34]. 2.4.2 Quantização Na figura 2.15 (c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na figura 2.15 (d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na figura 2.15 (c). Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital[8, p. 8]. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo[11, p. 9]. No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos \\(\\{0,\\dots, L-1\\}\\), onde \\(L\\) é uma potência de dois, ou seja, \\(L = 2_k\\) [11, p. 10]. Isso significa que L é o número de tons de cinza que podem ser representados com uma quantidade k de bits. Em muitas situações é utilizado \\(k = 8\\), ou seja, temos 256 níveis de cinza. Ao realizar a quantização e a amostragem linha por linha no objeto da figura 2.16 (a) é produzida uma imagem digital bidimensional como na figura 2.16. Figura 2.16: Filtro Bayer [2, p. 35]. 2.5 Definição de imagem digital Uma imagem pode ser definida como uma função bidimensional, \\(f(x, y)\\), em que \\(x\\) e \\(y\\) são coordenadas espaciais (plano), e a amplitude de \\(f\\) em qualquer par de coordenadas \\((x, y)\\) é chamada de intensidade ou nível de cinza da imagem nesse ponto [2]. Quando \\(x\\), \\(y\\) e os valores de intensidade de f são quantidades finitas e discretas, chamamos de imagem digital. A função \\(f(x, y)\\) pode ser representada na forma de uma matriz (M x N) como na Figura, em que as M linhas são identificadas pelas coordenadas em \\(x\\), e as N colunas em \\(y\\). Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou pel. O formato numérico da matriz, imagem 2.17, é apropriado para o desenvolvimento de algoritmos, particularmente quando se escreve a equação da matriz (M x N): \\[f(x,y) = \\begin{bmatrix} f(0,0) &amp; f(0,1) &amp; \\cdots &amp; f(0,N-1) \\\\ f(1,0) &amp; f(1,1) &amp; \\cdots &amp; f(1, N-1) \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ f(M-1,0) &amp; f(M-1, 1) &amp; \\cdots &amp; f(M-1, N-1) \\end{bmatrix}\\] Figura 2.17: Representações da imagem digital [2, p. 36]. Na figura 2.17 (a) temos a representação da imagem em 3D, onde a intensidade de cada pixel é representada no eixo z, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na figura 2.17 (b), formato que seria visualizado em um monitor ou uma fotografia [2]. Em cada ponto da figura 2.17 (a), o nível de cinza é proporcional ao valor da intensidade \\(f\\), assumindo valores 0, 0,5 ou 1. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco. Note que na Figura, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo x positivo direcionado para baixo e o eixo y positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez [2]. De acordo com o tamanho da matriz (M x N) e dos níveis discretos de tons de cinza (\\(L = 2^k\\)) que os pixels podem assumir é possível determinar o número, \\(b\\), de bits necessários para armazenar uma imagem digitalizada: \\[b = M × N × k\\] Quando uma imagem pode ter \\(2^k\\) níveis de intensidade, geralmente ela é denominada como uma “imagem de k bits”. Por exemplo, uma imagem com 256 níveis discretos de intensidade é chamada de uma imagem de 8 bits. A figura 2.18 mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (N x N) para diferentes valores de N e k. O número de níveis de intensidade (L) correspondente a cada valor de k é mostrado entre parênteses. Observa-se na figura 2.18 que uma imagem de 8 bits com dimensões 1.024 × 1.024 exigiria aproximadamente 1MB para armazenamento. Figura 2.18: Número de bits de armazenamento para vários valores de N e k [2, p. 38]. 2.6 Resolução espacial e de intensidade Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (M x N) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente dots per inch (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2.400 dpi [2]. A figura 2.19 mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A figura 2.19 (a) tem resolução 512 x 512, e a resolução das demais 2.19 (b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução. Figura 2.19: Efeitos da redução da resolução espacial [3, p. 20]. A resolução de intensidade ou profundidade corresponde ao número de bits (k) utilizados para estabelecer os níveis de cinza da imagem (\\(L=2^k\\)). Por exemplo, em uma imagem cuja intensidade é quantizada em L= 256 níveis, a profundidade é de k = 8 bits por pixel. Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura. A imagem (a) apresenta 256 níveis de cinza (k = 8). As imagens (b) e (c) foram geradas pela redução do número de bits k = 4 e k = 2, respectivamente, mas mantendo a mesma dimensão. Figura 2.20: Efeitos da redução de profundidade [7, p. 19]. 2.7 Pixels A topologia de uma imagem digitalizada, desempenha papel importante na especificação, localização e relação entre as coordenadas dos pixels da imagem, tornando mais fácil a manipulação da imagem. Uma imagem digitalizada contém as seguintes propriedades de seus pixels: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas. Para especificar, localizar, e relacionar topologicamente à imagem, consideramos: p,q denotando os pontos e, \\(p(x,y)\\), \\(q(x,y)\\), \\(u(x,y)\\) coordenadas dos pontos denotados, expressaremos \\(V\\) Conjunto de valores em uma imagem binária \\(V=\\{0,1\\}\\). 2.7.1 Vizinhança Vizinhança 4 - \\(\\left[N_4(p)\\right]\\) \\(N_4(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na horizontal outros dois na vertical suas coordenadas, ou seja, é o conjunto de pixels ao redor de p, sem considerar as diagonais [12, p. 15]. Exemplo na figura 2.21 (a). \\[p(x,y): p(x+1, y), p(x-1, y), p(x, y+1), p(x, y-1)\\] Vizinhança D - \\(\\left[N_D(p)\\right]\\) \\(N_D(p)\\) em \\(p(x,y)\\) possui quatro vizinhos, dois na diagonais superiores (direita, esquerda ) outras duas na diagonais inferiores (direita, esquerda) suas coordenadas, ou seja o conjunto de pixels ao redor de \\(p\\), considerando apenas as diagonais [12, p. 15].Exemplo na figura 2.21 (b). \\[p(x,y): p(x+1,y+1), p(x+1, y-1), p(x-1, y+1), p(x-1, y-1)\\] Vizinhança 8 - \\(\\left[N_8(p)\\right]\\) \\(N_8(p)\\) em \\(p(x,y)\\) possui 8 vizinhos, quatro \\(N_4(p)\\) e outros 4 \\(N_D(p)\\)suas coordenadas ou seja o conjunto de pixels ao redor de \\(p\\), considerando união das vizinhanças-4 e vizinhança-8 [12, p. 15]. Exemplo na figura 2.21 (c). \\[p(x,y): N_8(p) = N_4(p) \\cup N_D(p)\\] Figura 2.21: Vizinhanças [12]. 2.7.2 Conectividade Conceito importante, usado no estabelecimento limite das bordas de objetos e, identifica componentes das regiões da imagem (obtenção de propriedades específicas do objeto para o processamento de mais alto nível ). Dois pixels \\(p(x,y)\\), \\(q(x,y)\\) estão conectados se: São de alguma forma vizinhos (\\(N_4\\),\\(N_D\\) ou \\(N_8\\)). Seus níveis de cinza satisfazem algum critério de similaridade (\\(V = \\{ \\dots \\}\\)). Conectividade de 4: Os pixels p e q, assumindo valores em &amp;V&amp; , são conectados de 4 somente se q pertence ao conjunto \\(N_4(p)\\). Exemplo em figura 2.22 (a). \\[C4_{p,q} \\text{ em } V \\Leftrightarrow q \\in N_4(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C4_{p.q} \\text{ verdadeiro}\\] Conectividade de m(conectividade mista): Dois pixels \\(p\\) e \\(q\\), assumindo valores em \\(V\\), são conectados de m somente se: \\(q\\) pertence ao conjunto \\(N_4(p)\\) \\(q\\) pertence ao conjunto \\(N_D(p)\\) e a interseção entre \\(N_4(p)\\) e \\(N_4(q)\\) for vazia. \\[Cm_{p,q} \\text{ em } V\\Leftrightarrow (q \\in N_4(p) \\vee (q \\in N_D(p) \\wedge N_4(p) \\cap N_4(q) = \\{\\})) \\vee f(p) e f(q) \\in V \\] \\[V = \\{0\\} \\to Cm_{p.q}\\text{ falso}\\] Exemplo em figura 2.22 (b). Conectividade de 8: Os pixels \\(p\\) e \\(q\\), assumindo valores em V, são conectados de 8 somente se \\(q\\) pertence ao conjunto \\(N_8(p)\\). Exemplo em figura 2.22 (c). \\[C8_{p,q} \\text{ em } V\\Leftrightarrow q \\in N_8(p) \\wedge f(p) \\wedge f(q) \\in V \\] \\[V = \\{0\\} \\to C8_{p.q}\\text{ verdadeiro}\\] Figura 2.22: Conectividades [12]. 2.7.3 Adjacência Dois pixels \\(p\\) e \\(q\\), com valores pertencendo a \\(V\\) são: Adjacentes-4 se \\(q\\) estiver no conjunto \\(N_4(p)\\). Adjacentes-8 se \\(q\\) estiver no conjunto \\(N_8(p)\\). Adjacentes-m, \\(p\\) e \\(q\\) subconjuntos de pixels onde \\(\\{(pq) \\vee (pp) \\vee (q q) \\}\\), são ditos adjacentes se pegamos um pixel do primeiro conjunto for adjacente a um pixel do segundo. Na figura 2.23 temos exemplos de 1. e 2. (em 2.23 (a)) e de 3. em 2.23 (b). Figura 2.23: Adjacências [12]. 2.7.4 Componente Conexa Dois pixels \\(p\\) e \\(q\\) de um subconjunto de pixels \\(V\\) da imagem são ditos conexos em \\(V\\) se existir um caminho de pa qinteiramente contido em \\(V\\). Para qualquer pixel \\(p\\) em \\(V\\), o conjunto de pixels em \\(V\\) que são conexos a pé chamado um componente conexo de \\(V\\). Note que em uma componente conexo qualquer dois pixels deste componentes são conexos entre si. Em componentes conexos distintos os pixels são disjuntos (não conectados). 2.7.5 Medidas de Distância Para pixels \\(p\\), \\(q\\) e \\(z\\) com coordenadas \\(p(x,y)\\), \\(q(s,t)\\) e \\((u,v)\\), respectivamente, \\(D\\) é uma função distância ou métrica se: \\(D(p,q) &gt;= 0 (D(p,q) = 0 ) \\Leftrightarrow p = q\\) \\(D(p,q) = D(q,p)\\) \\(D(p,z) &lt;= D(p,q) + D(q,u)\\) A distância entre dois pontos quaisquer pode ser definida por: \\[D_e(p,q) = \\sqrt{(x-s)^2 + (y-t)^2}\\] conhecida como Distância Euclidiana. Distância \\(D_4\\)(City Block ou Quarteirão) entre \\(p(x,y)\\) e \\(q(s,t)\\)é definida por: \\[D4(p,q)=|x-s|+|y-t|\\] Distância D8(Distância Xadrez) entre \\(p\\) e \\(q\\) é definida como: \\[D_8 = max(|x-s|,|y-t|)\\] 2.7.6 Operações Lógico-aritméticas As operações entre pixels são computadas pixel a pixel, considerando p e qpodemos efetuar as seguintes operações aritméticas e lógicas. Operações Aritméticas: Adição: \\(p+q\\). O uso ocorre ao se fazer a média para redução de ruído. Subtração: \\(p-q\\). É usada para remover informação estática de fundo, Detecção de diferenças entre imagens. Multiplicação: \\(p\\cdot q\\). Calibração de brilho. Divisão: \\(p\\div q\\) As operações Aritmética de Multiplicação e Divisão são usadas para corrigir sombras em níveis de cinza, produzidas em não uniformidades da iluminação ou no sensor utilizado para a aquisição da imagem. Operações Lógicas: Conjunção: \\(p\\wedge q\\) Disjunção: \\(p\\vee q\\) Complementar: \\(\\neg q(\\bar{q})\\) As operações lógicas podem ser combinadas para formar qualquer outra operação lógica, são aplicadas apenas em imagens binárias, tem seu uso no mascaramento, detecção de características e análise de forma. Refêrencias "],["transformacões-geométricas.html", "Capítulo 3 Transformacões Geométricas 3.1 Definição 3.2 Representação Vetorial e Matricial da Imagem 3.3 Sistema de coordenadas objetos (2D e 3D) 3.4 Transformações em pontos e objetos 3.5 Transformações Lineares 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação 3.9 Transformação Cisalhamento (Shearing ou Skew) 3.10 Transformação de Reflexão 3.11 Características especias 3.12 Transformação Isométrica 3.13 Transformação Semelhança 3.14 Transformação Afim 3.15 Transformação Projetiva 3.16 Transformação Inversão 3.17 Classificação das Projeções Geométricas", " Capítulo 3 Transformacões Geométricas Nessa seção abordaremos conceitos importantes que detalham processos de transformações geométricas na imagem digitalizada, definiremos a seção proposta; representação vetorial e matricial da Imagem; sistema de coordenadas objetos (2D e 3D); transformações em pontos e objetos; transformações (lineares, translação, escala, rotação, cisalhamento (Shearing ou Skew), reflexão )); características especiais transformação (isométrica, semelhança, afim, projetiva, inversão); classificação das projeções geométricas. 3.1 Definição As transformações geométricas são operações que podem ser utilizadas sobre uma imagem, visando alteração de características como: posição , orientação, forma ou tamanho da imagem apresentada. As operações (transformações geométricas) não alteram a topologia (pixels) da imagem operada, apenas possibilitam a projeção da imagem no espaço determinado. 3.2 Representação Vetorial e Matricial da Imagem Podemos definir um vetor como segmento de reta orientada (sentido e direção). Pensado em um vetor 2D, representada por V, coordenadas, para o ponto P(x,y), tendo assim um sentido e uma direção, um sentido e um comprimento especificado. Podemos então calcular o comprimento do vetor 2D, pela fórmula: \\(|V| = {\\sigma\\sqrt{x^2 + y^2}}\\) Exemplo 1: Calcule o comprimento de V com \\(X=2\\) e \\(Y=3\\). Resolução: $ V = 22+ 32 = 3.605 $ Se pensamos em Vem 3D, definindo a origem ao ponto \\(P(x,y, z)\\) seu comprimento seria: \\(|V| = {\\sigma\\sqrt{x^2+y^2+z^2}}\\) Exemplo 2: Calcule o comprimento de V com \\(X=2\\) , \\(Y=3\\) e \\(Z=1\\). Resolução: \\(|V| = {\\sigma\\sqrt{x^2+3^2+1^2 = 3.741}}\\) Uma matriz é um arranjo (array) de elementos em duas direções (linha e coluna). Para manipulá-la é necessário que definimos a quantidade de elementos existentes em cada direção. Representaremos matriz com a letra M, as direções (linha e coluna) pelas letras L e C. Suponhamos que L = 4 e C = 4 então \\(M[4][4]\\) possui 4 linhas e 4 colunas, como representada na figura 3.0. 3.3 Sistema de coordenadas objetos (2D e 3D) 3.4 Transformações em pontos e objetos 3.5 Transformações Lineares 3.6 Transformação de Translação 3.7 Transformação de Escala 3.8 Transformação de Rotação 3.9 Transformação Cisalhamento (Shearing ou Skew) 3.10 Transformação de Reflexão 3.11 Características especias 3.12 Transformação Isométrica 3.13 Transformação Semelhança 3.14 Transformação Afim 3.15 Transformação Projetiva 3.16 Transformação Inversão 3.17 Classificação das Projeções Geométricas "],["transformações-radiométricas.html", "Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência 4.4 Processamento de histograma 4.5 Equalização do histograma 4.6 Especificação de histograma", " Capítulo 4 Transformações radiométricas 4.1 Transformação Linear 4.2 Transformação Logarítmica 4.3 Transformação de Potência 4.4 Processamento de histograma 4.5 Equalização do histograma 4.6 Especificação de histograma "],["filtros.html", "Capítulo 5 Filtros 5.1 Convolução 5.2 Média 5.3 Mediana 5.4 Gaussiano", " Capítulo 5 Filtros Filtros são uma poderosa ferramenta para se realizar operações em imagens. Diferente das operações de ponto, que operam sobre um único pixel, as operações utilizando filtros levam em consideração os pixels próximos ao pixel atualmente em modificação. Isso nos permite realizar alterações muito mais complexas do que as realizadas anteriormente, como a operação sharpen (aguçamento) e blur (suavizar), que podem ser observados na figura 5.1 (a) e figura 5.1 (b), respectivamente. Figura 5.1: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] 5.1 Convolução Convolução é uma operação muito utilizada no PDI, a qual tem suas origens na matemática, onde ela é definida como uma operação realizada entre duas funções e que resulta numa terceira, ou, em outras palavras, ela recebe dois sinais de entrada e gera um sinal de saída. No caso do PDI, podemos imaginar os sinais de entrada como sendo a nossa imagem e o filtro (kernel), e a nossa saída como sendo a imagem filtrada. Quando dizemos kernel, estamos nos referindo a uma função ou, no caso do processamento de imagens, a uma matriz, que é aplicada em nossa imagem e produz como saída o objeto de entrada com modificações. Na tabela 5.1, temos exemplos de alguns tipos de kernels que podem ser utilizados na convolução e seus respectivos resultados. Além dos efeitos mais comuns, como o de desfoque (blur), podemos utilizar kernels que extraem informações mais complexas das imagens, como os detectores de borda, que serão discutidos mais a fundo nos próximos tópicos. Tabela 5.1: Exemplo de kernels (adaptada de [13]). Operação Kernel Resultado Identidade (Imagem Original) \\[\\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] Detecção de borda \\[\\begin{bmatrix} -1 &amp; -1 &amp; -1\\\\ -1 &amp; 8 &amp; -1\\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}\\] Média (box blur) \\[\\frac{1}{9}\\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\] Gaussian blur \\[\\frac{1}{16}\\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\ 2 &amp; 4 &amp; 2\\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\] Agora, veremos operação de convolução mais detalhadamente. Essa operação geralmente é representada por \\(*\\) e pode ser descrita, em poucas palavras, como uma soma de produtos que é realizada com um deslizamento sobre a função de entrada. A figura 5.2 representa de maneira visual a convolução em um cenário de uma dimensão, através do sinal de entrada (signal) e o kernel. Podemos ainda perceber que o kernel está rotacionado em 180º, isso se deve a definição de convolução. Figura 5.2: Convolução em uma dimensão [14] Algo interessante que podemos observar na imagem 5.2 é que nosso sinal de entrada é quase totalmente 0 e contém um único ponto 1, isso faz com que nosso resultado seja uma cópia do kernel. A partir disso, conseguimos imaginar o por que temos como resultado a própria imagem quando aplicado um kernel identidade, como mostrado na tabela 5.1. Antes de irmos mais adiante no assunto, é importante esclarecermos alguns conceitos para que não se tornem confusos. Existe outra operação matemática extremamente parecida com a convolução chamada correlação, sendo que ela também realiza a soma de multiplicações com a diferença de que ela não rotaciona o kernel. Para entendermos bem essa diferença, podemos observar a figura 5.3, onde temos um exemplo de correlação e convolução sendo executados em um espaço unidimensional. Temos uma função \\(f\\) e um filtro \\(w\\) na figura 5.3 (a) e (b), na sequência, de (b) e (j), temos as funções e os filtros prontos para se realizar a correlação e a convolução. Nas etapas (c) e (k), podemos ver o preenchimento com zeros, isso ocorre porque há partes das funções que não se sobrepõem, dessa forma, permite que \\(w\\) percorra todos os pixels de \\(f\\). Após isso, é realizado o primeiro passo da correlação e convolução, onde podemos observar que o resultado é 0 já que \\(w\\) está sobreposto por somente zeros, logo a soma da multiplicação de cada item de \\(w\\) por \\(f\\) será nulo. Deslocamos então o filtro \\(w\\) em uma unidade a direita, onde o resultado novamente será 0, sendo que o primeiro resultado não nulo se dará no terceiro deslocamento, sendo 8 para a correlação e \\(1\\) para a convolução. Temos o resultado de ambas operações em (g) e (o) e o resultado recortado em (h) e (p), recorte este que remove os zeros até o tamanho ficar igual ao da \\(f\\) inicial. Figura 5.3: Ilustração de correlação e convolução unidimensional [2, p. 96] Vamos extender agora essas duas operações à aplicação em duas dimensões. Uma representação disso pode ser vista na figura 5.4, onde temos novamente o kernel \\(w\\) e a função \\(f\\). Percebe-se outra vez o efeito de se aplicar o kernel em uma imagem com apenas o número 1 no meio, nos dois casos temos como saída a cópia do kernel, com a diferença que na correlação ele sai rotacionado. Assim, nota-se que se pré-rotacionarmos o filtro e realizarmos a correlação teremos no final uma convolução. Já que a correlação e convolução são iguais, quais delas devo utilizar? Segundo Gonzalez, [2, p. 98], isso é uma questão de preferência e qualquer uma das duas operações conseguem realizar a outra com uma simples rotação do kernel. Essa questão se torna ainda menos relevante quando utilizamos filtros que são simétricos, pois, como antes e após a rotação temos o mesmo kernel, tanto correlação ou convolução nos darão o mesmo resultado; já em kernels assimétricos, temos resultados diferentes. Ainda, segundo Moeslund, [7, p. 87], quando trabalhamos com filtros de desfoque, detectores de borda, entre outros, o processo de se aplicar o kernel é comumente chamado de convolução mesmo quando na prática se é implementada a correlação. Figura 5.4: Ilustração de correlação e convolução bidimensional [2, p. 98] 5.1.1 Definção matemática de convolução Vamos explorar um pouco das notações matemáticas utilizadas para representar a convolução e a correlação, assim também poderemos consolidar a idéia de que ambas são muito correlacionadas. Como dito no início desta seção, geralmente a convolução é identificada por \\(*\\), já a correlação costuma ser identificada por ☆. A correlação em duas dimensões segue a seguinte equação (Equação X): \\(g(x,y) = w(x,y)☆f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x+s,y+t)\\) Onde \\(w\\) é nosso \\(kernel\\) e \\(f\\) nossa imagem, podemos perceber que ambas são funções de duas variáveis, \\(x\\) e \\(y\\), pois estamos trabalhando em duas dimensões. Os limites dos somatórios são dados por \\(a=(m-1)/2\\) e \\(b=(n-1)/2\\). E o que essa função faz é andar em cada posição da imagem, ou seja, \\((x,y)\\), e substituir o píxel atual pela soma de produtos da multiplicação dos valores do \\(kernel\\) pelos valores dos píxels da imagem. Já a convolução tem uma equação bem similar, sendo diferente apenas pelos sinais negativos em \\(f\\), o que evidência a rotação do \\(kernel\\). Podemos notar que os sinais inversos estão em \\(f\\) e não em \\(w\\); segundo [2, p. 98], isso é usado para fins de simplicidade de notação e não alteram o resultado. \\(g(x,y) = w(x,y)*f(x,y) = \\sum_{s=-a}^{a}\\sum_{t=-b}^{b}\\ w(s,t)f(x-s,y-t)\\) Uma das melhores maneiras de entender bem as equações é ver um exemplo prático. Veremos isso a seguir, onde temos um exemplo passo-a-passo de correlação: \\[ \\text{w}\\text{*f}\\left(0,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(-1,-1\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot0+0\\cdot2+\\left(-2\\right)\\cdot1\\\\ +1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ =0\\,-2-3\\,=\\,-5 \\] \\[ \\text{w}\\text{*f}\\left(0,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,2\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot2+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ =0\\,+4+8=\\,12 \\] \\[ \\text{w}\\text{*f}\\left(0,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(0+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(-1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(-1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(-1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(1,3\\right)\\\\ =\\,1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ +2\\cdot1+0\\cdot0+\\left(-2\\right)\\cdot0\\\\ +1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ =0\\,+2+3=\\,5 \\] \\[ \\text{w}\\text{*f}\\left(1,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,0+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,1\\right)\\\\ =\\,1\\cdot0+0\\cdot2+\\left(-1\\right)\\cdot1\\\\ +2\\cdot0+0\\cdot9+\\left(-2\\right)\\cdot3\\\\ +1\\cdot0+0\\cdot5+\\left(-1\\right)\\cdot4\\\\ =\\left(-1\\right)\\,+\\left(-6\\right)+\\left(-4\\right)=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(1,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,2\\right)\\\\ =\\,1\\cdot2+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot9+0\\cdot3+\\left(-2\\right)\\cdot1\\\\ +1\\cdot5+0\\cdot4+\\left(-1\\right)\\cdot2\\\\ =2\\,+16+3=\\,21 \\] \\[ \\text{w}\\text{*f}\\left(1,2\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(1+s,2+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(0,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(0,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(0,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(2,3\\right)\\\\ =\\,1\\cdot1+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot3+0\\cdot1+\\left(-2\\right)\\cdot0\\\\ +1\\cdot4+0\\cdot2+\\left(-1\\right)\\cdot0\\\\ =1\\,+6+4=\\,11 \\] \\[ \\text{w}\\text{*f}\\left(2,0\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,0+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,-1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,1\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,-1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,1\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,-1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,1\\right)\\\\ =\\,1\\cdot0+0\\cdot9+\\left(-1\\right)\\cdot3\\\\ +2\\cdot0+0\\cdot5+\\left(-2\\right)\\cdot4\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =\\left(-3\\right)+\\left(-8\\right)+0=\\,-11 \\] \\[ \\text{w}\\text{*f}\\left(2,1\\right)\\text{=}\\sum_{s}^{}\\sum_{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,1+t\\right)\\,\\text{=}\\,\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,0\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,2\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,0\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,2\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,0\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,2\\right)\\\\ =\\,1\\cdot9+0\\cdot3+\\left(-1\\right)\\cdot1\\\\ +2\\cdot5+0\\cdot4+\\left(-2\\right)\\cdot2\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =8+6+0=\\,14\\\\ \\] \\[ \\text{w}\\text{f}\\left(2,2\\right)\\text{=}\\sum{s}^{}\\sum{t}^{}\\text{w}\\left(s,t\\right)\\text{f}\\left(2+s,2+t\\right)\\text{=}\\\\ \\text{+w}\\left(-1,-1\\right)\\text{f}\\left(1,1\\right)\\text{+w}\\left(-1,0\\right)\\text{f}\\left(1,2\\right)\\text{+w}\\left(-1,1\\right)\\text{f}\\left(1,3\\right)\\\\ \\text{+w}\\left(0,-1\\right)\\text{f}\\left(2,1\\right)\\text{+w}\\left(0,0\\right)\\text{f}\\left(2,2\\right)\\text{+w}\\left(0,1\\right)\\text{f}\\left(2,3\\right)\\\\ \\text{+w}\\left(1,-1\\right)\\text{f}\\left(3,1\\right)\\text{+w}\\left(1,0\\right)\\text{f}\\left(3,2\\right)\\text{+w}\\left(1,1\\right)\\text{f}\\left(3,3\\right)\\\\ =,1\\cdot3+0\\cdot1+\\left(-1\\right)\\cdot0\\\\ +2\\cdot4+0\\cdot2+\\left(-2\\right)\\cdot0\\\\ +1\\cdot0+0\\cdot0+\\left(-1\\right)\\cdot0\\\\ =3+8+0=11 \\] Fonte: Autoria própria inspirado em http://www.songho.ca/dsp/convolution/convolution2d_example.html e https://arxiv.org/abs/1603.07285 O exemplo utiliza um \\(kernel\\) assimétrico, então, como dito anteriormente, os resultados da correlação e convolução são diferentes. Deixaremos isso como um exercício ao leitor, realizar a convolução para provar a diferença. O caso anterior tem também uma característica que é o complemento das bordas com 0’s para que o \\(kernel\\) pudesse ser aplicado sobre toda a imagem. Essa é uma das maneiras de lidar com o problema das bordas, sendo que temos essa e mais algumas possibilidades [8, p. 125] [15, p. 60]: Preenchimento com constante (constant padding): Método onde as laterais são preenchidas com um valor constante, comumente 0. Preenchimento com vizinho (nearest neighbor): Se realiza o preenchimento das bordas adicionais com os valores dos vizinhos mais próximos. Reflexão (reflect): Os pixels das bordas da imagem são repetidos nas bordas adicionais. Repetição (ou wrap): A imagem é repetida nas bordas. Além dos métodos apresentados existem outras técnicas que podem ser utilizadas, sendo que em cada caso se escolhe a que melhor se ajusta à tarefa realizada. O último ponto a ser explorado é a escolha do tamanho ideal do preenchimento que deve ser aplicado à imagem. De forma geral, se temos uma imagem (\\(f\\)), um kernel (\\(w\\)) e um preenchimento (\\(p\\)), podemos escrever a seguinte relação1: \\[saída = (f_{vertical} - w_{vertical} + p_{vertical} + 1) \\times (f_{horizontal} - w_{horizontal} + p_{horizontal} + 1)\\] Isso nos leva a perceber que o tamanho da imagem de saída aumenta conforme o preenchimento. Na maioria dos casos, queremos que a imagem de entrada e saída tenha o mesmo tamanho, então usamos \\(p_{vertical} = k_{vertical} - 1\\) e \\(p_{horizontal} = k_{horizontal} - 1\\). E como, geralmente, o kernel tem tamanho ímpar, utilizamos \\(\\frac{p_{vertical}}{2}\\) e \\(\\frac{p_{horizontal}}{2}\\). 5.2 Média Um filtro de média é um tipo de filtro que utiliza a média dos valores dos píxels próximos ao píxel central. Como esse tipo de filtro realiza uma operação linear, ele é classificado como um filtro linear de suavização. Essa suavização se dá exatamente pelo tipo da operação utilizada, a média dos píxels vizinhos, que diminui a nitidez pela redução das transições abruptas nos níveis de intensidade. Um dos problemas que podem ocorrer é que as bordas também são mudanças abruptas, então podem ser comprometidas pelo filtro. Na figura 5.5, temos um exemplo do efeito desse filtro: Figura 5.5: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] A máscara (kernel) de um filtro de média pode ser representada por: \\[\\begin{bmatrix} \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9}\\\\ \\frac{1}{9} &amp; \\frac{1}{9} &amp; \\frac{1}{9} \\end{bmatrix} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\] A primeira forma é a soma de todos os valores divididos por 9, o qual é o tamanho do filtro. A segunda forma, onde todos os coeficientes do kernel são 1’s, é mais eficiente computacionalmente, pois realizamos todas as somas e multiplicações antes de dividirmos. Esse tipo de filtro é muitas vezes chamado de box filter. Figura 5.6: Imagem com diferentes tamanhos de filtro de média [2, p. 102] Na figura 5.6, temos um exemplo onde foram aplicados filtros de média com diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Podemos notar que com o menor filtro, o de tamanho 3, temos um leve borramento na imagem toda, mas que as partes que tem o mesmo tamanho ou são menores que o filtro tem um borramento maior. Isso exemplifica umas das importantes aplicações dos filtros de suavização, que é a de desfocar os objetos menores e deixar os maiores em maior evidência. Figura 5.7: Exemplo de uso do desfoque [2, p. 103] Na figura 5.7, podemos ver como o desfoque pode ser utilizado para encontrar os detalhes principais da imagem. Nesta imagem obtida a partir do telescópio Hubble foi aplicado o desfoque para diminuir a visibilidade dos objetos menores e dar maior ênfase aos da frente. E, então, foi limiarizado o resultado a fim de destacar esses objetos. 5.3 Mediana A mediana é um filtro não linear que utiliza como princípio a própria técnica estatística, que consiste em ordenar um conjunto de dados em ordem e selecionar o valor central. Esse tipo de filtro é muito eficiente na remoção de ruído, principalmente o tipo de ruído conhecido como ruído sal e pimenta, que tem uma característica aparência de pixels pretos e brancos. Como dito anteriormente que a mediana consiste em ordenar o conjunto de dados, nesse caso os valores da vizinhança do píxel. Se usarmos uma vizinhança de tamanho 3x3, a mediana será o quinto maior valor, pois consideramos também o valor do píxel central. Na figura 5.8, tem-se um exemplo mostrando uma imagem de raios X de uma placa de circuitos com ruído sal e pimenta (a). As respectivas aplicações de um filtro de média (b) e do filtro da mediana (c), ambos com dimensões 3x3. A partir dela, é visto o quanto o filtro da mediana se sai melhor na remoção do ruído. Figura 5.8: Remoção de ruído [2, p. 103] 5.4 Gaussiano Filtro de Gauss é um filtro simétrico (isotrópico) usado para suavizar imagens. Ele funciona análogo a um filtro de média ponderada, mas com um padrão, isto é, dá ênfase ao pixel central da máscara (kernel) e menor ênfase a medida que os demais se distanciam dele, seguindo um gradiente muito similar à distribuição gaussiana. O “similar” se deve somente ao fato que será necessário truncamentos, assim, havendo erros e por ser desconsiderado a constante multiplicadora, o que melhora o perfil da função para aplicação no PDI. Função gaussiana: \\[g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Função gaussiana modificada: \\[g(x) = e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Note que a função tem formato de sino, que varia de acordo com os valores atribuídos ao, \\(\\sigma\\), o qual dependerá da ênfase desejada no cálculo. A medida que \\(\\sigma\\) aumenta, maior é o peso dado às “caldas”, no caso, aos pixels mais distantes do pixel central. Característica representada no gráfico 5.9. A fim de que o pixel central da máscara esteja em \\((0,0)\\), considera-se \\(\\mu=0\\). Figura 5.9: Função gaussiana com \\(\\mu=0\\) e diferentes \\(\\sigma\\) O filtro gaussiano 2D é gerado através da convolução de dois vetores da mesma função - a imagem se refere ao subconjunto do contradomínio -, sendo estas, normalmente, oriundas da mesma função gaussiana modificada. De forma alternativa, ele também pode ser construído a partir da i-ésima linha do Triângulo de Pascal que corresponde ao tamanho do kernel desejado. Como o método é o mesmo para as duas fontes, explicaremos ele utilizando como fonte a função gaussiana modificada. O procedimento primário é obter a imagem da função, mas para obter um filtro com intuito gaussiano se deve selecionar a amostragem dos pontos no intervalo de \\(\\pm 2.5\\sigma\\) ou \\(\\pm 3.5\\sigma\\) [8, p. 114]. Assim, optando-se pela primeira opção, procura-se \\(g(-3)\\), \\(g(-2)\\), \\(g(-1)\\), \\(g(0)\\), \\(g(1)\\), \\(g(2)\\) e \\(g(3)\\). Entretanto, como alguns valores têm infinitas casas decimais, devemos truncá-los, no qual foi preferido com cinco casas decimais. \\(g(-3) = g(3) = 0,01110\\) \\(g(-2) = g(2) = 0,13533\\) \\(g(-1) = g(1) = 0,60653\\) \\(g(0) = 1\\) Então, considera-se o menor valor como 1, no caso \\(f(3)\\) e \\(f(-3)\\), e interpolamos os demais, truncando-os com propósito de obter a parte inteira. A divisão por 224 é oriunda da soma dos pesos da média. Pronto, temos a máscara gaussiana 1D necessária: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) Com a máscara em mãos, convoluciona-se os seguintes fatores: \\(\\frac{1}{224}\\begin{bmatrix}1 &amp; 12 &amp; 54 &amp; 90 &amp; 54 &amp; 12 &amp; 1\\end{bmatrix}\\) \\(\\frac{1}{224}\\begin{bmatrix}1 \\\\ 12 \\\\ 54 \\\\ 90 \\\\ 54 \\\\ 12 \\\\ 1\\end{bmatrix}\\) é a transposta da máscara 1D \\(f(x,y)\\) é a imagem de entrada Nessa propriedade há uma determinada ordem que é mais eficiente para o sistema digital, que é a convolução entre \\(f(x,y)\\) e uma das máscaras 1D e, depois, a convolução desse resultado com a transposta desta máscara. Pronto, como saída teremos o filtro gaussiano aplicado a imagem. Figura 5.10: Imagem com diferentes tamanhos de filtro gaussiano. (Adaptado:[2, p. 102]) Nesse exemplo, temos um exemplo similar ao usado no filtro de média, porém, usando-se o filtro gaussiano. A imagem de entrada (a) tem tamanho 500x500 onde foram aplicados filtros de diferentes tamanhos, 3x3(b) ; 5x5(c) ; 9x9(d) ; 15x15(e) e 35x35(f). Note que, ao contrário do exemplo anterior, há uma maior preservação da nítidez, todavia não seria um filtro eficiente para redução de ruído. Essa característica é devido ao maior peso dado ao pixel central do kernel. Refêrencias "],["segmentação.html", "Capítulo 6 Segmentação 6.1 Detecção por descontinuidade 6.2 Detecção de bordas 6.3 Transformada de Hough 6.4 Detecção de blobs 6.5 Detecção de junções ou cantos 6.6 Segmentação por limiarização 6.7 método de Otsu 6.8 Segmentação usando watersheds morfológicas", " Capítulo 6 Segmentação 6.1 Detecção por descontinuidade 6.1.1 Detecção de ponto 6.1.2 Detecção de linha 6.2 Detecção de bordas 6.2.1 Modelos de Bordas 6.2.2 Método do gradiente ( Roberts, Prewitt, Sobel). 6.2.3 Método de Marr-Hildreth 6.2.4 Método de Canny O algoritmo de Canny recebeu esse nome em alusão a John Canny, que o propôs em seu artigo, “A computational Approach to Edge Detection”[16], publicado em 1986. Sua formulação se baseava em três pontos principais: Uma baixa taxa de erro, ou seja, todas as bordas presentes na imagem devem ser encontradas e não deve haver respostas espúrias. O segundo critério diz que as bordas detectadas devem estar bem localizadas, em outras palavras, elas devem estar o mais próximo possível das bordas verdadeiras. O terceiro e último critério diz que se deve minimizar o número de máximos locais em torno da borda verdadeira, para que não sejam encontrados múltiplos pixels de borda onde deve haver somente um. Em seu trabalho, Canny buscou encontrar soluções ótimas, matematicamente, que obedecessem os três critérios. Apesar disso, é muito difícil, ou impossível, encontrar uma solução que satisfaça completamente os objetivos descritos[2, p. 474]. Todavia é possível utilizar uma aproximação por meio de otimização numérica com as bordas em degrau em um exemplo 1-D que contenham ruído branco gaussiano e mostrar que uma boa aproximação para um ótimo detector de bordas é a primeira derivada de uma gaussiana[2, p. 474]: \\[\\frac{\\mathrm{d} }{\\mathrm{d} x}e^{\\frac{-x^2}{2\\sigma^2}} = \\frac{-x}{\\sigma^2}e^{\\frac{-x^2}{2\\sigma^2}}\\] Canny demonstrou que a utilização dessa aproximação pode ser feita com uma taxa 20% inferior à solução numérica, o que a torna praticamente imperceptível para muitas das aplicações[2, p. 474]. A ideia anterior foi imaginada em um aspecto 1D, precisamos agora, expandir esse conceito para uma generalização 2D. Uma borda de degrau pode ser caracterizada pela sua posição, orientação e possível magnitude. Aplicar um filtro Gaussiano em uma imagem e depois diferenciá-la forma um simples e efetivo operador direcional[17, p. 145]. Digamos então que \\(f(x,y)\\) seja uma imagem e \\(G(x,y)\\) a função gaussiana: \\[G(x,y) = e^{-\\frac{x^2+y^2}{2\\sigma^2}}\\] Temos como saída a imagem suavizada: \\[f_s(x,y)=G(x,y)*f(x,y)\\] E após isso realizamos o cálculo da magnitude e direção do gradiente: \\[M(x,y) = \\sqrt{g_x^2+g_y^2}\\] \\[\\alpha(x,y)= \\tan^{-1}\\left ( \\frac{g_y}{g_x} \\right )\\] onde \\(g_x=\\partial f_s/\\partial x\\) e \\(g_y=\\partial f_s/\\partial y\\). Para o cálculo das derivadas parciais podemos utilizar tanto Prewitt quanto Sobel. Como essa primeira etapa utiliza operadores que calculam as primeiras derivadas, acabamos com bordas grossas, e o terceiro objetivo da proposta de Canny é ter bordas com único ponto, por isso o próximo passo é a de afinar as bordas encontradas. O método que usaremos para isso é chamado supressão dos não máximos. Esse processo tem como base a discretização das direções da normal da borda(vetor gradiente), ou seja, em uma região 3x3 temos 4 direções possíveis, como pode ser visto na figura 6.1(c), sendo que consideramos 4 pois é contando as duas direções, como exemplo, consideramos um borda de 45º se ela se encontra entre +157,5º e +112,5º ou -67,5º e -22,5º. Na figura 6.1(a) temos um exemplo de duas orientações que podem existir em uma borda horizontal, e na figura 6.1(b) podemos ver a normal de uma borda horizontal e o intervalo de valores onde a direção do vetor gradiente pode existir. Figura 6.1: Discretização das direções. (a)Borda horizontal. (b) Intervalo dos possíveis valores do ângulo, normal da borda, para uma borda horizontal. (c) Intervalo de valores do ângulo da normal para os diferentes tipos de borda. [2, p. 475] Se consideramos \\(d1\\), \\(d2\\), \\(d3\\) e \\(d4\\) como as direções possíveis em uma área 3x3, podemos formular o seguinte esquema de supressão de não máximos de uma região 3x3 centrada em todos os pontos \\((x,y)\\) de [2, p. 475]: - Encontrar a direção \\(d_k\\) que está mais perto de \\(\\alpha (x,y)\\). - Se o valor de \\(M(x,y)\\) for inferior a pelo menos um dos seus dois vizinhos ao logo de \\(d_k\\), deixe \\(g_N(x,y)=0\\)(supressão); caso contrário, deixe \\(g_N(x,y)=M(x,y)\\). Onde \\(g_N(x,y)\\) é a imagem suprimida. A última operação a ser realizada é a limiarização, para se remover os pontos de falsas bordas. Aqui usaremos a limiarização por histerese que utiliza dois limiares, um baixo(\\(T_L\\)) e um alto (\\(T_H\\)), sendo que Canny sugeriu, em seu trabalho, que a razão entre o limiar alto para o baixo deva ser de dois ou três para um. Podemos imaginar essa limiarização da seguinte forma, criamos duas imagens adicionais: \\[g_{NH}(x,y) = g_N(x,y)\\geq T_H\\] e \\[g_{NL}(x,y) = g_N(x,y)\\geq T_L\\] Onde \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) são definidas inicialmente como \\(0\\). Temos então que \\(g_{NH}(x,y)\\)conterá os pixels que são maiores que o nosso limiar e \\(g_{NL}(x,y)\\) terá os pixels que estão acima do nosso limiar baixo, o que significa que ele contém os pixels que se encontram no meio dos dois limiares mais o que está acima do limiar alto, temos então que remover esses pixels, o que significa: \\[g_{NL}(x,y)=g_{NL}(x,y)-g_{NH}(x,y)\\] Podemos chamar os pixels de \\(g_{NH}(x,y)\\) de pixels fortes e os de \\(g_{NL}(x,y)\\) de fracos. Ao final dessa limiarização todos os pixels fortes são classificados como borda válida, mas com falhas, que nos leva a outro processo: Localizar o próximo pixel borda a ser revisado em \\(g_{NH}(x,y)\\), chamaremos esse pixel de p. Classificar todos os pixels fracos de \\(g_{NL}(x,y)\\) que tenham conexão, como a conectividade-8, como bordas válidas. Quando todos os pixels de \\(g_{NL}(x,y)\\) Se forem analisados, pulamos para 4, senão voltamos para 1. Zerar todos os pixels de \\(g_{NL}(x,y)\\) que não são bordas válidas. Ao final desses processos teremos a imagem de saída do algoritmo de Canny. Como dito por [2, p. 476], o uso de duas imagens \\(g_{NH}(x,y)\\) e \\(g_{NL}(x,y)\\) é uma boa maneira para se explicar o algoritmo de uma maneira simples, mas na prática isso pode ser feito diretamente na imagem \\(g_N(x,y)\\). Por fim, sumarizando os passos do algoritmo, com um exemplo: Imagem original Figura 6.2: Imagem original. Aplicação do filtro gaussiano para suavizar a imagem. Figura 6.3: Imagem filtrada com filtro gaussiano. Cálculo da magnitude do gradiente e dos ângulos. Figura 6.4: (a) Sobel na direção vertical. (b) Sobel na direção horizontal. (c) Gradiente. (d) Angulos. [8, p. 98] Aplicação da supressão não máxima para afinar as bordas. Figura 6.5: Resultado da supressão não máxima. Usar limiarização por histerese e análise de conectividade para detectar e conectar as bordas. Figura 6.6: Resultado da histerese e conecção de bordas. Resultado final. Figura 6.7: Resultado final da detecção de bordas de Canny. 6.3 Transformada de Hough A Transformada de Hough é uma técnica utilizada para detectar formas em imagens, sejam elas linhas, círculos ou elipses. Apesar de ela ser muito utilizada e ter sido criada para detecção principalmente de linhas, ela pode ser usada para a detecção de outras formas, como dito anteriormente. 6.3.1 Transformada de Hough para detecção de linhas Para começar a entender essa transformada, imaginemos que temos um ponto \\((x_i, y_i)\\) no plano \\(xy\\) e a equação da reta \\(y_i=ax_i+b\\). Pelo ponto \\((x_i, y_i)\\) passam infinitas retas e todas satisfazem a equação. Podemos escrever a equação anterior em relação a \\(b\\), ou seja, \\(b=-x_ia+y_i\\), o que nos leva ao plano \\(ab\\)(espaço de parâmetros) onde essa nova equação gerará uma única reta. Agora imaginemos um outro ponto \\((x_j, y_j)\\) no plano \\(xy\\), podemos também levá-lo ao plano ab com a equação \\(b=-x_ja+y_j\\). Como podemos ver na figura 6.8(b) as duas retas geradas no plano \\(ab\\) se cruzam nas coordenadas \\((a&#39;, b&#39;)\\), e esse ponto de cruzamento representa a reta que cruza os dois pontos no plano \\(xy\\), como podemos ver na mesma representação 6.8(b). Na realidade, todos os pontos pertencentes a reta definida por esses dois pontos em \\(xy\\) tem sua reta respectiva em \\(ab\\) e todas elas se cruzam no ponto \\((a&#39;, b&#39;)\\), isso nos dá uma maneira de realizar a detecção de bordas, pois podemos imaginar essa reta como nossa borda, assim, para achá-la basta localizar o ponto no espaço de parâmetros onde um grande número de retas se cruzam. Figura 6.8: Plano xy e ab. [2, p. 483] Ocorre um pequeno problema nessa forma, pois quando a reta se aproxima da direção vertical, \\(ab\\) se aproxima do infinito. Para resolver essa dificuldade, em vez de levarmos os pontos a retas no espaço \\(ab\\) cartesiano utilizamos um espaço em coordenadas polares. Para isso utilizamos a seguinte equação: \\[\\rho=x\\cos{\\theta}+y\\ sen{\\ \\theta}\\] Na figura 6.9(a) podemos ver isso de maneira gráfica, temos que p corresponde à distância da origem até a reta. Cada uma das curvas senoidais da figura 6.9(b) representa um conjunto de linhas que cruzam os dois pontos da figura 6.9(a), sendo que na interseção das curvas temos a reta que cruza esses pontos. Figura 6.9: Imagem de ônibus com filtro de aguçamento e de suavização [8, p. 98] A figura 6.9(c) mostra como fazemos a representação do espaço , usamos uma matriz onde esse espaço é subdividido em várias células, chamadas células acumuladoras. Os valores de \\(\\theta_{\\text{min}}\\) e \\(\\theta_{\\text{max}}\\) são geralmente \\(-90^{\\circ}\\leq \\theta\\leq90^{\\circ}\\) e os valores de \\(\\rho_min\\) e \\(\\rho_max\\) são \\(-D\\leq\\rho\\leq D\\), onde \\(D\\) é o comprimento da diagonal da imagem, ou seja, \\(D=\\sqrt{vertical^2+horizontal^2}\\). O que fazemos então é andar por todos os pontos de borda da imagem de entrada e calcular o valor de a partir da equação apresentada anteriormente usando o valor de \\((x,y)\\) e variando o ângulo , com isso a cada valor do ângulo teremos um diferente e somamos mais um na célula correspondente da matriz acumuladora, que inicialmente é toda preenchida com zeros. Ao final de todo o processo termos determinadas células com valores mais altos, essas são conhecidas como picos e correspondem ao cruzamento de duas ou mais curvas senoidais do plano o que corresponde a uma linha ligando pontos no plano \\(xy\\). A seguir temos um exemplo, que nos ajuda a entender e ver na prática o funcionamento da transformada de Hough. A figura 6.10(a) contém uma imagem de tamanho 101x101 com um ponto no centro, ou seja \\((x,y)=(50,50)\\) e a figura 6.10(b) contém a matriz acumuladora da transformada, onde podemos ver a curva senóide formada pelo ponto. Verificando os valores nela vemos que para \\(\\rho=-90^{\\circ}\\) temos: \\[\\rho=50 \\cdot \\cos(-90^{\\circ})+50\\cdot\\text{sen}(-90^{\\circ}) = -50\\] para \\(\\theta=90^{\\circ}\\) temos: \\[\\rho=50\\cdot \\cos(90^{\\circ})+50\\cdot\\text{sen}(90^{\\circ})=50\\] para \\(\\theta=45^{\\circ}\\) temos: \\[\\rho=50\\cdot \\cos(45^{\\circ})+50\\cdot \\text{sen}(45^{\\circ})\\approx70,71\\] e para \\(\\theta=-45^{\\circ}\\) temos: \\[\\rho=50\\cdot\\cos(-45º)+50\\cdot \\text{sen}(-45º)=0\\] Figura 6.10: Transformada de Hough para um ponto. Na figura 6.11(a) temos dois pontos, a e b, onde foi realizada a transformada de Hough que tem como espaço de saída a figura 6.11(b). A reta que passa por esses dois pontos, chamada de reta c é representada por uma reta pontilhada na figura 6.11(a) e na figura 6.11(b) temos o ponto no plano que representa essa reta, ou seja, uma reta a uma distância \\(\\rho\\approx70,71\\) da origem com ângulo de \\(45^{\\circ}\\). Figura 6.11: Transformada de Hough para um ponto em 45º. Na figura 6.12(a) temos mais um exemplo, desta vez com um ponto localizado a sua direita, diferentemente da anterior, esses dois pontos formam uma reta de \\(-45^{\\circ}\\), fato que pode ser visto na figura 6.12(b) onde o ponto de encontro das duas curvas acontece em \\(\\theta=-45^{\\circ}\\) com um valor de \\(\\rho=0\\) já que a reta cruza a origem, ou seja, não possui distância em relação a ela. Figura 6.12: Transformada de Hough para um ponto em -45º. Nosso último exemplo contém uma imagem com três pontos, onde temos três tipos de retas possíveis. Observando a figura 6.13(a) podemos ver os pontos a, b e c e as retas que passam por eles d, e e f, e na figura 6.13(b) temos a transformada de Hough para esse imagem, algo interessante de se notar é o fato de a reta que passa pelos pontos b e c ser detectada duas vezes, isso se deve a uma característica da transformada de Hough chamada relação de adjacência reflexiva, ou seja, isso acontece como resultado pela maneira como \\(\\rho\\) e \\(\\theta\\) mudam de sinal quando chegamos as extremidades de \\(\\pm90^{\\circ}\\). Figura 6.13: Transformada de Hough para três pontos. Na figura 6.14 temos nosso último exemplo na detecção de linhas, dessa vez realizado em uma imagem real, neste caso primeiramente foi realizado a detecção de bordas pelo método de Canny, como pode ser visto na figura 6.14(a). Logo após foi realizada a transformação de Hough, com resultado em figura 6.14(b) e por fim temos a imagem original com as linhas detectadas em figura 6.14(c). Atenção ao fato de que nem todos os picos da transformada podem ser utilizados como linhas, pois teríamos um número enorme delas, para isso utilizamos um threshold, utilizando somente as linhas que tiverem o número de votos(acumulação na matriz) superior a um valor limítrofe. Figura 6.14: Resultado da transformada de Hough usada na detecção de linhas em uma imagem. 6.3.2 Transformada de Hough para detecção de círculos A transformada de Hough pode ser estendida para detecção de círculos, para isso substituímos a equação da reta pela equação do círculo: \\[(x-x_0)^2+(y-y_0)^2=r^2\\] Nesse caso também andamos por cada pixel das bordas da imagem e o levamos ao espaço de parâmetro com as seguintes equações: \\[x_0=x-r\\cos(\\theta)\\] e \\[y_0=y-\\text{sen}(\\theta)\\] A diferença é que neste caso o nosso espaço de parâmetro terá três dimensões, isso decorre do fato de que como desenhamos um círculo para cada pixel do círculo da imagem, a variação do diâmetro desse círculo deve levar a uma variação dos círculos descritos no espaço de parâmetros, então além da variação dos valores de \\(x\\) e \\(y\\) também devemos variar os valores de \\(r\\). Uma representação disso pode ser vista na figura 6.15(a) onde temos três pixels que definem um círculo, na figura 6.15(b) temos os círculos no espaço de parâmetros e na figura 6.15(c) podemos ver um representação de um espaço de parâmetros com diferentes raios. Figura 6.15: Transformada de Hough para círculos.[18, p. 255] A figura 6.16 contém uma imagem com algumas moedas, na figura 6.17(a) temos as bordas da imagem detectada com o método de Canny, logo após, na figura 6.17(b) - (f) temos a representação do espaço de Hough para diferentes valores de raio. E na figura 6.18 temos o resultado da detecção de círculo após encontrados os picos do espaço de parâmetros. Figura 6.16: Imagem original de moedas. [8, p. 98] Figura 6.17: Canny e espaço de parâmetros. [8, p. 98] Figura 6.18: Resultado final da transformada de Hough para círculos. [8, p. 98] 6.4 Detecção de blobs Blobs, do inglês bolhas, são regiões da imagem em que os pixels têm valores aproximadamente iguais. Uma boa representação - um tanto quanto artificial - disso é a função gaussiana, como pode ser vista na figura 6.19(a) e sua representação 2D na figura 6.19(b), nela temos um conjunto de pixels com valores bem próximos, que caracterizam um blob. Figura 6.19: Função gaussiana em 3D e 2D. Apesar do exemplo, a detecção de blobs não se restringe a elementos circulares, mas a qualquer conjunto de pixels. 6.4.1 LoG Esse método utiliza o do Laplaciano do Gaussiano, que já foi apresentado anteriormente, mas que em resumo é o cálculo de derivadas segunda em uma imagem que foi anteriormente convolucionada com um filtro gaussiano, isso irá gerar fortes respostas positivas em blobs escuros e negativas em blobs escuros nos blobs de tamanho \\(\\sqrt{2\\sigma}\\). Como existe uma relação entre a respostas e o tamanho do desvio padrão, é necessário realizar a operação com uma gama de valores para o sigma, e assim detectar blobs de diferentes tamanhos. Figura 6.20: Imagem de Campo Ultraprofundo do Hubble. [19] Como podemos ver na figura 6.19 com diferentes valores de sigma conseguimos detectar objetos de variados tamanhos, como exemplo na figura 6.21(a) detectamos as estrelas da figura 6.20 que apresentam uma menor resposta ao filtro laplaciano. Figura 6.21: Laplaciano do Gaussiano com diferentes valores de sigma. Na figura 6.22 temos o resultado da detecção dos blobs utilizando LoG. Note que nem todas as estrelas foram detectadas, isso se deve ao fato do uso de um valor de threshold, onde definimos que queremos as detecções acima de determinado limiar. Na figura 6.23 podemos ver o resultado utilizando um valor de limiar menor, onde muito mais objetos foram localizados. Figura 6.22: Resultado da detecção de blobs com LoG. Figura 6.23: Resultado da detecção de blobs com LoG com um threshold menor. 6.4.2 DoG Esse método é basicamente o mesmo do anterior, mas possui uma certa vantagem, que é o fato de ele ser mais eficiente. Como também já foi mencionado no tópico na seção anterior é possível aproximar o Laplaciano do Gaussiano através da Diferença do Gaussiano(DoG), ou seja, primeiramente se realiza a filtragem gaussiano com dois sigmas diferentes e se faz a subtração entre os dois. Realizamos esse processo para diferentes pares de valores, obtendo assim o mesmo espaço de escala construído com o processo do LoG. Na figura 6.24 temos um exemplo de detecção por DoG. Figura 6.24: Resultado da detecção de blobs com DoG. 6.4.3 DoH Uma matriz Hessiana é uma matriz que contém as derivadas de uma função. No nosso caso, utilizamos a Hessiana de ordem 2, pois estamos trabalhando com imagens, que possuem duas dimensões. Ela pode ser representada da seguinte maneira: \\[H[f(x_1, x_2, \\dots,x_n)]= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}\\] A matriz Hessiana tem muita utilidade pois com ela podemos descrever a curvatura em um ponto da função multivariável, o que no nosso caso pode ajudar a detectar os blobs, já que eles são aglomerados de pixels e devem estar separados do restante da imagem, ou seja, um aglomerado claro em um fundo escuro ou o contrário, e isso irá fazer com que sua função tenha uma mudança de sinal que pode ser detectada utilizando-se as informações da matriz. Além disso, como dito por Herbert Bay et al.[20] os detectores baseados na Hessiana são mais estáveis e repetíveis(tem a mesma resposta para a mesma imagem com diferentes ângulos, iluminações etc.). Um dos principais algoritmos que fazem uso dessa matriz se chama Speeded Up Robust Features (SURF)[20], esse método faz uso de várias técnicas que o tornam muito rápido, como seu próprio nome sugere. Uma dessas técnicas é o cálculo da integral da imagem, realizado a partir da soma de todos os pixels de uma área retangular a partir do x atual, sendo que este varia enquanto se é andado pela imagem. Figura 6.25: Integral de uma imagem. [21] Como pode ser visto na figura, a integral de uma imagem contém a soma das regiões, por exemplo, a primeira posição contém a soma de somente uma célula, no caso 1, a segunda tem a soma de duas células, na primeira linha estamos basicamente somando as células de uma só linha, na segunda começamos a formar regiões retangulares, por exemplo, na segunda linha e terceira coluna temos o valor 6, resultante da soma das seis células da primeira linha com a segunda. Com a integral podemos calcular a área de qualquer região com apenas quatro operações, da seguinte forma: \\[soma = D+A-B-C\\] Onde {A,B,C,D} forma uma região. Como exemplo, caso queiramos calcular a área na região quadrada 2x2 na direita inferior utilizados: \\[soma = 9 + 1 - 3 - 3 = 4\\] Isso nos ajuda na aplicação de box filters, já que precisaríamos da soma de determinadas áreas, e com isso aumentamos a velocidade do método. Sendo \\(X=(x,y)\\) um ponto em uma imagem, sua matriz Hessiana em \\(X\\) a uma escala é dada por: \\[H(X,\\sigma)=\\begin{bmatrix} L_{xx}(X, \\sigma) &amp; L_{xy}(X, \\sigma)\\\\ L_{xy}(X, \\sigma) &amp; L_{yy}(X, \\sigma) \\end{bmatrix}\\] Onde \\(L_{xx}(X, \\sigma)\\) é a convolução da imagem no ponto X com a derivada de segunda ordem gaussiana \\(\\frac{\\partial^2g(\\sigma)}{\\partial x^²}\\) e assim por diante[20]. Aqui entra em cena mais um elemento para melhorar a velocidade do algoritmo, Bay, Herbert et al. utilizam box filters para aproximar o filtro gaussiano. Como podemos ver na figura, onde os dois primeiros filtros são os derivativos gaussianos discretizados e os dois últimos são os aproximados a partir de box filters. Figura 6.26: Filtro gaussiano discretizado e aproximado na direção \\(y\\) e \\(xy\\). [20] Chamamos as derivadas realizadas na imagem de \\(D_{xx}\\), \\(D_{yy}\\), \\(D_{xy}\\). Essas derivadas não são realizadas com somente um valor de \\(\\sigma\\), mas como os detectores anteriores usam uma sequência de valores para assim criar um espaço de escalas e conseguir detectar blobs de diferentes tamanhos. Assim, a determinante da Hessiana é dado por: \\[det(H_{\\text{aprox}}) = D_{xx}D_{yy}-(0.9D_{xy})^2\\] Sendo que o valor \\(0.9\\) é um peso introduzido pelos autores Bay, Hebert et al. para corrigir as respostas quando utilizamos várias escalas de sigma e obter uma invariância escalar. Na figura temos o resultado de uma detecção de blobs realizada pela Determinante do Hessiano. Figura 6.27: Resultado da detecção de blobs com DoH. 6.5 Detecção de junções ou cantos 6.6 Segmentação por limiarização 6.7 método de Otsu 6.8 Segmentação usando watersheds morfológicas Refêrencias "],["refêrencias.html", "Refêrencias", " Refêrencias "]]
