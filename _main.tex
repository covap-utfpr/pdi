% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  brazilian,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Material introdutório de Processamento Digital de Imagens(PDI)},
  pdflang={pt-BR},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[variant=brazilian]{portuguese}
\else
  \usepackage[main=brazilian]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{book.bib}

\title{Material introdutório de Processamento Digital de Imagens(PDI)}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{inicio}{%
\chapter*{Inicio}\label{inicio}}
\addcontentsline{toc}{chapter}{Inicio}

Esse site foi desenvolvido pelo grupo \textbf{COVAP} (Computação Visual Aplicada) visando oferecer um material introdutório da disciplina de PDI para alunos e pessoas interessadas no assunto.
Os conteúdos se encontram dividos na seguinte estrutura:

\begin{itemize}
\tightlist
\item
  \href{introducao.md}{Princípios básicos}
\item
  \href{formacao_imagem.md}{Formação da imagem}
\item
  \href{tipos_arquivos.md}{Tipos de arquivo}
\item
  \href{espaco_cores.md}{Espaço de cores}
\item
  \href{transformacoes.md}{Transformações}
\item
  \href{compressao_imagem.md}{Compressão de imagens}
\item
  \href{filtros.md}{Filtros}
\item
  \href{detectores.md}{Detectores}
\item
  \href{morfologia.md}{Morfologia matemática}
\end{itemize}

Lembramos ainda que todo o material está hospedado no \href{https://github.com/GilsonJRS/covap-pdi}{Github}, agradeçemos então todo apontamento de erros e sugestões, para que assim consigamos sempre disponibilizar um conteudo revisado e livre de quaisquer erros.

\hypertarget{intro}{%
\chapter{Introdução}\label{intro}}

\hypertarget{relauxe7uxe3o-de-processamento-digital-de-imagem-visuxe3o-computacional-e-computauxe7uxe3o-gruxe1fica}{%
\section{Relação de Processamento Digital de Imagem, Visão Computacional e Computação Gráfica}\label{relauxe7uxe3o-de-processamento-digital-de-imagem-visuxe3o-computacional-e-computauxe7uxe3o-gruxe1fica}}

A visão desempenha um papel importante na vida das pessoas, pois com ela é possível uma percepção incrivelmente rica do mundo ao seu redor. Para tentar reproduzir as capacidades visuais humanas por sistemas autônomos manipulados por computadores foram desenvolvidas pelo menos três grandes áreas \autocite[p.~2]{velho2009}: \emph{Processamento Digital de Imagens} (PDI), \emph{Visão Computacional} (VC) e a \emph{Computação Gráfica}(CG), apresentados na Figura \ref{fig:areasPDI}. Essas áreas, apesar de serem correlacionadas, têm objetivos e métodos diferentes, por isso a importância de distingui-las.



\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{imagens/01-introducao/areasPDI} 

}

\caption{Processos Computacionais com Imagens \autocite[p.~2]{velho2009}.}\label{fig:areasPDI}
\end{figure}

O \emph{Processamento Digital de Imagens} (PDI) busca realizar o pré-processamento das imagens, utilizando para isso técnicas de tratamento, como a correção da iluminação, eliminação de ruído, e a segmentação. O foco da \emph{Visão Computacional} (VC) é a análise das imagens, identificando os seus componentes e obtendo informações. Diferente da VC, em que as imagens são os objetos de estudo, na \emph{Computação Gráfica} (CG), as imagens são o resultado do processo. Na CG são geradas representações visuais seguindo descrições e especificações geométricas \autocite[p.~3]{velho2009}.

A Tabela \ref{tab:processos} apresenta de forma resumida as diferenças entre PDI, VC e CG. Na segunda linha da tabela está uma descrição simples de cada área, e na terceira linha um esquema identificando o objeto e o produto de cada processo.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.34}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.32}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.34}}@{}}
\caption{\label{tab:processos} Processos Computacionais com Imagens.}\tabularnewline
\toprule
Computação Gráfica (CG) & Visão Computacional (VC) & Processamento Digital de Imagens (PDI) \\
\midrule
\endfirsthead
\toprule
Computação Gráfica (CG) & Visão Computacional (VC) & Processamento Digital de Imagens (PDI) \\
\midrule
\endhead
Cria e altera imagens a partir de dados. & Análise de imagem para criação de modelos. & Transformação de imagem (tratamento). \\
modelo → imagem & imagem → modelo & imagem → imagem \\
\bottomrule
\end{longtable}

As imagens tratadas em PDI têm como uma das finalidades servir de material para a Visão Computacional, como identificado na Figura \ref{fig:areasPDI}. Muitas vezes as áreas de Visão Computacional e PDI são confundidas devido a dificuldade em se definir em que ponto uma termina e a outra começa. Mesmo não existindo uma linha clara entre os limites destas duas áreas é possível utilizar um paradigma que considera três níveis de processamento \autocite[p.~2]{gonzalez2010}:

\begin{itemize}
\item
  \textbf{Baixo nível}

  A nível de pixel, realiza operações de pré-processamento, sendo utilizada, por exemplo, na redução de ruído, aumento de contraste e restauração.
\item
  \textbf{Médio nível}

  Operações mais complexas, como segmentação, partição e reconhecimento de objetos individuais. Entrada é uma imagem mas a saída pode ser um conjunto contendo os atributos extraídos das imagens.
\item
  \textbf{Alto nível}

  Interpretação do conteúdo da imagem e análise.
\end{itemize}

Baseado nesses níveis, iremos considerar que o processamento de imagem atua nos primeiros dois níveis, ou seja, envolve o pré-processamento e processos de extração de elementos de imagens até o reconhecimento de componentes individuais. Como o foco deste material é o Processamento de Imagens Digitais (PDI), estes dois níveis serão apresentados com detalhes nos próximos tópicos, mas primeiro vejamos alguns exemplos de aplicações do PDI.

\hypertarget{aplicauxe7uxf5es-processamento-digital-de-imagens}{%
\section{Aplicações Processamento Digital de Imagens}\label{aplicauxe7uxf5es-processamento-digital-de-imagens}}

As primeiras tarefas de processamento de imagens tiveram aplicações significativas por volta da década de 1960, quando se desenvolveram computadores com potencial suficiente para realizá-las. O programa espacial americano também foi um forte impulso para o contínuo desenvolvimento e aprimoramento das técnicas de processamento digital de imagem (PDI), já que imagens, como as obtidas da Lua através de sondas e transmitidas à terra, continham distorções provenientes das câmeras utilizadas. Era necessário então, a utilização de métodos para corrigir essas alterações \autocite[p.~4]{gonzalez2010}.

Outra área que também faz uso extensivo do processamento de imagens e impulsionou seu desenvolvimento é a área médica. Nessa área, o uso de imagens auxiliou no diagnóstico de doenças através de exames visuais como os de raio-x \autocite[p.~4]{gonzalez2010}.

A utilização do processamento de imagens para melhorar informações visuais, ajudando na interpretação humana, expandiu-se para diferentes setores. No sensoriamento remoto, o pré-processamento contribui para uma melhor análise de imagens aéreas e de satélite, aumentando a compreensão da superfície terrestre. Na arqueologia e nas artes, métodos de processamento de imagens podem restaurar fotografias com registros únicos de objetos raros, pinturas, documentos antigos e conteúdos em vídeos \autocite[p.~2]{pedrini2008}. Na física e em áreas da biologia, técnicas computacionais realçam imagens de experimentos em áreas como plasmas de alta energia e microscopia eletrônica \autocite[p.~5]{gonzalez2010}.

Com o aumento da automatização de tarefas, o processamento de imagens tem se destacado na aquisição de dados de imagens visando a percepção automática por máquinas \autocite[p.~3]{pedrini2008}. Técnicas de identificação de padrões podem ser aplicados no reconhecimento automático de caracteres, de impressões digitais, de faces, e de placas de veículos, contribuindo com setores de segurança. Na automação industrial tem sido utilizado no sistema de visão computacional para inspeção e montagem de produtos. Na área militar, pode ser aplicado na identificação e rastreamento de alvos em imagens de satélites, e na navegação de veículos autônomos. Nas áreas de medicina e biologia, rastreamentos automáticos em imagens radiográficas e amostras de sangue têm contribuído para os exames e testes \autocite[p.~3]{pedrini2008}. O processamento computacional de imagens aéreas e de satélites também é utilizado na previsão do tempo e em avaliações ambientais \autocite[p.~5]{gonzalez2010}.

Este variado campo de aplicações pode ser justificado pela capacidade dos aparelhos de processamento de imagens trabalharem com imagens de diversas fontes. Diferentemente dos seres humanos, que são limitados à banda visual do espectro eletromagnético (EM), o processamento computacional cobre todo o EM, variando de ondas gama a ondas de rádio \autocite[p.~1]{gonzalez2010}. No processamento digital ainda é possível trabalhar com imagens geradas por fontes que os humanos não estão acostumados a associar com imagens. Essas fontes incluem acústica, ultrassom, microscopia eletrônica e imagens geradas por computador \autocite[p.~13]{gonzalez2010}.

Uma das formas mais fáceis de desenvolver uma compreensão básica da extensão das aplicações do processamento de imagens é categorizar as imagens de acordo com sua fonte. Na Figura \ref{fig:espectro} temos uma representação do EM, iremos a seguir explorar cada uma dessa faixas, apresentando algumas das áreas onde podem ser utilizados:



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{imagens/01-introducao/espectro} 

}

\caption{Espectro eletromagnético \autocite{img:espectro}.}\label{fig:espectro}
\end{figure}

\begin{itemize}
\item
  \textbf{Imagens formadas por raios gama}

  As imagens formadas a partir de raios gama têm diferentes utilidades, sendo muito utilizadas na medicina e astronomia \autocite[p.~6]{gonzalez2010}. Na medicina, existem procedimentos onde se injetam isótopos radioativos no paciente e por meio dos detectores de raio gama é formada uma imagem, como exemplo, escaneamento ósseo e tomografia por emissão de pósitrons (PET-scan). Na astronomia, ela pode ser utilizada para se conseguir ver detalhes astronômicos que estão presentes na faixa eletromagnética dos raios gama.
\item
  \textbf{Imagens formadas por raios X}

  Imagens formadas a partir de raio X têm uma ampla gama de aplicações, desde seu uso na medicina até seu uso no meio industrial \autocite[p.~6]{gonzalez2010}. Na indústria, pode ser utilizado para se encontrar defeitos de fabricação em produtos, e na medicina, vêm se utilizando muito o processamento de imagem e a visão computacional para ajudar no diagnóstico de doenças, como por exemplo, artérias obstruídas, fraturas e tumores.
\item
  \textbf{Imagens na banda ultravioleta}

  O espectro ultravioleta também tem inúmeras aplicações, como a inspeção industrial, microscopia, imagens biológicas e observações astronômicas \autocite[p.~8]{gonzalez2010}.
\item
  \textbf{Imagens na banda visível e infravermelho}

  Essas duas bandas possuem uma gama extremamente ampla de aplicações, sendo utilizadas juntas ou separadas. Na banda visível, existem diversas aplicações, como em processos industriais, detecção de faces, detecção de placas de carros, etc \autocite[p.~11]{gonzalez2010}. A banda infravermelho também possui inúmeras aplicações, sendo uma delas imagens a partir de satélites, onde o infravermelho nos permite ver inúmeros detalhes que somente com a banda visível não seria possível \autocite[p.~9]{gonzalez2010}.
\item
  \textbf{Imagens na banda de micro-ondas e rádio}

  Na banda de micro-ondas o melhor exemplo que temos é o radar. Essa banda tem uma peculiaridade de ser extremamente penetrante, podendo gerar imagens através de nuvens, vegetação, etc \autocite[p.~12]{gonzalez2010}. Já a banda de rádio é muito utilizada na medicina, como exemplo na ressonância magnética e na astronomia \autocite[p.~12]{gonzalez2010}.
\end{itemize}

Como podemos observar, existem inúmeras maneiras de se conseguir imagens além da clássica imagem no espectro visível, isso nos dá a possibilidade de utilizar o PDI em inúmeras áreas e problemas. Na Figura \ref{fig:aplicacoes} temos uma nebulosa observada a partir de diferentes bandas dos EM, sendo possível observar detalhes que passariam despercebidos se usássemos somente alguma delas.



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{imagens/01-introducao/aplicacoes} 

}

\caption{Nebulosa CRAB em diferentes frequências \autocite{img:nebulosa}.}\label{fig:aplicacoes}
\end{figure}

\hypertarget{etapas-do-processamento-e-anuxe1lise-de-imagens}{%
\section{Etapas do Processamento e Análise de Imagens}\label{etapas-do-processamento-e-anuxe1lise-de-imagens}}

Um dos objetivos deste material é servir de referência para o estudo inicial de Visão computacional, assim, para compreender as relações entre as etapas de processamento e análise de imagens apresentamos na Figura \ref{fig:etapasPDI} uma sequência dos principais passos utilizados em uma aplicação de PDI. Neste material nos deteremos nos conteúdos de processamento, desde a aquisição de imagens, pré-processamento até segmentação.



\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{imagens/01-introducao/etapasPDI} 

}

\caption{Etapas de aplicação de PDI \autocite[p.~4]{pedrini2008}.}\label{fig:etapasPDI}
\end{figure}

Vale ressaltar que essas etapas não são fixas e podem ser modificadas, sendo que uma base de conhecimentos é importante para orientar em uma aplicação específica \autocite[p.~4]{pedrini2008}.

\begin{itemize}
\item
  \textbf{Aquisição da imagem}

  Captura a imagem por meio de um dispositivo ou sensor e a converte em uma imagem digitalizada \autocite[p.~3]{pedrini2008}. Podemos citar como exemplo as câmeras fotográficas, tomógrafos médicos, satélites e scanners. Na Figura \ref{fig:aquisicao}, temos um exemplo de aquisição de imagens do satélite Landsat, neste caso estão identificadas as bandas vermelha, verde e azul visíveis e o infravermelho próximo. Os detalhes sobre a aquisição de imagens serão discutidos no tópico Formação de Imagem.
\end{itemize}



\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{imagens/01-introducao/aquisicao} 

}

\caption{Aquisição de imagens de satélite.(a) a (d) mostram quatro imagens espectrais de satélite da cidade de Washington, D.C., banda vermelha, verde e azul visíveis e o infravermelho próximo, respectivamente. (e) Imagem colorida como combinação RGB de (a), (b) e (c). (f) Imagem colorida obtida pela combinação de (b), (c) e (d)\autocite[p.~279]{gonzalez2010}.}\label{fig:aquisicao}
\end{figure}

\begin{itemize}
\item
  \textbf{Pré processamento}

  Essa etapa busca realizar mudanças e ajustes na imagem visando melhorar seu uso nas etapas futuras \autocite[p.~3]{pedrini2008}. Como exemplo temos casos onde não precisamos das cores de uma imagem, podendo então realizar a conversão para grayscale(tons de cinza), ou precisamos gerar imagens coloridas como na Figura \ref{fig:aquisicao} em que são combinadas as bandas espectrais. Além disso, podemos realizar cortes ou realces, isolando somente a parte de maior interesse na imagem como na Figura \ref{fig:preProcessamento}, ou também atenuar o ruído na imagem, além de outras técnicas que serão abordadas em outros tópicos.
\end{itemize}



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/01-introducao/preProcessamento} 

}

\caption{Subtração de imagens para realce de diferenças.(a) Imagem da área de Washington D.C em infravermelho. (b) Resultado ao zerar o bit menos significativo de todos os pixels de (a). (c) Diferença entre as duas imagens ajustada para a faixa {[}0, 255{]}, sendo que valores em preto (0) indicam pontos nos quais não há nenhuma diferença \autocite[p.~49]{gonzalez2010}.}\label{fig:preProcessamento}
\end{figure}

\begin{itemize}
\item
  \textbf{Segmentação}

  Nessa etapa as informações de interesse são extraídas da imagem, geralmente, pela detecção de descontinuidades (bordas) ou de similaridades na imagem \autocite[p.~4]{pedrini2008}. Na Figura \ref{fig:segmentacao} é mostrado o resultado de um exemplo de segmentação por similaridade, em que o elemento de maior interesse é o rio.
\end{itemize}



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{imagens/01-introducao/segmentacao} 

}

\caption{Extração de características de uma imagem segmentada.(a) Imagem na banda infravermelha da área de Washington, D.C. (b) Segmentação da imagem por limiarização. (c) O maior componente conexo de (b). Técnica de representação por esqueleto de (c) \autocite[p.~544]{gonzalez2010}.}\label{fig:segmentacao}
\end{figure}

\begin{itemize}
\item
  \textbf{Representação e Descrição}

  Armazenar e manipular objetos de interesse extraídos da imagem. O processo de descrição visa a extração de características para discriminar classes de objetos \autocite[p.~4]{pedrini2008}. Na Figura \ref{fig:analise}, o objetivo é determinar o tamanho das ramificações do rio, para isto considerou-se que o tamanho de cada ramificação no esqueleto seria uma boa aproximação \autocite[p.~545]{gonzalez2010}. O esqueleto é uma representação do rio, e seus elementos são discriminados dentro do maior componente conexo da imagem segmentada.
\item
  \textbf{Reconhecimento e Interpretação}

  Essa etapa examina as informações produzidas na etapa anterior e classifica cada objeto como sendo de interesse ou não, atribuindo significado ao conjunto de objetos reconhecidos pelos rótulos \autocite[p.~3]{pedrini2008}. Uma aplicação de análise de imagens inclui a classificação de áreas em uma imagem multiespectral como na Figura \ref{fig:analise}. Neste exemplo utilizou-se o método bayesiano, em que cada pixel da imagem foi avaliado em relação a três classes (água, desenvolvimento urbano e vegetação). Nas Figuras \ref{fig:analise}, pontos pretos representam pontos classificados incorretamente, enquanto pontos brancos foram classificados corretamente \autocite[p.~579]{gonzalez2010}.
\end{itemize}



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/01-introducao/analise} 

}

\caption{Classificação bayesiana em uma imagem multiespectral. Resultado (em branco) da classificação na classe água, desenvolvimento urbano e vegetação, da esquerda para a direita \autocite[p.~579]{gonzalez2010}.}\label{fig:analise}
\end{figure}

\hypertarget{formauxe7uxe3o-da-imagem}{%
\chapter{Formação da imagem}\label{formauxe7uxe3o-da-imagem}}

Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é do espectro eletromagnético, mas podendo ser também, a partir da energia mecânica (ultrassom), feixe de elétrons, etc.. Cada fonte necessita de um método específico de captura, para algumas pode ser uma câmera fotográfica, porém a outras é necessário que o computador sintetize a imagem, como o microscópio eletrônico. Como já mencionado no tópico de introdução, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensitivos a essa faixa de luzes, que vêm da luz solar e nos ajuda a realizar nossas atividades. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas, como a ultravioleta\autocite[p.2]{cuthill2017}. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas\autocite[p.8]{moeslund2012}.

A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de 0,43 a 0,79 \(\mu m\). Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar\autocite[p.28]{gonzalez2010}.

Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir.

\hypertarget{cuxe2mera-pinhole-e-geometria}{%
\section{Câmera pinhole e geometria}\label{cuxe2mera-pinhole-e-geometria}}

Na figura \ref{fig:aquisicaoimagem} temos um esquema básico de como geralmente ocorre a aquisição de imagens, primeiramente a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida, sendo após isso capturada por um dispositivo, como uma câmera.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/aquisicao_imagem} 

}

\caption{Representação de uma típica captura de imagem \autocite[p.8]{moeslund2012}.}\label{fig:aquisicaoimagem}
\end{figure}

Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, este é conhecido como câmara pinhole(do inglês buraco de alfinete) ou câmara escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na figura \ref{fig:barreiraluz}, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruim. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/barreiraluz} 

}

\caption{Introdução de barreira para captura de imagem \autocite[p.11]{moeslund2012}.}\label{fig:barreiraluz}
\end{figure}

Na figura \ref{fig:barreiraluz} percebemos que a imagem resultante acaba invertida, isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/geometriapinhole} 

}

\caption{Geometria de uma câmera pinhole \autocite[p.5]{burger2009}.}\label{fig:geometriapinhole}
\end{figure}

Na figura \ref{fig:geometriapinhole}, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância \(Z\) da abertura e a uma distância \(Y\) o eixo óptico, podemos definir a altura \(y\) e a largura \(x\) da projeção do objeto utilizando a simetria de triângulos:

\[-\frac{y}{f}=\frac{Y}{Z}\Leftrightarrow y=-f\frac{Y}{Z} \text{ e } -\frac{x}{f}=\frac{x}{f} \Leftrightarrow x=-f\frac{X}{Z}\]

A variável \(f\) nessa equação se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera. Os sinais negativos das equações significam que a imagem projetada está rotacionada a 180º verticalmente e horizontalmente, como podemos confirmar na imagem acima. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens, como precisar de um longo tempo de exposição para captura da imagem.

\hypertarget{lentes}{%
\section{Lentes}\label{lentes}}

As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/lente} 

}

\caption{Ação de uma lente sobre os raios de luz \autocite[p.12]{moeslund2012}.}\label{fig:lente}
\end{figure}

Como podemos ver na figura \ref{fig:lente}, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando-se o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada.
O ponto \(F\) onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância \(f\), que vai do centro óptico \(O\) até \(F\) é conhecida como Distância Focal.
Definindo a distância do objeto real até a lente como g e a distância até a formação da imagem após passa pela lente como b temos que:

\[\frac{1}{g}+\frac{1}{b}=\frac{1}{f}\]

Como \(f\) e \(b\) estão normalmente entre 1 mm e 100mm isso mostra que \(\frac{1}{g}\) não tem quase nenhum impacto na equação e significa que \(b = f\). Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal.
Outro ponto importante das lentes é conhecido como zoom óptico. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, \(B\), aumenta quando \(f\) aumenta. Podemos representar isso na seguinte equação, onde \(g\) é o tamanho real do objeto:

\[\frac{b}{B}=\frac{g}{G}\]

Na prática \(f\) é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos.
Se o \(f\) for constante, quando alteramos a distância do objeto, no caso \(g\), sabemos que \(b\) também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos \(b\) temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando \(b\) para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/foco} 

}

\caption{Uma imagem focada e desfocada \autocite[p.11]{moeslund2012}.}\label{fig:foco}
\end{figure}

A figura \ref{fig:foco} ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos.



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/profundidade} 

}

\caption{Profundidade de campo \autocite[p.13]{moeslund2012}.}\label{fig:profundidade}
\end{figure}

A figura \ref{fig:profundidade} apresenta outro ponto muito importante, chamado Profundidade de Campo(Depth of field), que representa a soma das distâncias \(g_l\) e \(g_r\), que representam o quando os objetos podem ser movidos e permanecerem em foco.

Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão(Field of View ou FOV) que representa a área observável de uma câmera. Na figura \ref{fig:campovisao} essa área observável é denotada pelo ângulo \(V\). O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as equações seguintes para o FOV vertical e horizontal:
\[FOV_x = 2*\tan^{-1}\left(\frac{\frac{comprimento\ do\ sensor}{2}}{f}\right) \text{ e }  FOV_y = 2*\tan^{-1}\left(\frac{\frac{altura\ do\ sensor}{2}}{f}\right)\]



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/campovisao} 

}

\caption{Campo de visão \autocite[p.14]{moeslund2012}.}\label{fig:campovisao}
\end{figure}

Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de 14mm, altura de 10mm e uma distância focal de 5mm temos:
\[FOV_x=2*tan^{-1}\left(\frac{7}{5}\right)=108.0^{\circ} \text{ e } FOV_y=2*tan^{-1}(1)=90^{\circ}\]

Isso significa que essa câmera tem uma área observal de 108.9º horizontalmente e 90º verticalmente. Na figura \ref{fig:diferentesprofundidades} temos o mesmo objeto fotografado com diferentes profundidades de campo:



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{imagens/02-formacao/diferentesprofundidades} 

}

\caption{Diferentes profundidades de campo \autocite[p.15]{moeslund2012}.}\label{fig:diferentesprofundidades}
\end{figure}

Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris no olho humano, ela controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para capturar a imagem.

\hypertarget{sensor}{%
\section{Sensor}\label{sensor}}

Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD, que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS, usado em casos mais gerais, como câmeras de celulares.
Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na figura \ref{fig:sensor}, conhecido como PDA(Photodiode Array):



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/sensor} 

}

\caption{Sensor(area matricial de celulas), Single Cell(uma única celula sensora) \autocite[p.17]{moeslund2012}.}\label{fig:sensor}
\end{figure}

Como podemos ver, o sensor consiste em várias pequenas células, cada uma um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na figura \ref{fig:exposicao} podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta(correctly exposed), logo em seguida temos uma que sofreu de superexposição(overexposed) e na terceira temos uma com subexposição(under exposed). Por último temos uma imagem que sobre com o movimento do objeto que estava sendo capturado, oque ocasionou o borramento(motion blur).



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/02-formacao/exposicao} 

}

\caption{Diferentes níveis de exposição \autocite[p.17]{moeslund2012}.}\label{fig:exposicao}
\end{figure}

Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas em imagens coloridas, como são capturadas?
Imagens coloridas utilizam, especialmente, o formato RGB, que significa Red-Green-Blue, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na figura \ref{fig:componentes} podemos ver uma imagem com seus componentes separados:



\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{imagens/02-formacao/componentes} 

}

\caption{Imagem colorida separada em seus três componentes \autocite[p.28]{moeslund2012}.}\label{fig:componentes}
\end{figure}

Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na figura \ref{fig:tressensores}. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo.



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/02-formacao/tres_sensores} 

}

\caption{Captura de imagem com três sensores \autocite[p.242]{teubner2019}.}\label{fig:tressensores}
\end{figure}

Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel, isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na figura \ref{fig:bayer}:



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{imagens/02-formacao/bayer} 

}

\caption{Filtro Bayer \autocite[p.29]{moeslund2012}.}\label{fig:bayer}
\end{figure}

Podemos perceber que ocorre uma maior ocorrência das cores verdes, isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase a sua captura. Na figura 14 temos uma esquematização de como cada pixel recebe informação de somente uma cor, por meio da filtragem, onde a luz que entra(Incoming light) é filtrada e somente a cor de interesse consegue passar, após isso ela chega a malha de sensores(sensor array):



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/02-formacao/sensorarray} 

}

\caption{Sensores com padrão Bayer \autocite{img:sensorarray}.}\label{fig:sensorarray}
\end{figure}

Vemos na figura \ref{fig:sensorarray} que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos.

\hypertarget{amostragem-e-quantizauxe7uxe3o}{%
\section{Amostragem e Quantização}\label{amostragem-e-quantizauxe7uxe3o}}

Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto.

\hypertarget{amostragem}{%
\subsection{Amostragem}\label{amostragem}}

Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura \ref{fig:amostragemquant}, na qual a figura \ref{fig:amostragemquant} (a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto.

A figura \ref{fig:amostragemquant} (b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha AB. Nas extremidades do gráfico na figura \ref{fig:amostragemquant} (b), a intensidade é mais alta devido a parte branca da imagem, já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na figura \ref{fig:amostragemquant} (c).

Esse procedimento de amostragem, na prática, é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo teríamos que ter um número infinito de pixels, como isso não é possível recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem \autocite{gonzalez2010}.



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{imagens/02-formacao/amostragemquant} 

}

\caption{Filtro Bayer \autocite[p.34]{gonzalez2010}.}\label{fig:amostragemquant}
\end{figure}

\hypertarget{quantizauxe7uxe3o}{%
\subsection{Quantização}\label{quantizauxe7uxe3o}}

Na figura \ref{fig:amostragemquant} (c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na figura \ref{fig:amostragemquant} (d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na figura \ref{fig:amostragemquant} (c).
Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital\autocite[p.8]{burger2009}. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo\autocite[p.9]{bovik2009essential}.
No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos \(\{0,\dots, L-1\}\), onde \(L\) é uma potência de dois, ou seja, \(L = 2_k\) \autocite[p.10]{bovik2009essential}. Isso significa que L é o número de tons de cinza que podem ser representados com uma quantidade k de bits. Em muitas situações é utilizado \(k = 8\), ou seja, temos 256 níveis de cinza.
Ao realizar a quantização e a amostragem linha por linha no objeto da figura \ref{fig:quantizacao} (a) é produzida uma imagem digital bidimensional como na figura \ref{fig:quantizacao}.



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{imagens/02-formacao/quantizacao} 

}

\caption{Filtro Bayer \autocite[p.35]{gonzalez2010}.}\label{fig:quantizacao}
\end{figure}

\hypertarget{definiuxe7uxe3o-de-imagem-digital}{%
\section{Definição de imagem digital}\label{definiuxe7uxe3o-de-imagem-digital}}

Uma imagem pode ser definida como uma função bidimensional, \(f(x, y)\), em que \(x\) e \(y\) são coordenadas espaciais (plano), e a amplitude de \(f\) em qualquer par de coordenadas \((x, y)\) é chamada de intensidade ou nível de cinza da imagem nesse ponto \autocite{gonzalez2010}. Quando \(x\), \(y\) e os valores de intensidade de f são quantidades finitas e discretas, chamamos de imagem digital.

A função \(f(x, y)\) pode ser representada na forma de uma matriz (M x N) como na Figura, em que as M linhas são identificadas pelas coordenadas em \(x\), e as N colunas em \(y\). Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou pel. O formato numérico da matriz, imagem \ref{fig:imagemdigital}, é apropriado para o desenvolvimento de algoritmos, particularmente quando se escreve a equação da matriz (M x N):
\[f(x,y) = \begin{bmatrix}
 f(0,0)   & f(0,1)     & \cdots & f(0,N-1)    \\ 
 f(1,0)   & f(1,1)     & \cdots & f(1, N-1)   \\ 
 \vdots   & \vdots    & &         \vdots      \\ 
 f(M-1,0) & f(M-1, 1)  & \cdots & f(M-1, N-1)
\end{bmatrix}\]



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{imagens/02-formacao/imagemdigital} 

}

\caption{Representações da imagem digital \autocite[p.36]{gonzalez2010}.}\label{fig:imagemdigital}
\end{figure}

Na figura \ref{fig:imagemdigital} (a) temos a representação da imagem em 3D, onde a intensidade de cada pixel é representada no eixo z, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na figura \ref{fig:imagemdigital} (b), formato que seria visualizado em um monitor ou uma fotografia \autocite{gonzalez2010}. Em cada ponto da figura \ref{fig:imagemdigital} (a), o nível de cinza é proporcional ao valor da intensidade \(f\), assumindo valores 0, 0,5 ou 1. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco.

Note que na Figura, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo x positivo direcionado para baixo e o eixo y positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez \autocite{gonzalez2010}.
De acordo com o tamanho da matriz (M x N) e dos níveis discretos de tons de cinza (\(L = 2^k\)) que os pixels podem assumir é possível determinar o número, \(b\), de bits necessários para armazenar uma imagem digitalizada:

\[b = M × N × k\]

Quando uma imagem pode ter \(2^k\) níveis de intensidade, geralmente ela é denominada como uma ``imagem de k bits''. Por exemplo, uma imagem com 256 níveis discretos de intensidade é chamada de uma imagem de 8 bits. A figura \ref{fig:tabelabits} mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (N x N) para diferentes valores de N e k. O número de níveis de intensidade (L) correspondente a cada valor de k é mostrado entre parênteses. Observa-se na figura \ref{fig:tabelabits} que uma imagem de 8 bits com dimensões 1.024 × 1.024 exigiria aproximadamente 1MB para armazenamento.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{imagens/02-formacao/tabelabits} 

}

\caption{Número de bits de armazenamento para vários valores de N e k \autocite[p.38]{gonzalez2010}.}\label{fig:tabelabits}
\end{figure}

\hypertarget{resoluuxe7uxe3o-espacial-e-de-intensidade}{%
\section{Resolução espacial e de intensidade}\label{resoluuxe7uxe3o-espacial-e-de-intensidade}}

Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (M x N) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente dots per inch (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2.400 dpi \autocite{gonzalez2010}.

A figura \ref{fig:resolucaoespacial} mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A figura \ref{fig:resolucaoespacial} (a) tem resolução 512 x 512, e a resolução das demais \ref{fig:resolucaoespacial} (b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{imagens/02-formacao/resolucaoespacial} 

}

\caption{Efeitos da redução da resolução espacial \autocite[p.20]{pedrini2008}.}\label{fig:resolucaoespacial}
\end{figure}

A resolução de intensidade ou profundidade corresponde ao número de bits (k) utilizados para estabelecer os níveis de cinza da imagem (\(L=2^k\)). Por exemplo, em uma imagem cuja intensidade é quantizada em L= 256 níveis, a profundidade é de k = 8 bits por pixel.

Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura. A imagem (a) apresenta 256 níveis de cinza (k = 8). As imagens (b) e (c) foram geradas pela redução do número de bits k = 4 e k = 2, respectivamente, mas mantendo a mesma dimensão.



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{imagens/02-formacao/reducaoprofundidade} 

}

\caption{Efeitos da redução de profundidade \autocite[p.19]{moeslund2012}.}\label{fig:reducaoprofundidade}
\end{figure}

\hypertarget{pixels}{%
\section{Pixels}\label{pixels}}

A topologia digital da imagem desempenha muita importância na especificação, localização e relação entre as coordenadas da imagem, facilitando sua manipulação. A tipologia de uma imagem digital contém as seguintes propriedades dos pixels: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas.

Para especificar, localizar, e relacionar topologicamente uma imagem digitalizada, consideramos: p,q denotando os pontos e, \(p(x,y)\), \(q(x,y)\), \(u(x,y)\) coordenadas dos pontos denotados, expressaremos \(V\) Conjunto de valores em uma imagem binária \(V=\{0,1\}\).

\hypertarget{vizinhanuxe7a}{%
\subsection{Vizinhança}\label{vizinhanuxe7a}}

\begin{itemize}
\tightlist
\item
  \textbf{Vizinhança 4 - \(\left[N_4(p)\right]\)}
\end{itemize}

\(N_4(p)\) em \(p(x,y)\) possui quatro vizinhos, dois na horizontal outros dois na vertical suas coordenadas, ou seja, é o conjunto de pixels ao redor de p, sem considerar as diagonais \autocite[p.15]{thome2017}. Exemplo na figura \ref{fig:vizinhanca} (a).

\[p(x,y): p(x+1, y), p(x-1, y), p(x, y+1), p(x, y-1)\]

\begin{itemize}
\tightlist
\item
  \textbf{Vizinhança D - \(\left[N_D(p)\right]\)}
\end{itemize}

\(N_D(p)\) em \(p(x,y)\) possui quatro vizinhos, dois na diagonais superiores (direita, esquerda ) outras duas na diagonais inferiores (direita, esquerda) suas coordenadas, ou seja o conjunto de pixels ao redor de \(p\), considerando apenas as diagonais \autocite[p.15]{thome2017}.Exemplo na figura \ref{fig:vizinhanca} (b).

\[p(x,y): p(x+1,y+1), p(x+1, y-1), p(x-1, y+1), p(x-1, y-1)\]

\begin{itemize}
\tightlist
\item
  \textbf{Vizinhança 8 - \(\left[N_8(p)\right]\)}
\end{itemize}

\(N_8(p)\) em \(p(x,y)\) possui 8 vizinhos, quatro \(N_4(p)\) e outros 4 \(N_D(p)\)suas coordenadas ou seja o conjunto de pixels ao redor de \(p\), considerando união das vizinhanças-4 e vizinhança-8 \autocite[p.15]{thome2017}. Exemplo na figura \ref{fig:vizinhanca} (c).

\[p(x,y): N_8(p) = N_4(p) \cup N_D(p)\]



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/vizinhanca} 

}

\caption{Vizinhanças \autocite{thome2017}.}\label{fig:vizinhanca}
\end{figure}

\hypertarget{conectividade}{%
\subsection{Conectividade}\label{conectividade}}

Conceito importante, usado no estabelecimento limite das bordas de objetos e, identifica componentes das regiões da imagem (obtenção de propriedades específicas do objeto para o processamento de mais alto nível ).

Dois pixels \(p(x,y)\), \(q(x,y)\) estão conectados se:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  São de alguma forma vizinhos (\(N_4\),\(N_D\) ou \(N_8\)).
\item
  Seus níveis de cinza satisfazem algum critério de similaridade (\(V = \{ \dots \}\)).
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Conectividade de 4}:
\end{itemize}

Os pixels p e q, assumindo valores em \&V\& , são conectados de 4 somente se q pertence ao conjunto \(N_4(p)\). Exemplo em figura \ref{fig:conectividade} (a).

\[C4_{p,q} \text{ em } V \Leftrightarrow q \in N_4(p) \wedge f(p) \wedge f(q) \in V \]
\[V = \{0\} \to C4_{p.q} \text{ verdadeiro}\]

\begin{itemize}
\tightlist
\item
  \textbf{Conectividade de m(conectividade mista)}:
\end{itemize}

Dois pixels \(p\) e \(q\), assumindo valores em \(V\), são conectados de m somente se:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(q\) pertence ao conjunto \(N_4(p)\)
\item
  \(q\) pertence ao conjunto \(N_D(p)\) e a interseção entre \(N_4(p)\) e \(N_4(q)\) for vazia.
\end{enumerate}

\[Cm_{p,q} \text{ em } V\Leftrightarrow (q \in N_4(p) \vee (q \in N_D(p) \wedge N_4(p) \cap N_4(q) = \{\})) \vee f(p) e f(q) \in V  \]
\[V = \{0\} \to Cm_{p.q}\text{ falso}\]
Exemplo em figura \ref{fig:conectividade} (b).

\begin{itemize}
\tightlist
\item
  \textbf{Conectividade de 8}:
\end{itemize}

Os pixels \(p\) e \(q\), assumindo valores em V, são conectados de 8 somente se \(q\) pertence ao conjunto \(N_8(p)\). Exemplo em figura \ref{fig:conectividade} (c).

\[C8_{p,q} \text{ em } V\Leftrightarrow q \in N_8(p) \wedge f(p) \wedge f(q) \in V \]
\[V = \{0\} \to C8_{p.q}\text{ verdadeiro}\]



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/conectividade} 

}

\caption{Conectividades \autocite{thome2017}.}\label{fig:conectividade}
\end{figure}

\hypertarget{adjacuxeancia}{%
\subsection{Adjacência}\label{adjacuxeancia}}

Dois pixels \(p\) e \(q\), com valores pertencendo a \(V\) são:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjacentes-4 se \(q\) estiver no conjunto \(N_4(p)\).
\item
  Adjacentes-8 se \(q\) estiver no conjunto \(N_8(p)\).
\item
  Adjacentes-m, \(p\) e \(q\) subconjuntos de pixels onde \(\{(pq) \vee (pp) \vee (q q) \}\), são ditos adjacentes se pegamos um pixel do primeiro conjunto for adjacente a um pixel do segundo.
\end{enumerate}

Na figura \ref{fig:adjacencia} temos exemplos de 1. e 2. (em \ref{fig:adjacencia} (a)) e de 3. em \ref{fig:adjacencia} (b).



\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{imagens/02-formacao/adjacencia} 

}

\caption{Adjacências \autocite{thome2017}.}\label{fig:adjacencia}
\end{figure}

\hypertarget{componente-conexa}{%
\subsection{Componente Conexa}\label{componente-conexa}}

Dois pixels \(p\) e \(q\) de um subconjunto de pixels \(V\) da imagem são ditos conexos em \(V\) se existir um caminho de pa qinteiramente contido em \(V\).

Para qualquer pixel \(p\) em \(V\), o conjunto de pixels em \(V\) que são conexos a pé chamado um componente conexo de \(V\).

Note que em uma componente conexo qualquer dois pixels deste componentes são conexos entre si.

Em componentes conexos distintos os pixels são disjuntos (não conectados).

\hypertarget{medidas-de-distuxe2ncia}{%
\subsection{Medidas de Distância}\label{medidas-de-distuxe2ncia}}

Para pixels \(p\), \(q\) e \(z\) com coordenadas \(p(x,y)\), \(q(s,t)\) e \((u,v)\), respectivamente, \(D\) é uma função distância ou métrica se:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(D(p,q) >= 0 (D(p,q) = 0 ) \Leftrightarrow p = q\)
\item
  \(D(p,q) = D(q,p)\)
\item
  \(D(p,z) <= D(p,q) + D(q,u)\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  A distância entre dois pontos quaisquer pode ser definida por:
\end{itemize}

\[D_e(p,q) = \sqrt{(x-s)^2 + (y-t)^2}\]

conhecida como Distância Euclidiana.

\begin{itemize}
\tightlist
\item
  Distância \(D_4\)(City Block ou Quarteirão) entre \(p(x,y)\) e \(q(s,t)\)é definida por:
\end{itemize}

\[D4(p,q)=|x-s|+|y-t|\]

Distância D8(Distância Xadrez) entre \(p\) e \(q\) é definida como:

\[D_8 = max(|x-s|,|y-t|)\]

\hypertarget{operauxe7uxf5es-luxf3gico-aritmuxe9ticas}{%
\subsection{Operações Lógico-aritméticas}\label{operauxe7uxf5es-luxf3gico-aritmuxe9ticas}}

As operações entre pixels são computadas pixel a pixel, considerando p e qpodemos efetuar as seguintes operações aritméticas e lógicas.

\begin{itemize}
\tightlist
\item
  Operações Aritméticas:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Adição}: \(p+q\). O uso ocorre ao se fazer a média para redução de ruído.
  \item
    \textbf{Subtração}: \(p-q\). É usada para remover informação estática de fundo, Detecção de diferenças entre imagens.
  \item
    \textbf{Multiplicação}: \(p\cdot q\). Calibração de brilho.
  \item
    \textbf{Divisão}: \(p\div q\)
  \end{enumerate}
\end{itemize}

As operações Aritmética de Multiplicação e Divisão são usadas para corrigir sombras em níveis de cinza, produzidas em não uniformidades da iluminação ou no sensor utilizado para a aquisição da imagem.

\begin{itemize}
\tightlist
\item
  Operações Lógicas:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Conjunção}: \(p\wedge q\)
  \item
    \textbf{Disjunção}: \(p\vee q\)
  \item
    \textbf{Complementar}: \(\neg q(\bar{q})\)
  \end{enumerate}
\end{itemize}

As operações lógicas podem ser combinadas para formar qualquer outra operação lógica, são aplicadas apenas em imagens binárias, tem seu uso no mascaramento, detecção de características e análise de forma.

\hypertarget{transformacuxf5es-geomuxe9tricas}{%
\chapter{Transformacões Geométricas}\label{transformacuxf5es-geomuxe9tricas}}

\hypertarget{representauxe7uxe3o-vetorial-e-matricial-da-imagem}{%
\section{Representação Vetorial e Matricial da Imagem}\label{representauxe7uxe3o-vetorial-e-matricial-da-imagem}}

\hypertarget{sistema-de-coordenadas-objetos-2d-e-3d}{%
\section{Sistema de coordenadas objetos (2D e 3D)}\label{sistema-de-coordenadas-objetos-2d-e-3d}}

\hypertarget{transformauxe7uxf5es-em-pontos-e-objetos}{%
\section{Transformações em pontos e objetos}\label{transformauxe7uxf5es-em-pontos-e-objetos}}

\hypertarget{transformauxe7uxf5es-lineares}{%
\section{Transformações Lineares}\label{transformauxe7uxf5es-lineares}}

\hypertarget{transformauxe7uxe3o-de-translauxe7uxe3o}{%
\section{Transformação de Translação}\label{transformauxe7uxe3o-de-translauxe7uxe3o}}

\hypertarget{transformauxe7uxe3o-de-escala}{%
\section{Transformação de Escala}\label{transformauxe7uxe3o-de-escala}}

\hypertarget{transformauxe7uxe3o-de-rotauxe7uxe3o}{%
\section{Transformação de Rotação}\label{transformauxe7uxe3o-de-rotauxe7uxe3o}}

\hypertarget{transformauxe7uxe3o-cisalhamento-shearing-ou-skew}{%
\section{Transformação Cisalhamento (Shearing ou Skew)}\label{transformauxe7uxe3o-cisalhamento-shearing-ou-skew}}

\hypertarget{transformauxe7uxe3o-de-reflexuxe3o}{%
\section{Transformação de Reflexão}\label{transformauxe7uxe3o-de-reflexuxe3o}}

\hypertarget{caracteruxedsticas-especias}{%
\section{Características especias}\label{caracteruxedsticas-especias}}

\hypertarget{transformauxe7uxe3o-isomuxe9trica}{%
\section{Transformação Isométrica}\label{transformauxe7uxe3o-isomuxe9trica}}

\hypertarget{transformauxe7uxe3o-semelhanuxe7a}{%
\section{Transformação Semelhança}\label{transformauxe7uxe3o-semelhanuxe7a}}

\hypertarget{transformauxe7uxe3o-afim}{%
\section{Transformação Afim}\label{transformauxe7uxe3o-afim}}

\hypertarget{transformauxe7uxe3o-projetiva}{%
\section{Transformação Projetiva}\label{transformauxe7uxe3o-projetiva}}

\hypertarget{transformauxe7uxe3o-inversuxe3o}{%
\section{Transformação Inversão}\label{transformauxe7uxe3o-inversuxe3o}}

\hypertarget{classificauxe7uxe3o-das-projeuxe7uxf5es-geomuxe9tricas}{%
\section{Classificação das Projeções Geométricas}\label{classificauxe7uxe3o-das-projeuxe7uxf5es-geomuxe9tricas}}

\hypertarget{transformauxe7uxf5es-radiomuxe9tricas}{%
\chapter{Transformações radiométricas}\label{transformauxe7uxf5es-radiomuxe9tricas}}

\hypertarget{transformauxe7uxe3o-linear}{%
\section{Transformação Linear}\label{transformauxe7uxe3o-linear}}

\hypertarget{transformauxe7uxe3o-logaruxedtmica}{%
\section{Transformação Logarítmica}\label{transformauxe7uxe3o-logaruxedtmica}}

\hypertarget{transformauxe7uxe3o-de-potuxeancia}{%
\section{Transformação de Potência}\label{transformauxe7uxe3o-de-potuxeancia}}

\hypertarget{processamento-de-histograma}{%
\section{Processamento de histograma}\label{processamento-de-histograma}}

\hypertarget{equalizauxe7uxe3o-do-histograma}{%
\section{Equalização do histograma}\label{equalizauxe7uxe3o-do-histograma}}

\hypertarget{especificauxe7uxe3o-de-histograma}{%
\section{Especificação de histograma}\label{especificauxe7uxe3o-de-histograma}}

\hypertarget{filtros}{%
\chapter{Filtros}\label{filtros}}

\hypertarget{convoluuxe7uxe3o}{%
\section{Convolução}\label{convoluuxe7uxe3o}}

\hypertarget{muxe9dia}{%
\section{Média}\label{muxe9dia}}

\hypertarget{mediana}{%
\section{Mediana}\label{mediana}}

\hypertarget{gaussiano}{%
\section{Gaussiano}\label{gaussiano}}

\hypertarget{canny}{%
\section{Canny}\label{canny}}

\hypertarget{segmentauxe7uxe3o}{%
\chapter{Segmentação}\label{segmentauxe7uxe3o}}

\hypertarget{detecuxe7uxe3o-por-descontinuidade}{%
\section{Detecção por descontinuidade}\label{detecuxe7uxe3o-por-descontinuidade}}

\hypertarget{detecuxe7uxe3o-de-ponto}{%
\subsection{Detecção de ponto}\label{detecuxe7uxe3o-de-ponto}}

\hypertarget{detecuxe7uxe3o-de-linha}{%
\subsection{Detecção de linha}\label{detecuxe7uxe3o-de-linha}}

\hypertarget{detecuxe7uxe3o-de-bordas}{%
\section{Detecção de bordas}\label{detecuxe7uxe3o-de-bordas}}

\hypertarget{modelos-de-bordas}{%
\subsection{Modelos de Bordas}\label{modelos-de-bordas}}

\hypertarget{muxe9todo-do-gradiente-roberts-prewitt-sobel.}{%
\subsection{Método do gradiente ( Roberts, Prewitt, Sobel).}\label{muxe9todo-do-gradiente-roberts-prewitt-sobel.}}

\hypertarget{muxe9todo-marr-hildreth}{%
\subsection{Método Marr-Hildreth}\label{muxe9todo-marr-hildreth}}

\hypertarget{muxe9todo-canny}{%
\subsection{Método Canny}\label{muxe9todo-canny}}

\hypertarget{transformada-de-hough}{%
\section{Transformada de Hough}\label{transformada-de-hough}}

\hypertarget{transformada-de-hough-para-detecuxe7uxe3o-de-linhas}{%
\subsection{Transformada de Hough para detecção de linhas}\label{transformada-de-hough-para-detecuxe7uxe3o-de-linhas}}

\hypertarget{transformada-de-hough-para-detecuxe7uxe3o-de-cuxedrculos}{%
\subsection{Transformada de Hough para detecção de círculos}\label{transformada-de-hough-para-detecuxe7uxe3o-de-cuxedrculos}}

\hypertarget{detecuxe7uxe3o-de-blobs}{%
\section{Detecção de blobs}\label{detecuxe7uxe3o-de-blobs}}

\hypertarget{log}{%
\subsection{LoG}\label{log}}

\hypertarget{dog}{%
\subsection{DoG}\label{dog}}

\hypertarget{doh}{%
\subsection{DoH}\label{doh}}

\hypertarget{detecuxe7uxe3o-de-junuxe7uxf5es-ou-cantos}{%
\section{Detecção de junções ou cantos}\label{detecuxe7uxe3o-de-junuxe7uxf5es-ou-cantos}}

\hypertarget{segmentauxe7uxe3o-por-limiarizauxe7uxe3o}{%
\section{Segmentação por limiarização}\label{segmentauxe7uxe3o-por-limiarizauxe7uxe3o}}

\hypertarget{muxe9todo-de-otsu}{%
\section{método de Otsu}\label{muxe9todo-de-otsu}}

\hypertarget{segmentauxe7uxe3o-usando-watersheds-morfoluxf3gicas}{%
\section{Segmentação usando watersheds morfológicas}\label{segmentauxe7uxe3o-usando-watersheds-morfoluxf3gicas}}

\backmatter

\printbibliography

\end{document}
