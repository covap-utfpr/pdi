<!DOCTYPE html>
<html lang="pt-BR" xml:lang="pt-BR">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 2 Formação da imagem | Material introdutório de Processamento Digital de Imagens e Visão Computacional</title>
  <meta name="description" content="Capítulo 2 Formação da imagem | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 2 Formação da imagem | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 2 Formação da imagem | Material introdutório de Processamento Digital de Imagens e Visão Computacional" />
  
  
  

<meta name="author" content="" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="transformacoesGeometricas.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="logo"><a href="./"><img src="imagens/logo.jpeg"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Início</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#relação-entre-processamento-digital-de-imagem-visão-computacional-e-computação-gráfica"><i class="fa fa-check"></i><b>1.1</b> Relação entre Processamento Digital de Imagem, Visão Computacional e Computação Gráfica</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#etapas-do-processamento-de-imagens-visão-computacional-e-aprendizado-de-máquina"><i class="fa fa-check"></i><b>1.2</b> Etapas do Processamento de Imagens, Visão Computacional e Aprendizado de Máquina</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#áreas-de-aplicações"><i class="fa fa-check"></i><b>1.3</b> Áreas de Aplicações</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="formacaoImagem.html"><a href="formacaoImagem.html"><i class="fa fa-check"></i><b>2</b> Formação da imagem</a><ul>
<li class="chapter" data-level="2.1" data-path="formacaoImagem.html"><a href="formacaoImagem.html#câmera-pinhole-e-geometria"><i class="fa fa-check"></i><b>2.1</b> Câmera <em>pinhole</em> e geometria</a></li>
<li class="chapter" data-level="2.2" data-path="formacaoImagem.html"><a href="formacaoImagem.html#lentes-finas"><i class="fa fa-check"></i><b>2.2</b> Lentes Finas</a></li>
<li class="chapter" data-level="2.3" data-path="formacaoImagem.html"><a href="formacaoImagem.html#sensor"><i class="fa fa-check"></i><b>2.3</b> Sensor</a></li>
<li class="chapter" data-level="2.4" data-path="formacaoImagem.html"><a href="formacaoImagem.html#amostragem-e-quantização"><i class="fa fa-check"></i><b>2.4</b> Amostragem e Quantização</a><ul>
<li class="chapter" data-level="2.4.1" data-path="formacaoImagem.html"><a href="formacaoImagem.html#amostragem"><i class="fa fa-check"></i><b>2.4.1</b> Amostragem</a></li>
<li class="chapter" data-level="2.4.2" data-path="formacaoImagem.html"><a href="formacaoImagem.html#quantização"><i class="fa fa-check"></i><b>2.4.2</b> Quantização</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="formacaoImagem.html"><a href="formacaoImagem.html#formacaoImg"><i class="fa fa-check"></i><b>2.5</b> Definição de imagem digital</a></li>
<li class="chapter" data-level="2.6" data-path="formacaoImagem.html"><a href="formacaoImagem.html#resolução-espacial-e-de-intensidade"><i class="fa fa-check"></i><b>2.6</b> Resolução espacial e de intensidade</a></li>
<li class="chapter" data-level="2.7" data-path="formacaoImagem.html"><a href="formacaoImagem.html#pixels"><i class="fa fa-check"></i><b>2.7</b> Pixels</a><ul>
<li class="chapter" data-level="2.7.1" data-path="formacaoImagem.html"><a href="formacaoImagem.html#vizin"><i class="fa fa-check"></i><b>2.7.1</b> Vizinhança</a></li>
<li class="chapter" data-level="2.7.2" data-path="formacaoImagem.html"><a href="formacaoImagem.html#contiv"><i class="fa fa-check"></i><b>2.7.2</b> Conectividade</a></li>
<li class="chapter" data-level="2.7.3" data-path="formacaoImagem.html"><a href="formacaoImagem.html#adja"><i class="fa fa-check"></i><b>2.7.3</b> Adjacência</a></li>
<li class="chapter" data-level="2.7.4" data-path="formacaoImagem.html"><a href="formacaoImagem.html#camin"><i class="fa fa-check"></i><b>2.7.4</b> Caminho</a></li>
<li class="chapter" data-level="2.7.5" data-path="formacaoImagem.html"><a href="formacaoImagem.html#componente-conexa"><i class="fa fa-check"></i><b>2.7.5</b> Componente Conexa</a></li>
<li class="chapter" data-level="2.7.6" data-path="formacaoImagem.html"><a href="formacaoImagem.html#borda-e-interior"><i class="fa fa-check"></i><b>2.7.6</b> Borda e Interior</a></li>
<li class="chapter" data-level="2.7.7" data-path="formacaoImagem.html"><a href="formacaoImagem.html#medidas-de-distância"><i class="fa fa-check"></i><b>2.7.7</b> Medidas de Distância</a></li>
<li class="chapter" data-level="2.7.8" data-path="formacaoImagem.html"><a href="formacaoImagem.html#operações-lógico-aritméticas"><i class="fa fa-check"></i><b>2.7.8</b> Operações Lógico-aritméticas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html"><i class="fa fa-check"></i><b>3</b> Transformacões geométricas</a><ul>
<li class="chapter" data-level="3.1" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#definição"><i class="fa fa-check"></i><b>3.1</b> Definição</a></li>
<li class="chapter" data-level="3.2" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#sistema-de-coordenadas-objetos-2d-e-3d"><i class="fa fa-check"></i><b>3.2</b> Sistema de coordenadas objetos (2D e 3D)</a></li>
<li class="chapter" data-level="3.3" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#representação-vetorial-e-matricial-de-imagens-digitalizadas"><i class="fa fa-check"></i><b>3.3</b> Representação Vetorial e Matricial de Imagens digitalizadas</a></li>
<li class="chapter" data-level="3.4" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#matrizes-em-computação-gráfica"><i class="fa fa-check"></i><b>3.4</b> Matrizes em Computação gráfica</a></li>
<li class="chapter" data-level="3.5" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#transformações-em-pontos-e-objetos"><i class="fa fa-check"></i><b>3.5</b> Transformações em Pontos e Objetos</a></li>
<li class="chapter" data-level="3.6" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#transformação-de-translação"><i class="fa fa-check"></i><b>3.6</b> Transformação de Translação</a></li>
<li class="chapter" data-level="3.7" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#transformação-de-escala"><i class="fa fa-check"></i><b>3.7</b> Transformação de Escala</a></li>
<li class="chapter" data-level="3.8" data-path="transformacoesGeometricas.html"><a href="transformacoesGeometricas.html#transformação-de-rotação"><i class="fa fa-check"></i><b>3.8</b> Transformação de Rotação</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html"><i class="fa fa-check"></i><b>4</b> Transformações radiométricas</a><ul>
<li class="chapter" data-level="4.1" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#transformação-linear"><i class="fa fa-check"></i><b>4.1</b> Transformação Linear</a></li>
<li class="chapter" data-level="4.2" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#transformação-logarítmica"><i class="fa fa-check"></i><b>4.2</b> Transformação Logarítmica</a></li>
<li class="chapter" data-level="4.3" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#transformação-de-potência"><i class="fa fa-check"></i><b>4.3</b> Transformação de Potência</a></li>
<li class="chapter" data-level="4.4" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#processamento-de-histograma"><i class="fa fa-check"></i><b>4.4</b> Processamento de histograma</a></li>
<li class="chapter" data-level="4.5" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#equalização-do-histograma"><i class="fa fa-check"></i><b>4.5</b> Equalização do histograma</a></li>
<li class="chapter" data-level="4.6" data-path="transformacoesRadiometricas.html"><a href="transformacoesRadiometricas.html#especificação-de-histograma"><i class="fa fa-check"></i><b>4.6</b> Especificação de histograma</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="filtros.html"><a href="filtros.html"><i class="fa fa-check"></i><b>5</b> Filtros Digitais</a><ul>
<li class="chapter" data-level="5.1" data-path="filtros.html"><a href="filtros.html#convolução"><i class="fa fa-check"></i><b>5.1</b> Convolução</a><ul>
<li class="chapter" data-level="5.1.1" data-path="filtros.html"><a href="filtros.html#definção-matemática-da-convolução"><i class="fa fa-check"></i><b>5.1.1</b> Definção matemática da convolução</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="filtros.html"><a href="filtros.html#filtro-da-média"><i class="fa fa-check"></i><b>5.2</b> Filtro da Média</a></li>
<li class="chapter" data-level="5.3" data-path="filtros.html"><a href="filtros.html#filtro-da-mediana"><i class="fa fa-check"></i><b>5.3</b> Filtro da Mediana</a></li>
<li class="chapter" data-level="5.4" data-path="filtros.html"><a href="filtros.html#filtro-gaussiano"><i class="fa fa-check"></i><b>5.4</b> Filtro Gaussiano</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="segmentacao.html"><a href="segmentacao.html"><i class="fa fa-check"></i><b>6</b> Segmentação</a><ul>
<li class="chapter" data-level="6.1" data-path="segmentacao.html"><a href="segmentacao.html#detecção-por-descontinuidade"><i class="fa fa-check"></i><b>6.1</b> Detecção por descontinuidade</a><ul>
<li class="chapter" data-level="6.1.1" data-path="segmentacao.html"><a href="segmentacao.html#detecção-de-pontos-isolados"><i class="fa fa-check"></i><b>6.1.1</b> Detecção de pontos isolados</a></li>
<li class="chapter" data-level="6.1.2" data-path="segmentacao.html"><a href="segmentacao.html#detecção-de-linhas"><i class="fa fa-check"></i><b>6.1.2</b> Detecção de linhas</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="segmentacao.html"><a href="segmentacao.html#detecção-de-bordas"><i class="fa fa-check"></i><b>6.2</b> Detecção de Bordas</a><ul>
<li class="chapter" data-level="6.2.1" data-path="segmentacao.html"><a href="segmentacao.html#modelos-de-bordas"><i class="fa fa-check"></i><b>6.2.1</b> Modelos de Bordas</a></li>
<li class="chapter" data-level="6.2.2" data-path="segmentacao.html"><a href="segmentacao.html#método-do-gradiente-roberts-prewitt-sobel"><i class="fa fa-check"></i><b>6.2.2</b> Método do gradiente (Roberts, Prewitt, Sobel)</a></li>
<li class="chapter" data-level="6.2.3" data-path="segmentacao.html"><a href="segmentacao.html#método-de-marr-hildreth"><i class="fa fa-check"></i><b>6.2.3</b> Método de Marr-Hildreth</a></li>
<li class="chapter" data-level="6.2.4" data-path="segmentacao.html"><a href="segmentacao.html#método-de-canny"><i class="fa fa-check"></i><b>6.2.4</b> Método de Canny</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="segmentacao.html"><a href="segmentacao.html#transformada-de-hough"><i class="fa fa-check"></i><b>6.3</b> Transformada de Hough</a><ul>
<li class="chapter" data-level="6.3.1" data-path="segmentacao.html"><a href="segmentacao.html#transformada-de-hough-para-detecção-de-linhas"><i class="fa fa-check"></i><b>6.3.1</b> Transformada de Hough para detecção de linhas</a></li>
<li class="chapter" data-level="6.3.2" data-path="segmentacao.html"><a href="segmentacao.html#transformada-de-hough-para-detecção-de-círculos"><i class="fa fa-check"></i><b>6.3.2</b> Transformada de Hough para detecção de círculos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="segmentacao.html"><a href="segmentacao.html#detecção-de-quinas"><i class="fa fa-check"></i><b>6.4</b> Detecção de Quinas</a><ul>
<li class="chapter" data-level="6.4.1" data-path="segmentacao.html"><a href="segmentacao.html#detector-de-quinas-de-moravec"><i class="fa fa-check"></i><b>6.4.1</b> Detector de Quinas de Moravec</a></li>
<li class="chapter" data-level="6.4.2" data-path="segmentacao.html"><a href="segmentacao.html#detector-de-quinas-de-harris"><i class="fa fa-check"></i><b>6.4.2</b> Detector de Quinas de Harris</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="segmentacao.html"><a href="segmentacao.html#detecção-de-blobs"><i class="fa fa-check"></i><b>6.5</b> Detecção de <em>Blobs</em></a><ul>
<li class="chapter" data-level="6.5.1" data-path="segmentacao.html"><a href="segmentacao.html#log"><i class="fa fa-check"></i><b>6.5.1</b> LoG</a></li>
<li class="chapter" data-level="6.5.2" data-path="segmentacao.html"><a href="segmentacao.html#dog"><i class="fa fa-check"></i><b>6.5.2</b> DoG</a></li>
<li class="chapter" data-level="6.5.3" data-path="segmentacao.html"><a href="segmentacao.html#doh"><i class="fa fa-check"></i><b>6.5.3</b> DoH</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="segmentacao.html"><a href="segmentacao.html#limiarização"><i class="fa fa-check"></i><b>6.6</b> Limiarização</a><ul>
<li class="chapter" data-level="6.6.1" data-path="segmentacao.html"><a href="segmentacao.html#limiarização-global-simples"><i class="fa fa-check"></i><b>6.6.1</b> Limiarização Global Simples</a></li>
<li class="chapter" data-level="6.6.2" data-path="segmentacao.html"><a href="segmentacao.html#limiarização-pelo-método-de-otsu"><i class="fa fa-check"></i><b>6.6.2</b> Limiarização pelo Método de Otsu</a></li>
<li class="chapter" data-level="6.6.3" data-path="segmentacao.html"><a href="segmentacao.html#uso-de-suavização-para-limiarização"><i class="fa fa-check"></i><b>6.6.3</b> Uso de suavização para limiarização</a></li>
<li class="chapter" data-level="6.6.4" data-path="segmentacao.html"><a href="segmentacao.html#uso-de-bordas-para-limiarização"><i class="fa fa-check"></i><b>6.6.4</b> Uso de bordas para limiarização</a></li>
<li class="chapter" data-level="6.6.5" data-path="segmentacao.html"><a href="segmentacao.html#limiares-múltiplos"><i class="fa fa-check"></i><b>6.6.5</b> Limiares Múltiplos</a></li>
<li class="chapter" data-level="6.6.6" data-path="segmentacao.html"><a href="segmentacao.html#limiarização-variável"><i class="fa fa-check"></i><b>6.6.6</b> Limiarização variável</a></li>
<li class="chapter" data-level="6.6.7" data-path="segmentacao.html"><a href="segmentacao.html#limiarização-baseada-em-diversas-variáveis"><i class="fa fa-check"></i><b>6.6.7</b> Limiarização baseada em diversas variáveis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deepLearning.html"><a href="deepLearning.html"><i class="fa fa-check"></i><b>7</b> Deep Learning em visão computacional</a><ul>
<li class="chapter" data-level="7.1" data-path="deepLearning.html"><a href="deepLearning.html#caracterização-de-ia-machine-learning-e-deep-learning"><i class="fa fa-check"></i><b>7.1</b> Caracterização de IA, Machine Learning e Deep Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="deepLearning.html"><a href="deepLearning.html#aprendizado-supervisionado-e-não-supervisionado"><i class="fa fa-check"></i><b>7.1.1</b> Aprendizado supervisionado e não supervisionado</a></li>
<li class="chapter" data-level="7.1.2" data-path="deepLearning.html"><a href="deepLearning.html#redes-neurais-artificiais"><i class="fa fa-check"></i><b>7.1.2</b> Redes Neurais Artificiais</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="deepLearning.html"><a href="deepLearning.html#cnn"><i class="fa fa-check"></i><b>7.2</b> Redes neurais convolucionais (CNN)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="deepLearning.html"><a href="deepLearning.html#blocos-de-construção-de-uma-cnn"><i class="fa fa-check"></i><b>7.2.1</b> Blocos de construção de uma CNN</a></li>
<li class="chapter" data-level="7.2.2" data-path="deepLearning.html"><a href="deepLearning.html#por-que-usar-convoluções"><i class="fa fa-check"></i><b>7.2.2</b> Por que usar convoluções</a></li>
<li class="chapter" data-level="7.2.3" data-path="deepLearning.html"><a href="deepLearning.html#redes-cnns-clássicas"><i class="fa fa-check"></i><b>7.2.3</b> Redes CNN’s clássicas</a></li>
<li class="chapter" data-level="7.2.4" data-path="deepLearning.html"><a href="deepLearning.html#aprendizado-por-transferência"><i class="fa fa-check"></i><b>7.2.4</b> Aprendizado por transferência</a></li>
<li class="chapter" data-level="7.2.5" data-path="deepLearning.html"><a href="deepLearning.html#redes-neurais-na-prática"><i class="fa fa-check"></i><b>7.2.5</b> Redes neurais na prática</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="deepLearning.html"><a href="deepLearning.html#redes-neurais-siamesas"><i class="fa fa-check"></i><b>7.3</b> Redes Neurais Siamesas</a><ul>
<li class="chapter" data-level="7.3.1" data-path="deepLearning.html"><a href="deepLearning.html#arquitetura"><i class="fa fa-check"></i><b>7.3.1</b> Arquitetura</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="separacaoFundo.html"><a href="separacaoFundo.html"><i class="fa fa-check"></i><b>8</b> Separação Plano de Fundo</a><ul>
<li class="chapter" data-level="8.1" data-path="separacaoFundo.html"><a href="separacaoFundo.html#fixo"><i class="fa fa-check"></i><b>8.1</b> Fixo</a></li>
<li class="chapter" data-level="8.2" data-path="separacaoFundo.html"><a href="separacaoFundo.html#média-temporal"><i class="fa fa-check"></i><b>8.2</b> Média Temporal</a></li>
<li class="chapter" data-level="8.3" data-path="separacaoFundo.html"><a href="separacaoFundo.html#mediana-temporal"><i class="fa fa-check"></i><b>8.3</b> Mediana Temporal</a></li>
<li class="chapter" data-level="8.4" data-path="separacaoFundo.html"><a href="separacaoFundo.html#exemplos-comparativos-da-média-temporal-média-espaço-temporal-e-mediana-temporal"><i class="fa fa-check"></i><b>8.4</b> Exemplos comparativos da Média Temporal, Média Espaço-Temporal e Mediana Temporal</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="espaco3D.html"><a href="espaco3D.html"><i class="fa fa-check"></i><b>9</b> Espaço 3D</a><ul>
<li class="chapter" data-level="9.1" data-path="espaco3D.html"><a href="espaco3D.html#geometria-projetiva"><i class="fa fa-check"></i><b>9.1</b> Geometria projetiva</a><ul>
<li class="chapter" data-level="9.1.1" data-path="espaco3D.html"><a href="espaco3D.html#homogenea"><i class="fa fa-check"></i><b>9.1.1</b> Coordenadas homogêneas</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="espaco3D.html"><a href="espaco3D.html#homografia"><i class="fa fa-check"></i><b>9.2</b> Homografia</a><ul>
<li class="chapter" data-level="9.2.1" data-path="espaco3D.html"><a href="espaco3D.html#transformação-linear-direta-dlt"><i class="fa fa-check"></i><b>9.2.1</b> Transformação Linear Direta (DLT)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="espaco3D.html"><a href="espaco3D.html#transformações-de-câmera"><i class="fa fa-check"></i><b>9.3</b> Transformações de câmera</a></li>
<li class="chapter" data-level="9.4" data-path="espaco3D.html"><a href="espaco3D.html#distorção-das-lentes"><i class="fa fa-check"></i><b>9.4</b> Distorção das lentes</a></li>
<li class="chapter" data-level="9.5" data-path="espaco3D.html"><a href="espaco3D.html#calibração"><i class="fa fa-check"></i><b>9.5</b> Calibração</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="refêrencias.html"><a href="refêrencias.html"><i class="fa fa-check"></i>Refêrencias</a></li>
<li class="divider"></li>
<li><center>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
</a></li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Material introdutório de Processamento Digital de Imagens e Visão Computacional</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="formacaoImagem" class="section level1">
<h1><span class="header-section-number">Capítulo 2</span> Formação da imagem</h1>
<p>Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é o espectro eletromagnético na faixa de ondas visíveis. Outras fontes de energia também podem ser utilizadas como energia mecânica na forma de ultrassom, feixe de elétrons em microscópio eletrônicos, ondas de rádio no radar, etc. Cada fonte necessita de um método específico de captura. Para ondas eletromagnéticas pode ser usada uma câmera fotográfica equipada com sensores adequados ao comprimento de onda. Porém, para outras fontes, é necessário que o computador sintetize a imagem, como o microscópio eletrônico.</p>
<p>Como já mencionado no tópico de <a href="intro.html#intro">Introdução</a>, o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensíveis a essa faixa de luz, que vem da luz solar e nos ajuda a realizar nossas atividades cotidianas. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas de onda, como a ultravioleta <span class="citation">[<a href="#ref-cuthill2017" role="doc-biblioref">9</a>, p. 2]</span>. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 8]</span>.</p>
<p>A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza ou escala de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de <span class="math inline">\(0.43\)</span> a <span class="math inline">\(0.79 \mu m\)</span>. Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 28]</span>.</p>
<p>Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir.</p>
<div id="câmera-pinhole-e-geometria" class="section level2">
<h2><span class="header-section-number">2.1</span> Câmera <em>pinhole</em> e geometria</h2>
<p>Na Figura <a href="formacaoImagem.html#fig:aquisicaoimagem">2.1</a>, temos um esquema básico de como geralmente ocorre a aquisição de imagens. Primeiramente, a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida. A parte refletida é capturada por um dispositivo, como uma câmera fotográfica.</p>

<div class="figure" style="text-align: center"><span id="fig:aquisicaoimagem"></span>
<img src="imagens/02-formacao/aquisicao_imagem.png" alt="Representação de uma típica captura de imagem [4, p. 8]." width="55%" />
<p class="caption">
Figura 2.1: Representação de uma típica captura de imagem <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 8]</span>.
</p>
</div>
<p>Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, conhecido como câmera <em>pinhole</em> (do inglês buraco de alfinete) ou câmera escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício, tão pequeno quanto possível, por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na Figura <a href="formacaoImagem.html#fig:barreiraluz">2.2</a>, se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruidosa. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores.</p>

<div class="figure" style="text-align: center"><span id="fig:barreiraluz"></span>
<img src="imagens/02-formacao/barreiraluz.png" alt="Introdução de barreira para captura de imagem [4, p. 11]." width="55%" />
<p class="caption">
Figura 2.2: Introdução de barreira para captura de imagem <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 11]</span>.
</p>
</div>
<p>Na Figura <a href="formacaoImagem.html#fig:barreiraluz">2.2</a> percebemos que a imagem resultante acaba invertida. Isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir.</p>

<div class="figure" style="text-align: center"><span id="fig:geometriapinhole"></span>
<img src="imagens/02-formacao/geometriapinhole.png" alt="Geometria de uma câmera pinhole [10, p. 5]." width="55%" />
<p class="caption">
Figura 2.3: Geometria de uma câmera pinhole <span class="citation">[<a href="#ref-burger2009" role="doc-biblioref">10</a>, p. 5]</span>.
</p>
</div>
<p>Na Figura <a href="formacaoImagem.html#fig:geometriapinhole">2.3</a>, considerando que o eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância horizontal <span class="math inline">\(Z\)</span> da abertura e a uma distância vertical <span class="math inline">\(Y\)</span> do eixo óptico, podemos definir a altura <span class="math inline">\(y\)</span> e a largura <span class="math inline">\(x\)</span> da projeção do objeto utilizando a simetria de triângulos:</p>
<p><span class="math display" id="eq:semelhancaTriangulos">\[
-\frac{y}{f}=\frac{Y}{Z}\Leftrightarrow y=-f\frac{Y}{Z} \ \ \ \ \text{e}\ \ \ \ -\frac{x}{f}=\frac{X}{Z} \Leftrightarrow x=-f\frac{X}{Z}
\tag{2.1}
\]</span></p>
<p>A variável <span class="math inline">\(f\)</span> nessa Equação <a href="formacaoImagem.html#eq:semelhancaTriangulos">(2.1)</a> se refere a distância focal, que é, nesse caso, o tamanho da caixa da câmera, pois a imagem é formada em seu fundo <span class="citation">[<a href="#ref-burger2009" role="doc-biblioref">10</a>, p. 4]</span>. Os sinais negativos das equações significam que a imagem projetada está rotacionada a <span class="math inline">\(180^\circ\)</span> verticalmente e horizontalmente devido a semelhança de triângulos <span class="citation">[<a href="#ref-burger2009" role="doc-biblioref">10</a>, p. 5]</span>, como podemos confirmar na Figura <a href="formacaoImagem.html#fig:geometriapinhole">2.3</a>. Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens como precisar de um longo tempo de exposição para captura da imagem.</p>
<p>As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso.</p>
</div>
<div id="lentes-finas" class="section level2">
<h2><span class="header-section-number">2.2</span> Lentes Finas</h2>

<div class="figure" style="text-align: center"><span id="fig:lente"></span>
<img src="imagens/02-formacao/lente.png" alt="Ação de uma lente sobre os raios de luz [4, p. 12]." width="55%" />
<p class="caption">
Figura 2.4: Ação de uma lente sobre os raios de luz <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 12]</span>.
</p>
</div>
<p>Como podemos ver na Figura <a href="formacaoImagem.html#fig:lente">2.4</a>, em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada.
O ponto <span class="math inline">\(F\)</span> onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância <span class="math inline">\(f\)</span>, que vai do centro óptico <span class="math inline">\(O\)</span> até <span class="math inline">\(F\)</span> é conhecida como Distância Focal.
Definindo a distância do objeto real até a lente como <span class="math inline">\(g\)</span> e a distância até a formação da imagem, após passar pela lente, como <span class="math inline">\(b\)</span> temos que:</p>
<p><span class="math display" id="eq:relacaoDistancias">\[
\frac{1}{g}+\frac{1}{b}=\frac{1}{f}
\tag{2.2}
\]</span></p>
<p>Como <span class="math inline">\(f\)</span> e <span class="math inline">\(b\)</span> estão normalmente entre 1mm e 100mm isso mostra que <span class="math inline">\(\frac{1}{g}\)</span> não tem quase nenhum impacto na Equação <a href="formacaoImagem.html#eq:relacaoDistancias">(2.2)</a> e significa que <span class="math inline">\(b = f\)</span>. Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal.
Outro ponto importante das lentes é conhecido como <em>zoom</em> óptico, ilustrado na Figura <a href="formacaoImagem.html#fig:proporcaoObjetoImagem">2.5</a>. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, <span class="math inline">\(B\)</span>, aumenta quando <span class="math inline">\(f\)</span> aumenta. Podemos representar isso na seguinte Equação <a href="formacaoImagem.html#eq:relacaoTamanhoObjetoImagem">(2.3)</a>, onde <span class="math inline">\(g\)</span> é o tamanho real do objeto:</p>
<p><span class="math display" id="eq:relacaoTamanhoObjetoImagem">\[
\frac{b}{B}=\frac{g}{G}
\tag{2.3}
\]</span></p>

<div class="figure" style="text-align: center"><span id="fig:proporcaoObjetoImagem"></span>
<img src="imagens/02-formacao/proporcaoObjetoImagem.png" alt="Zoom óptico através de lentes com diferentes distâncias focais [4, p. 13]." width="55%" />
<p class="caption">
Figura 2.5: <em>Zoom</em> óptico através de lentes com diferentes distâncias focais <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 13]</span>.
</p>
</div>
<p>Na prática <span class="math inline">\(f\)</span> é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos.
Se o <span class="math inline">\(f\)</span> for constante, quando alteramos a distância do objeto, no caso <span class="math inline">\(g\)</span>, sabemos que <span class="math inline">\(b\)</span> também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos <span class="math inline">\(b\)</span> temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando <span class="math inline">\(b\)</span> para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco.</p>

<div class="figure" style="text-align: center"><span id="fig:foco"></span>
<img src="imagens/02-formacao/foco.png" alt="Uma imagem focada em (a) e desfocada em (b) [4, p. 11]." width="55%" />
<p class="caption">
Figura 2.6: Uma imagem focada em (a) e desfocada em (b) <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 11]</span>.
</p>
</div>
<p>A Figura <a href="formacaoImagem.html#fig:foco">2.6</a> ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do pixel. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o pixel, gerando uma mistura de diferentes pontos.</p>

<div class="figure" style="text-align: center"><span id="fig:profundidade"></span>
<img src="imagens/02-formacao/profundidade.png" alt="Profundidade de campo [4, p. 13]." width="55%" />
<p class="caption">
Figura 2.7: Profundidade de campo <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 13]</span>.
</p>
</div>
<p>A Figura <a href="formacaoImagem.html#fig:profundidade">2.7</a> apresenta outro ponto muito importante, chamado Profundidade de Campo (<em>Depth of field</em>), que representa a soma das distâncias <span class="math inline">\(g_l\)</span> e <span class="math inline">\(g_r\)</span>, que representam o quanto os objetos podem ser movidos e permanecerem em foco.</p>
<p>Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão (<em>Field of View</em> ou FOV) que representa a área observável de uma câmera. Na Figura <a href="formacaoImagem.html#fig:campovisao">2.8</a>, essa área observável é denotada pelo ângulo <span class="math inline">\(V\)</span>. O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as Equações <a href="formacaoImagem.html#eq:fovH">(2.4)</a> e <a href="formacaoImagem.html#eq:fovV">(2.5)</a>, respectivamente, do campo de visão horizontal e vertical:
<span class="math display" id="eq:fovH">\[
FOV_x = 2*\tan^{-1}\left(\frac{\frac{comprimento\ do\ sensor}{2}}{f}\right)
\tag{2.4}
\]</span>
<span class="math display" id="eq:fovV">\[
FOV_y = 2*\tan^{-1}\left(\frac{\frac{altura\ do\ sensor}{2}}{f}\right)
\tag{2.5}
\]</span></p>

<div class="figure" style="text-align: center"><span id="fig:campovisao"></span>
<img src="imagens/02-formacao/campovisao.png" alt="Dois diferentes Campos de visão [4, p. 14]." width="55%" />
<p class="caption">
Figura 2.8: Dois diferentes Campos de visão <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 14]</span>.
</p>
</div>
<p>Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de <span class="math inline">\(14mm\)</span>, altura de <span class="math inline">\(10mm\)</span> e uma distância focal de <span class="math inline">\(5mm\)</span> temos:
<span class="math display" id="eq:FOVexemplo">\[
FOV_x=2*tan^{-1}\left(\frac{7}{5}\right)=108.9^{\circ} \ \ \ \ \text{e}\ \ \ \  FOV_y=2*tan^{-1}(1)=90^{\circ}
\tag{2.6}
\]</span></p>
<p>Isso significa que essa câmera tem uma área observável de <span class="math inline">\(108.9^\circ\)</span> horizontalmente e <span class="math inline">\(90^\circ\)</span> verticalmente. Na Figura <a href="formacaoImagem.html#fig:diferentesprofundidades">2.9</a>, temos o mesmo objeto fotografado com diferentes profundidades de campo:</p>

<div class="figure" style="text-align: center"><span id="fig:diferentesprofundidades"></span>
<img src="imagens/02-formacao/diferentesprofundidades.png" alt="Objeto fotografado com diferentes profundidades de campo [4, p. 15]." width="90%" />
<p class="caption">
Figura 2.9: Objeto fotografado com diferentes profundidades de campo <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 15]</span>.
</p>
</div>
<p>Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris do olho humano. É ele que controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para a captura da imagem.</p>
</div>
<div id="sensor" class="section level2">
<h2><span class="header-section-number">2.3</span> Sensor</h2>
<p>Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD (<em>Charge-coupled device</em>), que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS (<em>Complementary metal–oxide semiconductor</em>), usado em casos mais gerais, como câmeras de celulares.
Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na Figura <a href="formacaoImagem.html#fig:sensor">2.10</a>, conhecido como PDA (<em>Photodiode Array</em>):</p>

<div class="figure" style="text-align: center"><span id="fig:sensor"></span>
<img src="imagens/02-formacao/sensor.png" alt="Sensor (área matricial de células), Single Cell (uma única célula sensor) [4, p. 17]." width="55%" />
<p class="caption">
Figura 2.10: Sensor (área matricial de células), <em>Single Cell</em> (uma única célula sensor) <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 17]</span>.
</p>
</div>
<p>Como podemos ver, o sensor consiste em várias pequenas células, cada uma com um pixel, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um pixel, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na Figura <a href="formacaoImagem.html#fig:exposicao">2.11</a>, podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta (<em>correctly exposed</em>), logo em seguida temos uma que sofreu de superexposição (<em>overexposed</em>) e na terceira temos uma com subexposição (<em>underexposed</em>). Por último temos uma imagem que sofre com o movimento do objeto cuja imagem estava sendo capturada, o que ocasionou o borramento (<em>motion blur</em>).</p>

<div class="figure" style="text-align: center"><span id="fig:exposicao"></span>
<img src="imagens/02-formacao/exposicao.png" alt="Diferentes níveis de exposição [4, p. 17]." width="60%" />
<p class="caption">
Figura 2.11: Diferentes níveis de exposição <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 17]</span>.
</p>
</div>
<p>Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas como são capturadas imagens coloridas?
Imagens coloridas utilizam, especialmente, o formato RGB, que significa <em>Red-Green-Blue</em>, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na Figura <a href="formacaoImagem.html#fig:componentes">2.12</a>, podemos ver uma imagem com seus componentes separados:</p>

<div class="figure" style="text-align: center"><span id="fig:componentes"></span>
<img src="imagens/02-formacao/componentes.png" alt="Imagem colorida separada em seus três componentes, em que Red é a vermelha, Green é a verde e Blue é a azul [4, p. 28]." width="65%" />
<p class="caption">
Figura 2.12: Imagem colorida separada em seus três componentes, em que <em>Red</em> é a vermelha, <em>Green</em> é a verde e <em>Blue</em> é a azul <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 28]</span>.
</p>
</div>
<p>Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na Figura <a href="formacaoImagem.html#fig:tressensores">2.13</a>. Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo.</p>

<div class="figure" style="text-align: center"><span id="fig:tressensores"></span>
<img src="imagens/02-formacao/tres_sensores.png" alt="Captura de imagem com três sensores [11, p. 242]." width="60%" />
<p class="caption">
Figura 2.13: Captura de imagem com três sensores <span class="citation">[<a href="#ref-teubner2019" role="doc-biblioref">11</a>, p. 242]</span>.
</p>
</div>
<p>Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada pixel. Isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na Figura <a href="formacaoImagem.html#fig:bayer">2.14</a>:</p>

<div class="figure" style="text-align: center"><span id="fig:bayer"></span>
<img src="imagens/02-formacao/bayer.png" alt="Filtro Bayer [4, p. 29]." width="80%" />
<p class="caption">
Figura 2.14: Filtro Bayer <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 29]</span>.
</p>
</div>
<p>Podemos perceber que ocorre uma maior ocorrência das cores verdes. Isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase à sua captura. Na Figura <a href="formacaoImagem.html#fig:sensorarray">2.15</a>, temos um esquema de como cada pixel recebe informação de somente uma cor, por meio da filtragem. Nesse esquema a luz que entra (<em>Incoming light</em>) é filtrada e somente a cor de interesse consegue passar. Após isso, ela chega a malha de sensores (<em>sensor array</em>):</p>

<div class="figure" style="text-align: center"><span id="fig:sensorarray"></span>
<img src="imagens/02-formacao/sensorarray.png" alt="Sensores com padrão Bayer [12]." width="60%" />
<p class="caption">
Figura 2.15: Sensores com padrão Bayer <span class="citation">[<a href="#ref-img:sensorarray" role="doc-biblioref">12</a>]</span>.
</p>
</div>
<p>Vemos na Figura <a href="formacaoImagem.html#fig:sensorarray">2.15</a> que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos pixels referentes às outras cores. As informações desses pixels são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos pixels vizinhos.</p>
</div>
<div id="amostragem-e-quantização" class="section level2">
<h2><span class="header-section-number">2.4</span> Amostragem e Quantização</h2>
<p>Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns). Ainda como etapas da aquisição de imagens, serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto.</p>
<div id="amostragem" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Amostragem</h3>
<p>Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>, na qual a Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto.</p>
<p>A Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha <span class="math inline">\(\overline{AB}\)</span>. Nas extremidades do gráfico na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(b), a intensidade é mais alta devido a parte branca da imagem. Já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(c).</p>
<p>Na prática, esse procedimento de amostragem é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo, teríamos que ter um número infinito de pixels. Como isso não é possível, recorremos a opção de utilizar o maior número de pixels possíveis. Quanto mais pixels houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 34]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:amostragemquant"></span>
<img src="imagens/02-formacao/amostragemquant.png" alt="Produzindo uma imagem digital. (a) Imagem contínua. (b) Linha de varredura de \(A\) a \(B\) na imagem contínua utilizada para ilustrar os conceitos de amostragem e quantização. (c) Amostragem e quantização. (d) Linha de varredura digital. [2, p. 34]" width="70%" />
<p class="caption">
Figura 2.16: Produzindo uma imagem digital. (a) Imagem contínua. (b) Linha de varredura de <span class="math inline">\(A\)</span> a <span class="math inline">\(B\)</span> na imagem contínua utilizada para ilustrar os conceitos de amostragem e quantização. (c) Amostragem e quantização. (d) Linha de varredura digital. <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 34]</span>
</p>
</div>
</div>
<div id="quantização" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Quantização</h3>
<p>Na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na Figura <a href="formacaoImagem.html#fig:amostragemquant">2.16</a>(c).</p>
<p>Na prática, geralmente a etapa de quantização é realizada diretamente no hardware utilizando um conversor analógico-digital <span class="citation">[<a href="#ref-burger2009" role="doc-biblioref">10</a>, p. 8]</span>. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo <span class="citation">[<a href="#ref-bovik2009essential" role="doc-biblioref">13</a>, p. 9]</span>.
No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos <span class="math inline">\(\{0,\dots, L-1\}\)</span>, onde <span class="math inline">\(L\)</span> é uma potência de dois, ou seja, <span class="math inline">\(L = 2^k\)</span> <span class="citation">[<a href="#ref-bovik2009essential" role="doc-biblioref">13</a>, p. 10]</span>. Isso significa que <span class="math inline">\(L\)</span> é o número de tons de cinza que podem ser representados com uma quantidade <span class="math inline">\(k\)</span> de bits. Em muitas situações é utilizado <span class="math inline">\(k = 8\)</span>, ou seja, temos <span class="math inline">\(256\)</span> níveis de cinza.
Ao realizar a quantização e a amostragem linha por linha no objeto da Figura <a href="formacaoImagem.html#fig:quantizacao">2.17</a>(a) é produzida uma imagem digital bidimensional como na Figura <a href="formacaoImagem.html#fig:quantizacao">2.17</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:quantizacao"></span>
<img src="imagens/02-formacao/quantizacao.png" alt="(a) Imagem contínua projetada em uma matriz de sensores. (b) Resultado da amostragem e quantização da imagem. [2, p. 35]" width="60%" />
<p class="caption">
Figura 2.17: (a) Imagem contínua projetada em uma matriz de sensores. (b) Resultado da amostragem e quantização da imagem. <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 35]</span>
</p>
</div>
</div>
</div>
<div id="formacaoImg" class="section level2">
<h2><span class="header-section-number">2.5</span> Definição de imagem digital</h2>
<p>Uma imagem pode ser definida como uma função bidimensional, <span class="math inline">\(f(x, y)\)</span>, em que <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> são coordenadas espaciais (plano), e a amplitude de <span class="math inline">\(f\)</span> em qualquer par de coordenadas <span class="math inline">\((x, y)\)</span> é chamada de intensidade ou nível de cinza da imagem nesse ponto <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 36]</span>. Quando <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> e os valores de intensidade de <span class="math inline">\(f\)</span> são quantidades finitas e discretas, chamamos de imagem digital.</p>
<p>A função <span class="math inline">\(f(x, y)\)</span> pode ser representada na forma de uma matriz MxN como na equação <a href="formacaoImagem.html#eq:imagemDigital">(2.7)</a>, em que as <span class="math inline">\(M\)</span> linhas são identificadas pelas coordenadas em <span class="math inline">\(x\)</span>, e as <span class="math inline">\(N\)</span> colunas em <span class="math inline">\(y\)</span>. Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, pixel ou <em>pel</em>. O formato numérico da matriz, imagem <a href="formacaoImagem.html#fig:imagemdigital">2.18</a>, é apropriado para o desenvolvimento de algoritmos, representado pela Equação <a href="formacaoImagem.html#eq:imagemDigital">(2.7)</a>.</p>
<p><span class="math display" id="eq:imagemDigital">\[f(x,y) = \begin{bmatrix}
 f(0,0)   &amp; f(0,1)     &amp; \cdots &amp; f(0,N-1)    \\ 
 f(1,0)   &amp; f(1,1)     &amp; \cdots &amp; f(1, N-1)   \\ 
 \vdots   &amp; \vdots    &amp; &amp;         \vdots      \\ 
 f(M-1,0) &amp; f(M-1, 1)  &amp; \cdots &amp; f(M-1, N-1)
\end{bmatrix}
\tag{2.7}
\]</span></p>

<div class="figure" style="text-align: center"><span id="fig:imagemdigital"></span>
<img src="imagens/02-formacao/imagemdigital.png" alt="Representações da imagem digital [2, p. 36]." width="70%" />
<p class="caption">
Figura 2.18: Representações da imagem digital <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 36]</span>.
</p>
</div>
<p>Na Figura <a href="formacaoImagem.html#fig:imagemdigital">2.18</a>(a) temos a representação da imagem em 3-D, onde a intensidade de cada pixel é representada no eixo <span class="math inline">\(z\)</span>, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na Figura <a href="formacaoImagem.html#fig:imagemdigital">2.18</a>(b), formato que seria visualizado em um monitor ou uma fotografia <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 36]</span>. Em cada ponto da Figura <a href="formacaoImagem.html#fig:imagemdigital">2.18</a>(a), o nível de cinza é proporcional ao valor da intensidade <span class="math inline">\(f\)</span>, assumindo valores <span class="math inline">\(0\)</span>, <span class="math inline">\(0.5\)</span> ou <span class="math inline">\(1\)</span>. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco.</p>
<p>Note que na Figura <a href="formacaoImagem.html#fig:imagemdigital">2.18</a>, a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo <span class="math inline">\(x\)</span> positivo direcionado para baixo e o eixo <span class="math inline">\(y\)</span> positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 36]</span>.
De acordo com o tamanho da matriz (MxN) e dos níveis discretos de tons de cinza (<span class="math inline">\(L = 2^k\)</span>) que os pixels podem assumir é possível determinar o número, <span class="math inline">\(b\)</span>, de bits necessários para armazenar uma imagem digitalizada:</p>
<p><span class="math display" id="eq:tamanhoBits">\[
b = M × N × k
\tag{2.8}
\]</span></p>
<p>Quando uma imagem pode ter <span class="math inline">\(2^k\)</span> níveis de intensidade, geralmente ela é denominada como uma “imagem de <span class="math inline">\(k\)</span> bits”. Por exemplo, uma imagem com <span class="math inline">\(256\)</span> níveis discretos de intensidade é chamada de uma imagem de <span class="math inline">\(8\)</span> bits. A Figura <a href="formacaoImagem.html#fig:tabelabits">2.19</a> mostra o número de bits utilizados para armazenar imagens quadradas de dimensão (NxN) para diferentes valores de <span class="math inline">\(N\)</span> e <span class="math inline">\(k\)</span>. O número de níveis de intensidade (<span class="math inline">\(L\)</span>) correspondente a cada valor de <span class="math inline">\(k\)</span> é mostrado entre parênteses. Observa-se na Figura <a href="formacaoImagem.html#fig:tabelabits">2.19</a> que uma imagem de <span class="math inline">\(8\)</span> bits com dimensões 1024x1024 exigiria aproximadamente 1MB para armazenamento.</p>

<div class="figure" style="text-align: center"><span id="fig:tabelabits"></span>
<img src="imagens/02-formacao/tabelabits.png" alt="Número de bits de armazenamento para vários valores de \(N\) e \(k\) [2, p. 38]." width="100%" />
<p class="caption">
Figura 2.19: Número de bits de armazenamento para vários valores de <span class="math inline">\(N\)</span> e <span class="math inline">\(k\)</span> <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 38]</span>.
</p>
</div>
</div>
<div id="resolução-espacial-e-de-intensidade" class="section level2">
<h2><span class="header-section-number">2.6</span> Resolução espacial e de intensidade</h2>
<p>Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho (MxN) em quantidades de pixels. Outra medida para especificar a resolução espacial é a densidade de pixels, podendo ser expressa como pontos (pixels) por unidade de distância, comumente <em>dots per inch</em> (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2400 dpi <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 38]</span>.</p>
<p>A Figura <a href="formacaoImagem.html#fig:resolucaoespacial">2.20</a> mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A Figura <a href="formacaoImagem.html#fig:resolucaoespacial">2.20</a>(a) tem resolução 512x512, e a resolução das demais <a href="formacaoImagem.html#fig:resolucaoespacial">2.20</a>(b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do pixel para deixar mais evidente a perda de detalhes nas imagens de baixa resolução.</p>

<div class="figure" style="text-align: center"><span id="fig:resolucaoespacial"></span>
<img src="imagens/02-formacao/resolucaoespacial.png" alt="Efeitos da redução da resolução espacial [3, p. 21]." width="50%" />
<p class="caption">
Figura 2.20: Efeitos da redução da resolução espacial <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 21]</span>.
</p>
</div>
<p>A resolução de intensidade ou profundidade corresponde ao número de bits (<span class="math inline">\(k\)</span>) utilizados para estabelecer os níveis de cinza da imagem (<span class="math inline">\(L=2^k\)</span>). Por exemplo, em uma imagem cuja intensidade é quantizada em <span class="math inline">\(L= 256\)</span> níveis, a profundidade é de <span class="math inline">\(k = 8\)</span> bits por pixel.</p>
<p>Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura <a href="formacaoImagem.html#fig:reducaoprofundidade">2.21</a>. A Figura <a href="formacaoImagem.html#fig:reducaoprofundidade">2.21</a>(a) apresenta <span class="math inline">\(256\)</span> níveis de cinza (<span class="math inline">\(k = 8\)</span>). As Figuras <a href="formacaoImagem.html#fig:reducaoprofundidade">2.21</a>(b) e (c) foram geradas pela redução do número de bits <span class="math inline">\(k = 4\)</span> e <span class="math inline">\(k = 2\)</span>, respectivamente, mas mantendo a mesma dimensão.</p>

<div class="figure" style="text-align: center"><span id="fig:reducaoprofundidade"></span>
<img src="imagens/02-formacao/reducaoprofundidade.png" alt="Efeitos da redução de profundidade [4, p. 19]." width="90%" />
<p class="caption">
Figura 2.21: Efeitos da redução de profundidade <span class="citation">[<a href="#ref-moeslund2012" role="doc-biblioref">4</a>, p. 19]</span>.
</p>
</div>
</div>
<div id="pixels" class="section level2">
<h2><span class="header-section-number">2.7</span> Pixels</h2>
<p>Os pixels são elementos principais na formação da imagem, sua importância determina topologicamente as características da imagem, nesta seção mostraremos conceitos básicos da topologia da imagem em relação seus elementos (pixels). Em memória computacional os pixels da imagem são representados nos formatos de matrizes (bidimensional ou tridimensional) já abordado na seção <a href="formacaoImagem.html#formacaoImg">Seção 2.5</a>, através delas é permitido aplicar operações sobre seus elementos (pixels), efetuar análises, identificação de padrões, acessar regiões, alterar cores, posições e tamanho tendo como referência espaço projetado usando as coordenadas do plano.</p>
<p>Propriedades topológicas dos pixels da imagem:</p>
<ul>
<li>Vizinhança (4, D, 6, 8 e 26)</li>
<li>Conectividade</li>
<li>Adjacência</li>
<li>Caminho</li>
<li>Componente Conexa</li>
<li>Borda e Interior</li>
<li>Medidas de Distância</li>
<li>Operações Lógico-aritméticas</li>
</ul>
<div id="vizin" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Vizinhança</h3>
<p>As vizinhanças de um elemento (pixels) <span class="math inline">\(f\)</span> pertencente ao conjunto <span class="math inline">\(S\)</span> (matriz de pixels) com coordenadas <span class="math inline">\((x,y)\)</span> possui vizinhos dos tipos 4, D e 8.
Para definir uma vizinhança-4 (denotado <span class="math inline">\([N_4(f)])\)</span> de <span class="math inline">\(f(x,y)\)</span> ilustrado na Figura <a href="formacaoImagem.html#fig:vizinhanca">2.22</a>, <span class="math inline">\(f_{(1,1)}\)</span> possui quatro vizinhos, dois na horizontal e outros dois na vertical cuja as coordenadas da vizinhança são <span class="math inline">\((x + 1, y), (x - 1, y), (x, y + 1) \ e \ (x, y -1)\)</span>. Os quatros vizinhos diagonais de <span class="math inline">\(f_{(5, 1)}\)</span> são de coordenadas <span class="math inline">\((x - 1 , y - 1), (x - 1, y + 1), (x + 1, y + 1)\)</span>, que constituem o conjunto <span class="math inline">\([N_d(f)]\)</span>. A vizinhança-8 (denotado <span class="math inline">\([N_8(f)]\)</span> e ilustrada na Figura <a href="formacaoImagem.html#fig:vizinhanca">2.22</a> <span class="math inline">\(f_{(4, 5)}\)</span>, é definida como <span class="math inline">\(N_8(f)= N_4(f) \ U \ Nd(f)\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:vizinhanca"></span>
<img src="imagens/02-formacao/vizinhanca4-8-d.png" alt="Vizinhança de 4-D e 8 de um pixel, matriz de pixel da imagem." width="40%" />
<p class="caption">
Figura 2.22: Vizinhança de 4-D e 8 de um pixel, matriz de pixel da imagem.
</p>
</div>
</div>
<div id="contiv" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Conectividade</h3>
<p>A conectividade entre os pixels é outro conceito importante na topologia da imagem, usada para determinar limites de objetos e componentes de regiões na imagem. Dois pixels são conexos entre si primeiro verificar-se seus tipos de <a href="formacaoImagem.html#vizin">(vizinhos Seção 2.7.1)</a>, de seguida se existe similaridade na intensidade de cinza, cor ou textura. Por exemplo, em uma imagem binária, em que os pixels podem assumir os valores <span class="math inline">\(0\)</span> ou <span class="math inline">\(1\)</span>, dois pixels podem ter vizinhança-4, mas somente serão considerados conexos se possuírem o mesmo valor. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 31]</span>.</p>
</div>
<div id="adja" class="section level3">
<h3><span class="header-section-number">2.7.3</span> Adjacência</h3>
<p>Um conjunto <span class="math inline">\(V\)</span> com valores de intensidade para definir adjacência, em uma imagem binária, com <span class="math inline">\(V={1}\)</span> e outra imagem com mesmo conjunto contendo escala de nível de cinza no limite <span class="math inline">\(0\)</span> a <span class="math inline">\(255\)</span> com vários elementos (pixels) o conjunto <span class="math inline">\(V\)</span> poderia ser qualquer subconjunto desses <span class="math inline">\(256\)</span> valores se eles estiverem conectados de acordo com o tipo de <a href="formacaoImagem.html#vizin">(vizinhança Seção 2.7.1)</a> especificado. <span class="citation">[<a href="#ref-gonzalez2010" role="doc-biblioref">2</a>, p. 31]</span>.</p>
</div>
<div id="camin" class="section level3">
<h3><span class="header-section-number">2.7.4</span> Caminho</h3>
<p>Uma imagem com coordenada de pixels <span class="math inline">\((x_1 , y_1), (x_2 , y_2) … (x_n , y_n)\)</span> que determina a sequência do caminho dos variados pixels onde né o comprimento do caminho, e <span class="math inline">\((x_i , y_i)e (x_i + 1 , y_i + 1)\)</span> são adjacentes, tal que <span class="math inline">\(i = 1,2,...,n - 1\)</span>.Se na relação de conectividade considerar vizinhança-4, então existe um caminho-4; para vizinhança-8, tem-se um caminho-8. Exemplo de caminhos são mostrados na Figura <a href="formacaoImagem.html#fig:caminho">2.23</a> sendo que o caminho-4 possui comprimento <span class="math inline">\(10\)</span> e o caminho-8 possui comprimento <span class="math inline">\(7\)</span>. O conceito de caminho também pode ser estendido para imagens tridimensionais. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 31]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:caminho"></span>
<img src="imagens/02-formacao/caminho.png" alt="(a) Caminho-4, (b) Caminho-8. [3, p. 31]" width="40%" />
<p class="caption">
Figura 2.23: (a) Caminho-4, (b) Caminho-8. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 31]</span>
</p>
</div>
</div>
<div id="componente-conexa" class="section level3">
<h3><span class="header-section-number">2.7.5</span> Componente Conexa</h3>
<p>Componentes conexos de pixels de uma imagem é um conjunto de elementos (pixels) que de alguma forma estão conectados entre si, dois ou mais pixels são componentes conexos se existir um caminho seção 2.7.4 Caminho pertencente ao conjunto. A Figura @ref(fig:componente_conexa) mostra uma imagem bidimensional contendo três componentes conexos caso seja considerada a vizinhança-4 ou, então, dois componentes conexos se considerada a vizinhança-8. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 32]</span>.</p>
<p>(ref:componente_conexa) Componentes Conexos de uma imagem bidimensional <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 32]</span>.</p>
<div class="figure" style="text-align: center">
<img src="imagens/02-formacao/componente_conexos.png" alt="(ref:componente_conexa)" width="20%" />
<p class="caption">
(#fig:componente_conexa)(ref:componente_conexa)
</p>
</div>
</div>
<div id="borda-e-interior" class="section level3">
<h3><span class="header-section-number">2.7.6</span> Borda e Interior</h3>
<p>A borda de um componente conexo <a href="###%20Componente-Conexa">seção 2.7.5</a> <span class="math inline">\(S\)</span> em uma imagem bidimensional é o conjunto de pixels pertencentes ao componente e que possuem vizinhança-4 com um ou mais pixels externos a <span class="math inline">\(S\)</span> <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 32]</span>. Intuitivamente, a borda corresponde ao conjunto de pontos no contorno do componente conexo. O interior é o conjunto de pixels de <span class="math inline">\(S\)</span> que não estão em sua borda. A Figura <a href="formacaoImagem.html#fig:borda">2.24</a> mostra um exemplo de uma imagem binária com sua borda interior.</p>

<div class="figure" style="text-align: center"><span id="fig:borda"></span>
<img src="imagens/02-formacao/borda_interior.png" alt="Borda e interior de um componente, Figura do lado esquerdo, mostra a imagem original, a Figura do lado direito mostra os pixels da borda e interior. [3, p. 32]." width="40%" />
<p class="caption">
Figura 2.24: Borda e interior de um componente, Figura do lado esquerdo, mostra a imagem original, a Figura do lado direito mostra os pixels da borda e interior. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 32]</span>.
</p>
</div>
</div>
<div id="medidas-de-distância" class="section level3">
<h3><span class="header-section-number">2.7.7</span> Medidas de Distância</h3>
<p>As medidas de distância são aplicadas sobre os pixels e componentes da imagem, considerando <span class="math inline">\(D\)</span> uma função com os pixels <span class="math inline">\(f_1\)</span>, <span class="math inline">\(f_2\)</span> e <span class="math inline">\(f_3\)</span> de coordenadas <span class="math inline">\((x_1, y_1)\)</span>, <span class="math inline">\((x_2, y_2)\)</span>, <span class="math inline">\((x_3, y_3)\)</span>, para verificar se <span class="math inline">\(D\)</span> é uma função distância ou medida de distância, deve satisfazer as seguintes propriedades:</p>
<ol style="list-style-type: decimal">
<li><span class="math display" id="eq:medida1">\[
D(f_1,f_2) \geq 0 (D(f_1,f_2) = 0 \ se \ f_1 = f_2 \tag{2.9}
\]</span></li>
<li><span class="math display" id="eq:medida2">\[
(f_1,f_2) = D(f_2, f_1) \tag{2.10}
\]</span></li>
<li><span class="math display" id="eq:medida3">\[
D(f_1,f_3) \leq  D(f_1,f_2) + D(f_2,f_3) \tag{2.11}
\]</span></li>
</ol>
<p>Aplicabilidade da fórmula Euclidiana uma das formas de satisfazer as propriedades acima, desse modo podemos analisar a métrica de dois ou mais pixels de uma imagem, supondo que os pixels <span class="math inline">\(f_1 \ e \ f_2\)</span> de coordenadas <span class="math inline">\((x_1, x_2), (y_1, y_2)\)</span> para medir suas distância aplicamos:</p>
<p><span class="math display" id="eq:formEclidiana">\[D_4(f_1 , f_2) = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} 
\tag{2.12}
\]</span></p>
<p>Na medida de distância Euclidiana, todos os pixels menores ou igual de qualquer valor <span class="math inline">\(d\)</span> formam um disco de raio <span class="math inline">\(d\)</span> centro em <span class="math inline">\(f_1\)</span>. Como exemplo, os pontos com distância <span class="math inline">\(D_E \leq 3\)</span> de um ponto central <span class="math inline">\((x, y)\)</span> formam o conjunto mostra na Figura: <a href="formacaoImagem.html#fig:medidaD1">2.25</a></p>

<div class="figure" style="text-align: center"><span id="fig:medidaD1"></span>
<img src="imagens/02-formacao/medida_distancia1.png" alt="Conjunto de pontos com distância Euclidiana menor ou igual de ponto central. [3, p. 33]." width="40%" />
<p class="caption">
Figura 2.25: Conjunto de pontos com distância Euclidiana menor ou igual de ponto central. <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 33]</span>.
</p>
</div>
<p>A distância <span class="math inline">\(D_4\)</span> entre <span class="math inline">\(f_1\)</span> e <span class="math inline">\(f_2\)</span>, também denotada de <em>city-block</em>, é definida como:
<span class="math display" id="eq:formCity">\[
D_4(f_1 , f_2)   = |x_1 - x_2 | + |y_1 - y_2 |
\tag{2.13}
\]</span></p>
<p>Os pixels com uma distância <span class="math inline">\(D_4\)</span> de <span class="math inline">\(f_4\)</span> menor ou igual a algum valor <span class="math inline">\(d\)</span> formam um losango centrado em <span class="math inline">\(f_1\)</span>. Em particular, os pontos com distância <span class="math inline">\(1\)</span> são os pixels com vizinhança-4 de ponto central. Por exemplo, os pontos com distância <span class="math inline">\(D_4 \leq 3\)</span> de um ponto central <span class="math inline">\((x, y)\)</span> formam o conjunto mostrado na Figura <a href="formacaoImagem.html#fig:medidaD2">2.26</a></p>

<div class="figure" style="text-align: center"><span id="fig:medidaD2"></span>
<img src="imagens/02-formacao/medida_distancia2.png" alt="Conjunto de pontos com distância \(D_4\) menor ou igual a \(3\) de um ponto central [3, p. 33]." width="40%" />
<p class="caption">
Figura 2.26: Conjunto de pontos com distância <span class="math inline">\(D_4\)</span> menor ou igual a <span class="math inline">\(3\)</span> de um ponto central <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 33]</span>.
</p>
</div>
<p>A distância <span class="math inline">\(D_8\)</span> entre <span class="math inline">\(f_1\)</span> e <span class="math inline">\(f_2\)</span>, também denotado de <em>chessboard</em>, é definida como
<span class="math inline">\(D_8(f_1 , f_2) = max(| x_1 = x_2 | , | y_1 =y_2 |)\)</span>
Os pixels com uma distância <span class="math inline">\(D_8\)</span> de <span class="math inline">\(f_1\)</span> menor ou igual a algum valor <span class="math inline">\(d\)</span> formam um quadrado central em <span class="math inline">\(f_1\)</span>. Os pontos com distância <span class="math inline">\(1\)</span> são os pixels com a vizinhança-8 do ponto central. Por exemplo, os pontos com distância <span class="math inline">\(D_8 \leq 3\)</span> de um ponto central <span class="math inline">\((x, y)\)</span> formam o conjunto mostrado na Figura <a href="formacaoImagem.html#fig:medidaD3">2.27</a></p>

<div class="figure" style="text-align: center"><span id="fig:medidaD3"></span>
<img src="imagens/02-formacao/medida_distancia3.png" alt="Conjunto de pontos com distância \(D_4\) menor ou igual a \(3\) de um ponto central [3, p. 33]." width="40%" />
<p class="caption">
Figura 2.27: Conjunto de pontos com distância <span class="math inline">\(D_4\)</span> menor ou igual a <span class="math inline">\(3\)</span> de um ponto central <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 33]</span>.
</p>
</div>
</div>
<div id="operações-lógico-aritméticas" class="section level3">
<h3><span class="header-section-number">2.7.8</span> Operações Lógico-aritméticas</h3>
<p>Operações lógicas e aritméticas podem ser utilizadas para modificar imagens. Embora essas operações permitam uma forma simples de processamento, há uma grande variedade de aplicações em que tais operações podem produzir resultados de interesse prático <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 34]</span>.
Dadas duas imagens, <span class="math inline">\(f_1\)</span> e <span class="math inline">\(f_2\)</span>, as operações aritméticas mais comuns entre dois pixels <span class="math inline">\(f_1(x, y)\)</span> e <span class="math inline">\(f_2(x, y)\)</span> são a adição, subtração. multiplicação e divisão, definida de acordo com:</p>
<ul>
<li><p>Operações Aritméticas:</p>
<ol style="list-style-type: decimal">
<li><p>Adição: <span class="math inline">\(f_1(x_1, y_1) + f_2(x_2, y_2)\)</span>.</p></li>
<li><p>Subtração: <span class="math inline">\(f_1(x_1, y_1) - f_2(x_2, y_2)\)</span>.</p></li>
<li><p>Multiplicação: <span class="math inline">\(f_1(x_1, y_1) \cdot f_2(x_2, y_2)\)</span></p></li>
<li><p>Divisão: <span class="math inline">\(f_1(x_1, y_1) \div f_2(x_2, y_2)\)</span></p></li>
</ol></li>
</ul>
<p>Durante o processo de operações aritméticas sobre uma imagem alguns cuidados devem ser tomados para contornar a produção de valores fora do intervalo de níveis de cinza sobre a imagem original no processo. Neste caso, a operação de adição de duas imagens com <span class="math inline">\(256\)</span> níveis de cinza, pode resultar em um número maior que valor <span class="math inline">\(255\)</span> para determinados pixels, por outro lado, a operação de subtração de duas imagens podem resultar em valores negativos para alguns pixels durante o processo. Para solucionar esse problema depois do processo aritmético (aplicação aritmética), realizar uma transformação de escala de cinza na imagem resultante para manter seus valores do intervalo adequado <span class="citation">[<a href="#ref-pedrini2008" role="doc-biblioref">3</a>, p. 34]</span>.</p>

</div>
</div>
</div>
<h3>Refêrencias</h3>
<div id="refs" class="references">
<div id="ref-gonzalez2010">
<p>[2] R. C. Gonzalez e R. C. Woods, <em>Processamento digital de imagens</em>, 3º ed. São Paulo: Pearson Prentice Hall, 2010.</p>
</div>
<div id="ref-pedrini2008">
<p>[3] H. Pedrini e W. Robson Schwartz, <em>Análise de imagens digitais: princípios, algoritmos e aplicações</em>, 3º ed. São Paulo: Thomson Learning Edicoes Ltda, 2007.</p>
</div>
<div id="ref-moeslund2012">
<p>[4] T. B. Moeslund, <em>Introduction to video and image processing: Building real systems and applications</em>. Springer Science &amp; Business Media, 2012.</p>
</div>
<div id="ref-cuthill2017">
<p>[9] I. C. Cuthill <em>et al.</em>, “The biology of color”, <em>Science</em>, vol. 357, nº 6350, 2017.</p>
</div>
<div id="ref-burger2009">
<p>[10] W. Burger, M. J. Burge, M. J. Burge, e M. J. Burge, <em>Principles of digital image processing</em>, vol. 111. Springer, 2009.</p>
</div>
<div id="ref-teubner2019">
<p>[11] U. Teubner e H. J. Brückner, <em>Optical Imaging and Photography: Introduction to Science and Technology of Optics, Sensors and Systems</em>. Walter de Gruyter GmbH &amp; Co KG, 2019.</p>
</div>
<div id="ref-img:sensorarray">
<p>[12] W. Commons, “Bayer pattern on sensor profile”. 2006, [Online]. Disponível em: <a href="https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor_profile.svg">https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor_profile.svg</a>.</p>
</div>
<div id="ref-bovik2009essential">
<p>[13] A. C. Bovik, <em>The essential guide to image processing</em>. Academic Press, 2009.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="transformacoesGeometricas.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": null
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/covap-utfpr/pdi/edit/master/02-formacao_imagem.Rmd",
"text": "Editar "
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"citation_package": "biblatex"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
