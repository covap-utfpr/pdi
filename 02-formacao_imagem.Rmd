# Formação da imagem 

Existem diferentes tipos de fontes utilizadas para geração de imagens, sendo que a mais comum é o espectro eletromagnético na faixa de ondas visíveis. Outras fontes de energia também podem ser utilizadas como energia mecânica na forma de ultrassom, feixe de elétrons em microscópio eletrônicos, ondas de rádio no radar, etc. Cada fonte necessita de um método específico de captura. Para ondas eletromagnéticas pode ser usada uma câmera fotográfica equipada com sensores adequados ao comprimento de onda. Porém, para outras fontes, é necessário que o computador sintetize a imagem, como o microscópio eletrônico. 

Como já mencionado no tópico de [Introdução](#intro), o espectro eletromagnético contém diferentes frequências de energia, mas os humanos conseguem enxergar somente uma pequena parte desse espectro, conhecido como luz visível. Isso se deve ao fato de que nossos olhos evoluíram para serem sensíveis a essa faixa de luz, que vem da luz solar e nos ajuda a realizar nossas atividades cotidianas. Existem outros animais, como pássaros e insetos, que conseguem ver luz em outras faixas de onda, como a ultravioleta [@cuthill2017, p.2]. Caso nossos olhos fossem também sensíveis a outras frequências, como por exemplo a de rádio, nossos celulares e torres telefônicas pareceriam lanternas [@moeslund2012, p.8].

A luz sem cor, isto é, a luz com maior energia dentro do espectro visível humano, é chamada de luz monocromática (ou acromática). Pelo fato de a intensidade da luz monocromática ser percebida como variações de preto a tons de cinza até chegar ao branco, utiliza-se o termo nível de cinza ou escala de cinza. Já a luz cromática (colorida) cobre o espectro de energia eletromagnética na faixa de 0,43 a 0,79 $\mu m$. Além da frequência, três medidas básicas são utilizadas para descrever a qualidade de uma fonte de luz cromática: radiância, luminância e brilho. A radiância é a quantidade total de energia que é emitida pela fonte de luz e é normalmente medida em watts (W). A luminância, medida em lumens (lm), mede a quantidade de energia que um observador percebe de uma fonte de luz. O brilho, que incorpora a noção acromática de intensidade, é um descritor subjetivo da percepção da luz, então é praticamente impossível mensurar [@gonzalez2010, p.28].

Nos próximos tópicos iremos explorar alguns conceitos e o funcionamento da aquisição de imagem. Esse processo é um pouco complexo e envolve conceitos de ótica, que serão apresentados, de maneira introdutória, a seguir.

## Câmera *pinhole* e geometria

Na Figura \@ref(fig:aquisicaoimagem) temos um esquema básico de como geralmente ocorre a aquisição de imagens. Primeiramente a energia em forma de luz, vinda de uma fonte, atinge um objeto e é refletida. A parte refletida é capturada por um dispositivo, como uma câmera fotográfica.

(ref:aquisicaoimagem) Representação de uma típica captura de imagem [@moeslund2012, p.8].

```{r aquisicaoimagem, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:aquisicaoimagem)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/aquisicao_imagem.png'))

```

Baseado nesse princípio pode-se criar um dispositivo muito simples para captura de imagens, conhecido como câmera *pinhole* (do inglês buraco de alfinete) ou câmera escura. Este dispositivo consiste basicamente de uma caixa fechada com somente um pequeno orifício, tão pequeno quanto possível, por onde os raios de luz possam entrar. Mas por que utilizar somente uma pequena entrada? Como podemos ver na Figura \@ref(fig:barreiraluz), se tentarmos realizar a captura da imagem, usando filme fotográfico ou um sensor, sem essa limitação, a área sensível acaba recebendo raios de inúmeras direções, que acabam se misturando tendo como resultado uma imagem ruidosa. Com a barreira de entrada, limitamos a quantidade de luz e conseguimos resultados melhores.

(ref:barreiraluz) Introdução de barreira para captura de imagem [@moeslund2012, p.11].

```{r barreiraluz, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:barreiraluz)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/barreiraluz.png'))

```

Na Figura \@ref(fig:barreiraluz) percebemos que a imagem resultante acaba invertida. Isso pode ser explicado através de algumas relações geométricas que serão apresentadas a seguir.

(ref:geometriapinhole) Geometria de uma câmera pinhole [@burger2009, p.5].

```{r geometriapinhole, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:geometriapinhole)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/geometriapinhole.png'))

```

Na Figura \@ref(fig:geometriapinhole), considerando que o  eixo óptico corresponde a uma reta perpendicular ao orifício de entrada de luz, que o objeto está localizado a uma distância $Z$ da abertura e a uma distância $Y$ o eixo óptico, podemos definir a altura $y$ e a largura $x$ da projeção do objeto utilizando a simetria de triângulos:

$$
-\frac{y}{f}=\frac{Y}{Z}\Leftrightarrow y=-f\frac{Y}{Z} \ \ \ \ \text{e}\ \ \ \ -\frac{x}{f}=\frac{X}{Z} \Leftrightarrow x=-f\frac{X}{Z}
(\#eq:semelhancaTriangulos)
$$

A variável $f$ nessa equação \@ref(eq:semelhancaTriangulos) se refere  a distância focal, que é, nesse caso, o tamanho da caixa da câmera, pois a imagem é formada em seu fundo [@burger2009, p.4]. Os sinais negativos das equações significam que a imagem projetada está rotacionada a $180^\circ{}$ verticalmente e horizontalmente devido a semelhança de triângulos [@burger2009, p.5], como podemos confirmar na Figura \@ref(fig:geometriapinhole). Câmeras que usavam esse princípio de funcionamento foram utilizadas a partir do século XIII mas hoje em dia não são utilizadas, somente por hobbistas ou curiosos, já que tem muitas desvantagens como precisar de um longo tempo de exposição para captura da imagem. 

As câmeras mais modernas não possuem somente uma pequena entrada para luz, mas um sistema de lentes que focam a luz recebida no sensor. Discutiremos a seguir alguns dos conceitos por trás disso.

## Lentes

(ref:lente) Ação de uma lente sobre os raios de luz [@moeslund2012, p.12].

```{r lente, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:lente)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/lente.png'))

```

Como podemos ver na Figura \@ref(fig:lente), em cada ponto de um objeto há inúmeros raios de luz refletidos - neste caso são ilustrados três - e os que chegam à lente são focalizados no seu lado direito. As imagens são capturadas colocando o sensor exatamente onde esses raios são focalizados, ou seja, onde a imagem é formada.
O ponto $F$ onde os raios paralelos se cruzam é conhecido como Ponto Focal. A distância $f$, que vai do centro óptico $O$ até $F$ é conhecida como Distância Focal.
Definindo a distância do objeto real até a lente como $g$ e a distância até a formação da imagem após passar pela lente como $b$ temos que:

$$
\frac{1}{g}+\frac{1}{b}=\frac{1}{f}
(\#eq:relacaoDistancias)
$$

Como $f$ e $b$ estão normalmente entre 1mm e 100mm isso mostra que $\frac{1}{g}$ não tem quase nenhum impacto na equação \@ref(relacaoDistancias) e significa que $b = f$. Isso significa que a imagem dentro da câmera é formada muito próxima ao ponto focal.
Outro ponto importante das lentes é conhecido como *zoom* óptico. Isto deriva de um aspecto das lentes de que o tamanho do objeto na imagem formada, $B$, aumenta quando $f$ aumenta. Podemos representar isso na seguinte equação \@ref(relacaoTamanhoObjetoImagem), onde $g$ é o tamanho real do objeto:

$$
\frac{b}{B}=\frac{g}{G}
(\#eq:relacaoTamanhoObjetoImagem)
$$

Na prática $f$ é alterado através de mudanças na distância entre diferentes lentes dentro do sistema óptico da câmera, aqui estamos usando somente uma lente para exemplificar de maneira fácil alguns de seus conceitos básicos.
Se o $f$ for constante, quando alteramos a distância do objeto, no caso $g$, sabemos que $b$ também aumenta, isso significa que o sensor tem que ser movido mais para trás, pois a imagem estará sendo formada mais longe da lente. Se não movermos $b$ temos uma imagem fora de foco, como mostrado a seguir. Quando usamos uma câmera, o ato de colocar a imagem em foco significa que estamos alterando $b$ para que a imagem seja formada onde o sensor está localizado, para que a imagem esteja em foco.

(ref:foco) Uma imagem focada em (a) e desfocada em (b) [@moeslund2012, p.11].

```{r foco, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:foco)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/foco.png'))
```

A Figura \@ref(fig:foco) ilustra exatamente o que significa uma imagem estar fora de foco, no sensor cada pixel tem um tamanho específico, quando a imagem está em foco os raios de um ponto específico estão dentro da área do *pixel*. Uma imagem fica fora de foco quando os raios de outros pontos também interceptam o *pixel*, gerando uma mistura de diferentes pontos.

(ref:profundidade) Profundidade de campo [@moeslund2012, p.13].

```{r profundidade, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:profundidade)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/profundidade.png'))
```

A Figura \@ref(fig:profundidade) apresenta outro ponto muito importante, chamado Profundidade de Campo (*Depth of field*), que representa a soma das distâncias $g_l$ e $g_r$, que representam o quanto os objetos podem ser movidos e permanecerem em foco.

Um tópico que também tem muita importância na aquisição de imagens é o Campo de Visão (*Field of View* ou FOV) que representa a área observável de uma câmera. Na Figura \@ref(fig:campovisao) essa área observável é denotada pelo ângulo $V$. O FOV de uma câmera depende de alguns aspectos, como sua distância focal e tamanho do sensor. Em muitos casos os sensores não são quadrados, mas retangulares, então para representarmos matematicamente o campo de visão, utilizamos as equações seguintes para o FOV vertical e horizontal:
$$
FOV_x = 2*\tan^{-1}\left(\frac{\frac{comprimento\ do\ sensor}{2}}{f}\right) \ \ \ \ \text{e}\ \ \ \  FOV_y = 2*\tan^{-1}\left(\frac{\frac{altura\ do\ sensor}{2}}{f}\right)
(\#eq:FOV)
$$

(ref:campovisao) Campo de visão [@moeslund2012, p.14].

```{r campovisao, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:campovisao)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/campovisao.png'))
```

Por exemplo, se tivermos uma câmera com um sensor que tenha o comprimento de 14mm, altura de 10mm e uma distância focal de 5mm temos:
$$
FOV_x=2*tan^{-1}\left(\frac{7}{5}\right)=108.9^{\circ} \ \ \ \ \text{e}\ \ \ \  FOV_y=2*tan^{-1}(1)=90^{\circ}
(\#eq:FOVexemplo)
$$

Isso significa que essa câmera tem uma área observável de 108.9º horizontalmente e 90º verticalmente. Na Figura \@ref(fig:diferentesprofundidades) temos o mesmo objeto fotografado com diferentes profundidades de campo: 

(ref:diferentesprofundidades) Diferentes profundidades de campo. Em (a) ? em (b)? e em (c)? [@moeslund2012, p.15].

```{r diferentesprofundidades, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:diferentesprofundidades)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/02-formacao/diferentesprofundidades.png'))
```

Outros dois fatores importantes na aquisição de imagem são a abertura e o obturador. A abertura é, em uma câmera, o mesmo que a íris do olho humano. É ele que controla a quantidade de luz que chega ao sensor. E o obturador é um dispositivo que controla o tempo ao qual o sensor será exposto à luz para a captura da imagem.

## Sensor

Existem dois tipos principais de sensores que são empregados em dispositivos fotográficos. Um deles é o CCD (*Charge-coupled device*), que é usado principalmente em aplicações mais específicas ou que precisam de uma qualidade muito alta, e o CMOS (*Complementary metal–oxide semiconductor*), usado em casos mais gerais, como câmeras de celulares.
Após a luz passar por todo o sistema de lentes ela chega a esses sensores, que tem sua estrutura exemplificada na Figura \@ref(fig:sensor), conhecido como PDA (*Photodiode Array*):

(ref:sensor) Sensor (área matricial de células), *Single Cell* (uma única célula sensor) [@moeslund2012, p.17].

```{r sensor, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:sensor)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/sensor.png'))
```

Como podemos ver, o sensor consiste em várias pequenas células, cada uma com um *pixel*, que recebe a energia luminosa e a converte para um número digital. Quanto maior a incidência de luz em um *pixel*, maior a quantidade de energia e por isso maior será o valor do número gerado. O trabalho de controlar esse tempo de exposição é do obturador da câmera, sendo que um tempo muito longo ou muito curto podem produzir efeitos indesejados nas imagens obtidas, por isso a maioria das câmeras contam com um sistema que controla automaticamente esse tempo para o melhor resultado. Na Figura \@ref(fig:exposicao) podemos ver isso em uma imagem real, na primeira temos uma imagem que foi capturada com a exposição correta (*correctly exposed*), logo em seguida temos uma que sofreu de superexposição (*overexposed*)  e na terceira temos uma com subexposição (*underexposed*). Por último temos uma imagem que sofre com o movimento do objeto cuja imagem estava sendo capturada, o que ocasionou o borramento (*motion blur*).

(ref:exposicao) Diferentes níveis de exposição [@moeslund2012, p.17].

```{r exposicao, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:exposicao)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/02-formacao/exposicao.png'))
```

Vimos até agora, principalmente, como se capturam imagens em tons de cinza, mas como são capturadas imagens coloridas?
Imagens coloridas utilizam, especialmente, o formato RGB, que significa *Red-Green-Blue*, ou seja, é formado pelas cores primárias vermelho, verde e azul. Podemos a partir disso gerar imagens coloridas tendo as informações sobre sua intensidade de cada uma dessas cores. Na Figura \@ref(fig:componentes) podemos ver uma imagem com seus componentes separados:

(ref:componentes) Imagem colorida separada em seus três componentes, em que *Red* é a vermelha, *Green* é a verde e *Blue* é a azul [@moeslund2012, p.28].

```{r componentes, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:componentes)', fig.align='center', out.width='65%'}
knitr::include_graphics(rep('imagens/02-formacao/componentes.png'))
```

Precisamos assim dessas três informações para formar uma imagem colorida, uma das implementações pensadas para resolver esse problema foi a de dividir a luz de entrada e enviar cada um dos raios filtrados para um sensor diferente, como representado na Figura \@ref(fig:tressensores). Apesar de essa implementação funcionar, ela não se tornou o padrão pelo fato de que utilizar três sensores faz com que seu preço de construção fique elevado e o projeto em si muito mais complexo.

(ref:tressensores) Captura de imagem com três sensores [@teubner2019, p.242].

```{r tressensores, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:tressensores)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/02-formacao/tres_sensores.png'))
```

Ao invés disso, as câmeras modernas utilizam somente um sensor e fazem uso de um filtro que separa uma das três cores para cada *pixel*. Isso porque os fotodiodos não reconhecem por si só as cores, mas a intensidade, o que nos levaria a ter somente fotos com tons de cinza. Esse filtro pode conter diferentes configurações, sendo que uma das mais utilizadas é o filtro Bayer, que pode ser visto na Figura \@ref(fig:bayer):

(ref:bayer) Filtro Bayer [@moeslund2012, p.29].

```{r bayer, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:bayer)', fig.align='center', out.width='80%'}
knitr::include_graphics(rep('imagens/02-formacao/bayer.png'))
```

Podemos perceber que ocorre uma maior ocorrência das cores verdes. Isso se deve ao fato de que o olho humano é mais sensível a essa cor, logo se dá uma maior ênfase a sua captura.  Na Figura \@ref(fig:sensorarray) temos um esquema de como cada *pixel* recebe informação de somente uma cor, por meio da filtragem. Nesse esquema a luz que entra (*Incoming light*) é filtrada e somente a cor de interesse consegue passar. Após isso, ela chega a malha de sensores (*sensor array*):

(ref:sensorarray) Sensores com padrão Bayer [@img:sensorarray].

```{r sensorarray, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:sensorarray)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/02-formacao/sensorarray.png'))
```

Vemos na Figura \@ref(fig:sensorarray) que temos ao final três grupos de informações diferentes mas que têm dados faltantes nos *pixels* referentes às outras cores. As informações desses *pixels* são preenchidas em um processo chamado interpolação que completa as informações baseada nos valores dos *pixels* vizinhos.

## Amostragem e Quantização

Nas seções anteriores foram apresentados processos para a captura de imagens a partir de sensores (principalmente de câmeras comuns).  Ainda como etapas da aquisição de imagens, serão abordados nesta seção a amostragem e a quantização, procedimentos em que os dados contínuos dos sensores são convertidos para o formato digital, que é discreto.

### Amostragem

Na amostragem ocorre a discretização espacial, ou seja, a conversão de um espaço contínuo em um espaço discreto, que pode ser representado digitalmente. Este procedimento é exemplificado na Figura \@ref(fig:amostragemquant), na qual a Figura \@ref(fig:amostragemquant) (a) representa um objeto de atributos contínuos, e a linha AB é um segmento horizontal do objeto.

A Figura \@ref(fig:amostragemquant) (b) contém a representação da amplitude (nível de intensidade) da imagem contínua ao longo da linha AB. Nas extremidades do gráfico na Figura \@ref(fig:amostragemquant) (b), a intensidade é mais alta devido a parte branca da imagem. Já os vales representam as partes com menos intensidade, ou seja, as partes mais escuras. Como o computador ainda não tem a capacidade de armazenar uma sequência infinita de números reais, então na quantização são selecionados pontos espaçados igualmente, como na Figura \@ref(fig:amostragemquant) (c).

Na prática, esse procedimento de amostragem é realizado pelos sensores, nos casos mais comuns por um sensor de uma câmera, que geralmente é retangular. Desta forma, a quantidade de células sensíveis na matriz do sensor determina os limites da amostragem. Dito isso, percebe-se que para representar de maneira real o mundo, teríamos que ter um número infinito de *pixels*. Como isso não é possível, recorremos a opção de utilizar o maior número de *pixels* possíveis. Quanto mais *pixels* houver no sensor, maior será a quantidade de detalhes por ele capturado, melhorando a qualidade da imagem [@gonzalez2010, p.34].

(ref:amostragemquant) Produzindo uma imagem digital. (a) Imagem contínua. (b) Linha de varredura de A a B na imagem contínua utilizada para ilustrar os conceitos de amostragem e quantização. (c) Amostragem e quantização. (d) Linha de varredura digital [@gonzalez2010, p.34].

```{r amostragemquant, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:amostragemquant)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/02-formacao/amostragemquant.png'))
```

### Quantização

Na Figura \@ref(fig:amostragemquant) (c), os níveis de intensidade ainda variam dentro de uma faixa contínua. A função digital da intensidade é obtida pela quantização, em que as intensidades das amostras são mapeadas em um conjunto de quantidades discretas. Na Figura \@ref(fig:amostragemquant) (d), os valores contínuos de intensidade são quantizados estabelecendo um dos oito valores para cada amostra de acordo com a escala de intensidade na Figura \@ref(fig:amostragemquant) (c).

Na prática, geralmente a etapa de quantização é realizada diretamente no *hardware* utilizando um conversor analógico-digital [@burger2009, p.8]. A conversão dos valores contínuos para valores discretos pode ser realizada por meio de arredondamento, truncamento ou algum outro processo [@bovik2009essential, p.9].
No processo de quantização, geralmente os níveis de intensidade são mapeados por uma transformação linear para um conjunto finitos de inteiros não negativos $\{0,\dots, L-1\}$, onde $L$ é uma potência de dois, ou seja, $L = 2^k$ [@bovik2009essential, p.10]. Isso significa que $L$ é o número de tons de cinza que podem ser representados com uma quantidade $k$ de bits. Em muitas situações é utilizado $k = 8$, ou seja, temos 256 níveis de cinza.
Ao realizar a quantização e a amostragem linha por linha no objeto da Figura \@ref(fig:quantizacao) (a) é produzida uma imagem digital bidimensional como na Figura \@ref(fig:quantizacao).

(ref:quantizacao) (a) Imagem contínua projetada em uma matriz de sensores. (b) Resultado da amostragem e quantização da imagem [@gonzalez2010, p.35].

```{r quantizacao, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:quantizacao)', fig.align='center', out.width='60%'}
knitr::include_graphics(rep('imagens/02-formacao/quantizacao.png'))
```

## Definição de imagem digital

Uma imagem pode ser definida como uma função bidimensional, $f(x, y)$, em que $x$ e $y$ são coordenadas espaciais (plano), e a amplitude de $f$ em qualquer par de coordenadas $(x, y)$ é chamada de intensidade ou nível de cinza da imagem nesse ponto [@gonzalez2010, p.36]. Quando $x$, $y$ e os valores de intensidade de $f$ são quantidades finitas e discretas, chamamos de imagem digital. 

A função $f(x, y)$ pode ser representada na forma de uma matriz $M \times N$ como na equação \@ref(eq:imagemDigital), em que as $M$ linhas são identificadas pelas coordenadas em $x$, e as $N$ colunas em $y$. Cada elemento dessa matriz é chamado de elemento de imagem, elemento pictórico, *pixel* ou *pel*. O formato numérico da matriz, imagem \@ref(fig:imagemdigital), é apropriado para o desenvolvimento de algoritmos, representado pela equação \@ref(eq:imagemDigital).

$$f(x,y) = \begin{bmatrix}
 f(0,0)   & f(0,1)     & \cdots & f(0,N-1)    \\ 
 f(1,0)   & f(1,1)     & \cdots & f(1, N-1)   \\ 
 \vdots   & \vdots    & &         \vdots      \\ 
 f(M-1,0) & f(M-1, 1)  & \cdots & f(M-1, N-1)
\end{bmatrix}
(\#eq:imagemDigital)
$$

(ref:imagemdigital) Representações da imagem digital [@gonzalez2010, p.36].

```{r imagemdigital, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:imagemdigital)', fig.align='center', out.width='70%'}
knitr::include_graphics(rep('imagens/02-formacao/imagemdigital.png'))
```

Na Figura \@ref(fig:imagemdigital) (a) temos a representação da imagem em 3D, onde a intensidade de cada *pixel* é representada no eixo z, ou seja, sua altura. Como a matriz numérica transmite pouca informação visual é comum uma representação como na Figura \@ref(fig:imagemdigital) (b), formato que seria visualizado em um monitor ou uma fotografia [@gonzalez2010, p.36]. Em cada ponto da Figura \@ref(fig:imagemdigital) (a), o nível de cinza é proporcional ao valor da intensidade $f$, assumindo valores 0, 0,5 ou 1. Um monitor ou impressora simplesmente converte esses três valores em preto, cinza ou branco.

Note que na Figura \@ref(fig:imagemdigital), a origem de uma imagem digital se localiza na parte superior esquerda, com o eixo x positivo direcionado para baixo e o eixo y positivo para a direita. Esse padrão segue o comportamento de varredura de dispositivos de visualização de imagem, como os monitores de TV, que começam do canto superior esquerdo da imagem e se movem para a direita, fazendo uma linha por vez [@gonzalez2010, p.36].
De acordo com o tamanho da matriz ($M \times N$) e dos níveis discretos de tons de cinza ($L = 2^k$) que os *pixels* podem assumir é possível determinar o número, $b$, de bits necessários para armazenar uma imagem digitalizada: 

$$
b = M × N × k
(\#eq:tamanhoBits)
$$

Quando uma imagem pode ter $2^k$ níveis de intensidade, geralmente ela é denominada como uma “imagem de $k$ bits”. Por exemplo, uma imagem com 256 níveis discretos de intensidade é chamada de uma imagem de 8 bits. A Figura \@ref(fig:tabelabits)  mostra o número de bits utilizados para armazenar imagens quadradas de dimensão ($N \times N$) para diferentes valores de $N$ e $k$. O número de níveis de intensidade ($L$) correspondente a cada valor de $k$ é mostrado entre parênteses. Observa-se na Figura \@ref(fig:tabelabits) que uma imagem de 8 bits com dimensões 1024 × 1024 exigiria aproximadamente 1MB para armazenamento.

(ref:tabelabits) Número de bits de armazenamento para vários valores de $N$ e $k$ [@gonzalez2010, p.38].

```{r tabelabits, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:tabelabits)', fig.align='center', out.width='100%'}
knitr::include_graphics(rep('imagens/02-formacao/tabelabits.png'))
```

## Resolução espacial e de intensidade

Sem as especificações espaciais da imagem, não se pode inferir sobre a qualidade apenas pelo tamanho ($M \times N$) em quantidades de *pixels*. Outra medida para especificar a resolução espacial é a densidade de *pixels*, podendo ser expressa  como pontos (*pixels*)  por unidade de distância, comumente *dots per inch* (pontos por polegada ou dpi). Referências de qualidade em relação à resolução espacial são, por exemplo, jornais impressos com uma resolução de 75 dpi e páginas de livros geralmente impressas com 2400 dpi [@gonzalez2010, p.38].

A Figura \@ref(fig:resolucaoespacial) mostra os efeitos da redução da resolução espacial em uma imagem em seis resoluções diferentes. A Figura \@ref(fig:resolucaoespacial) (a) tem resolução 512 x 512, e a resolução das demais \@ref(fig:resolucaoespacial) (b-f) diminui pela metade de forma sequencial. Todas as imagens têm as mesmas dimensões, ampliando-se o tamanho do *pixel* para deixar mais evidente a perda de detalhes nas imagens de baixa resolução. 

(ref:resolucaoespacial) Efeitos da redução da resolução espacial [@pedrini2008, p.20].

```{r resolucaoespacial, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:resolucaoespacial)', fig.align='center', out.width='50%'}
knitr::include_graphics(rep('imagens/02-formacao/resolucaoespacial.png'))
```

A resolução de intensidade ou profundidade corresponde ao número de bits ($k$) utilizados para estabelecer os níveis de cinza da imagem ($L=2^k$). Por exemplo, em uma imagem cuja intensidade é quantizada em $L= 256$ níveis, a profundidade é de $k = 8$ bits por *pixel*. 

Os efeitos da redução dos níveis de cinza (profundidade) podem ser vistos na Figura \@ref(fig:reducaoprofundidade). A imagem (a) apresenta 256 níveis de cinza ($k = 8$).  As imagens (b) e (c) foram geradas pela redução do número de bits  $k = 4$ e $k = 2$, respectivamente, mas mantendo a mesma dimensão.

(ref:reducaoprofundidade) Efeitos da redução de profundidade [@moeslund2012, p.19].

```{r reducaoprofundidade, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:reducaoprofundidade)', fig.align='center', out.width='90%'}
knitr::include_graphics(rep('imagens/02-formacao/reducaoprofundidade.png'))
```

## *Pixels*

A topologia digital da imagem desempenha muita importância na especificação, localização e relação entre as coordenadas da imagem, facilitando sua manipulação. A topologia de uma imagem digital contém as seguintes propriedades dos *pixels*: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas.

Uma imagem digitalizada contém as seguintes propriedades de seus *pixels*: Vizinhança ( 4, D e 8 ), Conectividade, Adjacência, Caminho, Componente Conexa, Medidas de Distância, Operações Lógico-aritméticas.

Para especificar, localizar, e relacionar topologicamente à imagem, consideramos: $p$, $q$ e $u$ denotando os pontos e, $p(x,y)$, $q(x,y)$, $u(x,y)$ coordenadas dos pontos denotados, expressaremos $V$ Conjunto de valores em uma imagem binária $V=\{0,1\}$.

### Vizinhança

- **Vizinhança 4 - $\left[N_4(p)\right]$**

$N_4(p)$ em $p(x,y)$ possui quatro vizinhos,  dois na horizontal outros  dois na vertical de suas coordenadas, ou seja, é o conjunto de *pixels* ao redor de $p$, sem considerar as diagonais[@thome2017, p.15]. Exemplo na Figura \@ref(fig:vizinhanca) (a).

$$p(x,y): p(x+1, y), p(x-1, y), p(x, y+1), p(x, y-1)$$

- **Vizinhança D - $\left[N_D(p)\right]$**

$N_D(p)$ em $p(x,y)$ possui quatro vizinhos,  dois na diagonais superiores (direita, esquerda ) outras duas na diagonais inferiores (direita, esquerda) de  suas coordenadas, ou seja, o conjunto de *pixels* ao redor de $p$, considerando apenas as diagonais [@thome2017, p.15].Exemplo na Figura \@ref(fig:vizinhanca) (b).

$$p(x,y): p(x+1,y+1), p(x+1, y-1), p(x-1, y+1), p(x-1, y-1)$$

- **Vizinhança 8 - $\left[N_8(p)\right]$**

$N_8(p)$ em $p(x,y)$ possui oito vizinhos, quatro $N_4(p)$ e outros quatro $N_D(p)$ de suas coordenadas, ou seja, o conjunto de *pixels* ao redor de $p$, considerando união das  vizinhanças-4 e vizinhança-8 [@thome2017, p.15]. Exemplo na Figura \@ref(fig:vizinhanca) (c).

$$p(x,y): N_8(p) = N_4(p) \cup N_D(p)$$

(ref:vizinhanca) Vizinhanças[@thome2017].

```{r vizinhanca, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:vizinhanca)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/vizinhanca.png'))

```

### Conectividade

Conceito importante, usado no estabelecimento limite das bordas de objetos e identificação de componentes das regiões da imagem (obtenção de propriedades específicas do objeto para um processamento de mais alto nível). 

Dois *pixels* $p(x,y)$, $q(x,y)$ estão conectados se:

1. São de alguma forma vizinhos ($N_4$,$N_D$ ou $N_8$).
2. Seus níveis de cinza satisfazem algum critério de similaridade ($V = \{ \dots \}$).

- **Conectividade de 4**:

Os *pixels* $p$ e $q$, assumindo valores em $V$ , são conectados de 4 somente se $q$ pertence ao conjunto $N_4(p)$. Exemplo na Figura \@ref(fig:conectividade) (a).

$$C4_{p,q} \text{ em } V \Leftrightarrow q \in N_4(p) \wedge f(p) \wedge f(q) \in V $$
$$V = \{0\} \to C4_{p.q} \text{ verdadeiro}$$

- **Conectividade de $m$ (conectividade mista)**:

Dois *pixels* $p$ e $q$, assumindo valores em $V$, são conectados de $m$ somente se:

1. $q$ pertence ao conjunto $N_4(p)$
2. $q$ pertence ao conjunto $N_D(p)$ e a interseção entre $N_4(p)$ e  $N_4(q)$ for vazia.

$$Cm_{p,q} \text{ em } V\Leftrightarrow (q \in N_4(p) \vee (q \in N_D(p) \wedge N_4(p) \cap N_4(q) = \{\})) \vee f(p) e f(q) \in V  $$
$$V = \{0\} \to Cm_{p.q}\text{ falso}$$
Exemplo na Figura \@ref(fig:conectividade) (b).

- **Conectividade de 8**:

Os pixels $p$ e $q$, assumindo valores em V, são conectados de 8 somente se $q$ pertence ao conjunto $N_8(p)$. Exemplo na Figura \@ref(fig:conectividade) (c).

$$C8_{p,q} \text{ em } V\Leftrightarrow q \in N_8(p) \wedge f(p) \wedge f(q) \in V $$
$$V = \{0\} \to C8_{p.q}\text{ verdadeiro}$$

(ref:conectividade) Conectividades [@thome2017].

```{r conectividade, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:conectividade)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/conectividade.png'))

```

### Adjacência

Dois pixels $p$ e $q$, com valores pertencendo a $V$ são:

1. Adjacentes-4 se $q$ estiver no conjunto $N_4(p)$. 
2. Adjacentes-8 se $q$ estiver no conjunto $N_8(p)$.
3. Adjacentes-m, $p$ e $q$ subconjuntos de pixels onde $\{(pq) \vee (pp) \vee (q q) \}$, são ditos adjacentes se pegamos um pixel do primeiro conjunto for adjacente a um pixel do segundo.

Na Figura \@ref(fig:adjacencia) temos exemplos de 1. e 2. (em \@ref(fig:adjacencia) (a)) e de 3. em \@ref(fig:adjacencia) (b).

(ref:adjacencia) Adjacências [@thome2017].

```{r adjacencia, echo=FALSE, fig.asp=.7, fig.width= 4, fig.cap='(ref:adjacencia)', fig.align='center', out.width='55%'}
knitr::include_graphics(rep('imagens/02-formacao/adjacencia.png'))

```

### Componente Conexa  

Dois pixels $p$ e $q$ de um subconjunto de pixels $V$ da imagem são ditos conexos em $V$ se existir um caminho de $p$ a $q$ inteiramente contido em $V$.

Para qualquer pixel $p$ em $V$, o conjunto de pixels em $V$ que são conexos a $p$ é chamado um componente conexo de $V$.

Note que em uma componente conexo qualquer dois pixels deste componentes são conexos entre si.

Em componentes conexos distintos os pixels são disjuntos (não conectados).

### Medidas de Distância

Para pixels $p$, $q$ e $z$ com coordenadas $p(x,y)$, $q(s,t)$ e $z(u,v)$, respectivamente, $D$ é uma função distância ou métrica se: 

1. $D(p,q) >= 0 (D(p,q) = 0 ) \Leftrightarrow  p = q$ 
1. $D(p,q) = D(q,p)$
1. $D(p,z) <= D(p,q) + D(q,u)$ 

- A distância entre dois pontos quaisquer pode ser definida por: 

$$D_e(p,q) = \sqrt{(x-s)^2 + (y-t)^2}$$

conhecida como Distância Euclidiana.

- Distância $D_4$(City Block ou Quarteirão) entre $p(x,y)$ e $q(s,t)$ é definida por: 

$$D4(p,q)=|x-s|+|y-t|$$

Distância D8(Distância Xadrez) entre $p$ e $q$ é definida como: 

$$D_8 = max(|x-s|,|y-t|)$$

### Operações Lógico-aritméticas 

As operações entre pixels são computadas pixel a pixel, considerando p e q podemos efetuar as seguintes operações aritméticas e lógicas.

- Operações Aritméticas:
  1. **Adição**: $p+q$. O uso ocorre ao se fazer a média para redução de ruído. 
  2. **Subtração**: $p-q$. É usada para remover informação estática de fundo, Detecção de diferenças entre imagens.
  3. **Multiplicação**: $p\cdot q$. Calibração de brilho.
  4. **Divisão**: $p\div q$
  
As operações Aritmética de Multiplicação e Divisão são usadas para corrigir sombras em níveis de cinza, produzidas em não uniformidades da iluminação ou no sensor utilizado para a aquisição da imagem. 

- Operações Lógicas:
  1. **Conjunção**: $p\wedge q$
  2. **Disjunção**: $p\vee q$
  3. **Complementar**: $\neg q(\bar{q})$

As operações lógicas podem ser combinadas para formar qualquer outra operação lógica, são aplicadas apenas em imagens binárias, tem seu uso no mascaramento, detecção de características e análise de forma.

